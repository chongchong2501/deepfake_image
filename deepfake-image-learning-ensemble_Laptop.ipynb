{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Kaggle Multi-Model Ensemble Deepfake Detection with AMP\n",
      "PyTorch Version: 2.7.1+cu118\n",
      "Captum Available: True\n",
      "AMP Available: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MECHREUO\\AppData\\Roaming\\Python\\Python313\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: å¯¼å…¥ä¾èµ–å’Œç¯å¢ƒè®¾ç½®\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "# æ·»åŠ AMPæ··åˆç²¾åº¦è®­ç»ƒæ”¯æŒ\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, f1_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import font_manager\n",
    "import matplotlib.patches as patches\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "import time\n",
    "from PIL import Image\n",
    "import gc\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# è§£é‡Šå·¥å…·å¯¼å…¥\n",
    "try:\n",
    "    from captum.attr import LayerGradCam, IntegratedGradients\n",
    "    from captum.attr import visualization as viz\n",
    "    CAPTUM_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"âš ï¸ Captum not available. Install with: pip install captum\")\n",
    "    CAPTUM_AVAILABLE = False\n",
    "\n",
    "# è®¾ç½®matplotlibä½¿ç”¨è‹±æ–‡å­—ä½“å’Œé«˜DPI\n",
    "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
    "plt.rcParams['font.size'] = 10\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "plt.rcParams['savefig.dpi'] = 300\n",
    "plt.rcParams['savefig.bbox'] = 'tight'\n",
    "\n",
    "# è®¾ç½®éšæœºç§å­\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"ğŸš€ Kaggle Multi-Model Ensemble Deepfake Detection with AMP\")\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"Captum Available: {CAPTUM_AVAILABLE}\")\n",
    "print(f\"AMP Available: {torch.cuda.is_available() and hasattr(torch.cuda.amp, 'autocast')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "âœ… AMP (Automatic Mixed Precision) enabled\n",
      "ğŸ“ˆ Expected benefits: ~30% faster training, ~40% memory reduction\n",
      "GPU: NVIDIA GeForce RTX 4070 Laptop GPU\n",
      "GPU 0: NVIDIA GeForce RTX 4070 Laptop GPU\n",
      "GPU 0 Memory: 8.0GB\n",
      "Plots will be saved to: ./works/plots\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: å‚æ•°é…ç½®\n",
    "BASE_PATH = r'E:\\program\\deepfake_image\\Dataset'\n",
    "TRAIN_PATH = os.path.join(BASE_PATH, 'Train')\n",
    "VAL_PATH = os.path.join(BASE_PATH, 'Validation')\n",
    "\n",
    "# è®­ç»ƒå‚æ•°\n",
    "# å›¾åƒå¤§å°\n",
    "IMG_SIZE = 256\n",
    "\n",
    "# è®­ç»ƒæ‰¹æ¬¡å¤§å° (AMPå¯ä»¥æ”¯æŒæ›´å¤§çš„æ‰¹æ¬¡)\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# å­¦ä¹ ç‡\n",
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "# è®­ç»ƒè½®æ•°\n",
    "EPOCHS = 15\n",
    "\n",
    "# æƒé‡è¡°å‡ç³»æ•°\n",
    "WEIGHT_DECAY = 1e-4\n",
    "\n",
    "# æ—©åœè½®æ•°\n",
    "PATIENCE = 3\n",
    "\n",
    "# æ•°æ®åŠ è½½å™¨çš„å·¥ä½œè¿›ç¨‹æ•°é‡\n",
    "NUM_WORKERS = 0\n",
    "\n",
    "# AMPæ··åˆç²¾åº¦è®­ç»ƒå¼€å…³\n",
    "USE_AMP = True  # å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒ\n",
    "\n",
    "# è·å–å½“å‰è®¾å¤‡çš„GPUä¿¡æ¯\n",
    "NUM_GPUS = torch.cuda.device_count()\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {DEVICE}\")\n",
    "\n",
    "# AMPæ”¯æŒæ£€æŸ¥\n",
    "if torch.cuda.is_available() and USE_AMP:\n",
    "    print(\"âœ… AMP (Automatic Mixed Precision) enabled\")\n",
    "    print(\"ğŸ“ˆ Expected benefits: ~30% faster training, ~40% memory reduction\")\n",
    "elif not torch.cuda.is_available():\n",
    "    USE_AMP = False\n",
    "    print(\"âš ï¸ CUDA not available, AMP disabled\")\n",
    "else:\n",
    "    print(\"â„¹ï¸ AMP disabled by configuration\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    if NUM_GPUS > 1:\n",
    "        print(f\"Multi-GPU Training: {[torch.cuda.get_device_name(i) for i in range(NUM_GPUS)]}\")\n",
    "        print(f\"GPU Count: {NUM_GPUS}\")\n",
    "    else:\n",
    "        print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    \n",
    "    for i in range(NUM_GPUS):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        print(f\"GPU {i} Memory: {torch.cuda.get_device_properties(i).total_memory / 1024**3:.1f}GB\")\n",
    "else:\n",
    "    NUM_WORKERS = 0\n",
    "    print(\"Using CPU Training\")\n",
    "\n",
    "# åˆ›å»ºè¾“å‡ºç›®å½•\n",
    "PLOTS_DIR = './works/plots'\n",
    "os.makedirs(PLOTS_DIR, exist_ok=True)\n",
    "print(f\"Plots will be saved to: {PLOTS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 3: æ•°æ®åŠ è½½å‡½æ•°\n",
    "classes = ['Real', 'Fake']\n",
    "\n",
    "def create_dataframe(data_path, dataset_type):\n",
    "    \"\"\"åˆ›å»ºæ•°æ®é›†DataFrame\"\"\"\n",
    "    filepaths, labels = [], []\n",
    "    \n",
    "    for label_idx, cls in enumerate(classes):\n",
    "        folder = os.path.join(data_path, cls)\n",
    "        if os.path.exists(folder):\n",
    "            for img_name in os.listdir(folder):\n",
    "                if img_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                    filepaths.append(os.path.join(folder, img_name))\n",
    "                    labels.append(label_idx)\n",
    "    \n",
    "    df = pd.DataFrame({'filepath': filepaths, 'label': labels})\n",
    "    print(f\"{dataset_type}é›†å›¾ç‰‡æ•°: {len(df)}\")\n",
    "    if len(df) > 0:\n",
    "        print(f\"{dataset_type}é›†ç±»åˆ«åˆ†å¸ƒ:\")\n",
    "        for idx, cls in enumerate(classes):\n",
    "            count = len(df[df['label'] == idx])\n",
    "            print(f\"  {cls}: {count} ({count/len(df)*100:.1f}%)\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 4: æ•°æ®é¢„å¤„ç†å’Œå¢å¼º\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=10),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "class DeepfakeDataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.df.iloc[idx]['filepath']\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        label = self.df.iloc[idx]['label']\n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 5: æ¨¡å‹å®šä¹‰\n",
    "def create_efficientnet_b0():\n",
    "    \"\"\"åˆ›å»ºEfficientNet-B0æ¨¡å‹\"\"\"\n",
    "    model = models.efficientnet_b0(weights='IMAGENET1K_V1')\n",
    "    model.classifier[1] = nn.Linear(1280, 2)\n",
    "    return model\n",
    "\n",
    "def create_resnet18():\n",
    "    \"\"\"åˆ›å»ºResNet18æ¨¡å‹\"\"\"\n",
    "    model = models.resnet18(weights='IMAGENET1K_V1')\n",
    "    model.fc = nn.Linear(512, 2)\n",
    "    return model\n",
    "\n",
    "def create_convnext_tiny():\n",
    "    \"\"\"åˆ›å»ºConvNeXt-Tinyæ¨¡å‹\"\"\"\n",
    "    model = models.convnext_tiny(weights='IMAGENET1K_V1')\n",
    "    model.classifier[2] = nn.Linear(768, 2)\n",
    "    return model\n",
    "\n",
    "# æ¨¡å‹é…ç½®å­—å…¸\n",
    "MODEL_CONFIGS = {\n",
    "    'efficientnet_b0': {\n",
    "        'create_fn': create_efficientnet_b0,\n",
    "        'name': 'EfficientNet-B0'\n",
    "    },\n",
    "    'resnet18': {\n",
    "        'create_fn': create_resnet18,\n",
    "        'name': 'ResNet18'\n",
    "    },\n",
    "    'convnext_tiny': {\n",
    "        'create_fn': create_convnext_tiny,\n",
    "        'name': 'ConvNeXt-Tiny'\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 6: å•æ¨¡å‹è®­ç»ƒå‡½æ•° (æ”¯æŒAMP)\n",
    "def train_single_model(model_key, train_loader, val_loader, save_path):\n",
    "    \"\"\"è®­ç»ƒå•ä¸ªæ¨¡å‹ - æ”¯æŒAMPæ··åˆç²¾åº¦è®­ç»ƒ\"\"\"\n",
    "    print(f\"\\nğŸ”¥ Starting Training {MODEL_CONFIGS[model_key]['name']}\")\n",
    "    if USE_AMP:\n",
    "        print(\"âš¡ Using AMP (Automatic Mixed Precision) for faster training\")\n",
    "    \n",
    "    # è®°å½•è®­ç»ƒå¼€å§‹æ—¶é—´\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # åˆ›å»ºæ¨¡å‹\n",
    "    model = MODEL_CONFIGS[model_key]['create_fn']()\n",
    "    model = model.to(DEVICE)\n",
    "    \n",
    "    # å¤šGPUæ”¯æŒ\n",
    "    if NUM_GPUS > 1:\n",
    "        model = nn.DataParallel(model)\n",
    "        print(f\"âœ… Model configured for multi-GPU training with {NUM_GPUS} GPUs\")\n",
    "    \n",
    "    # æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
    "    \n",
    "    # AMPæ¢¯åº¦ç¼©æ”¾å™¨\n",
    "    scaler = GradScaler() if USE_AMP else None\n",
    "    \n",
    "    # è®­ç»ƒè®°å½•\n",
    "    best_val_acc = 0\n",
    "    patience_counter = 0\n",
    "    train_losses, val_losses, val_accuracies, learning_rates = [], [], [], []\n",
    "    val_f1_scores = []\n",
    "    \n",
    "    # æ˜¾å­˜ä½¿ç”¨ç›‘æ§\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"ğŸ” Initial GPU Memory: {torch.cuda.memory_allocated()/1024**3:.2f}GB / {torch.cuda.max_memory_allocated()/1024**3:.2f}GB\")\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        # è®­ç»ƒé˜¶æ®µ\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        \n",
    "        for imgs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} [Train]\"):\n",
    "            imgs, labels = imgs.to(DEVICE, non_blocking=True), labels.to(DEVICE, non_blocking=True)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # AMPå‰å‘ä¼ æ’­\n",
    "            if USE_AMP:\n",
    "                with autocast():\n",
    "                    outputs = model(imgs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                \n",
    "                # AMPåå‘ä¼ æ’­\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                # æ ‡å‡†è®­ç»ƒ\n",
    "                outputs = model(imgs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        # éªŒè¯é˜¶æ®µ\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        all_val_preds = []\n",
    "        all_val_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for imgs, labels in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} [Val]\"):\n",
    "                imgs, labels = imgs.to(DEVICE, non_blocking=True), labels.to(DEVICE, non_blocking=True)\n",
    "                \n",
    "                # AMPéªŒè¯\n",
    "                if USE_AMP:\n",
    "                    with autocast():\n",
    "                        outputs = model(imgs)\n",
    "                        loss = criterion(outputs, labels)\n",
    "                else:\n",
    "                    outputs = model(imgs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "                \n",
    "                all_val_preds.extend(predicted.cpu().numpy())\n",
    "                all_val_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        val_acc = correct / total\n",
    "        val_f1 = f1_score(all_val_labels, all_val_preds, average='weighted')\n",
    "        \n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_acc)\n",
    "        val_f1_scores.append(val_f1)\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        learning_rates.append(current_lr)\n",
    "        \n",
    "        # æ˜¾å­˜ç›‘æ§\n",
    "        if torch.cuda.is_available():\n",
    "            current_memory = torch.cuda.memory_allocated() / 1024**3\n",
    "            max_memory = torch.cuda.max_memory_allocated() / 1024**3\n",
    "            memory_info = f\"GPU Mem: {current_memory:.2f}GB/{max_memory:.2f}GB\"\n",
    "        else:\n",
    "            memory_info = \"\"\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}: Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, \"\n",
    "              f\"Val Acc: {val_acc:.4f}, Val F1: {val_f1:.4f}, LR: {current_lr:.6f} {memory_info}\")\n",
    "        \n",
    "        # ä¿å­˜æœ€ä½³æ¨¡å‹\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0\n",
    "            # ä¿å­˜æ¨¡å‹æ—¶å¤„ç†å¤šGPUæƒ…å†µ\n",
    "            if NUM_GPUS > 1:\n",
    "                torch.save(model.module.state_dict(), save_path)\n",
    "            else:\n",
    "                torch.save(model.state_dict(), save_path)\n",
    "            print(f\"âœ… Best model saved, validation accuracy: {best_val_acc:.4f}\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= PATIENCE:\n",
    "                print(\"â›” Early stopping triggered\")\n",
    "                break\n",
    "        \n",
    "        # æ¯ä¸ªepochåæ¸…ç†æ˜¾å­˜\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    # è®¡ç®—è®­ç»ƒæ—¶é—´\n",
    "    training_time = time.time() - start_time\n",
    "    print(f\"â±ï¸ Training completed in {training_time:.2f} seconds\")\n",
    "    \n",
    "    # æœ€ç»ˆæ˜¾å­˜æ¸…ç†\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        print(f\"ğŸ§¹ Final GPU Memory: {torch.cuda.memory_allocated()/1024**3:.2f}GB\")\n",
    "    \n",
    "    return {\n",
    "         'best_acc': best_val_acc,\n",
    "         'train_losses': train_losses,\n",
    "         'val_losses': val_losses,\n",
    "         'val_accuracies': val_accuracies,\n",
    "         'val_f1_scores': val_f1_scores,\n",
    "         'learning_rates': learning_rates,\n",
    "         'training_time': training_time,\n",
    "         'amp_enabled': USE_AMP\n",
    "     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 7: å¯è§†åŒ–å‡½æ•°\n",
    "def plot_training_history(model_results, save_dir=PLOTS_DIR):\n",
    "    \"\"\"ç»˜åˆ¶è®­ç»ƒå†å²å¯è§†åŒ–\"\"\"\n",
    "    print(\"ğŸ“Š Generating training history visualizations...\")\n",
    "    \n",
    "    # 1. å•æ¨¡å‹è®­ç»ƒå†å² (2x2 å­å›¾)\n",
    "    for model_key, results in model_results.items():\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        fig.suptitle(f'{MODEL_CONFIGS[model_key][\"name\"]} Training History', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        epochs = range(1, len(results['train_losses']) + 1)\n",
    "        \n",
    "        # Lossæ›²çº¿\n",
    "        axes[0, 0].plot(epochs, results['train_losses'], 'b-', label='Train Loss', linewidth=2)\n",
    "        axes[0, 0].plot(epochs, results['val_losses'], 'r-', label='Validation Loss', linewidth=2)\n",
    "        axes[0, 0].set_title('Training & Validation Loss', fontweight='bold')\n",
    "        axes[0, 0].set_xlabel('Epoch')\n",
    "        axes[0, 0].set_ylabel('Loss')\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Accuracyæ›²çº¿\n",
    "        axes[0, 1].plot(epochs, results['val_accuracies'], 'g-', label='Validation Accuracy', linewidth=2)\n",
    "        axes[0, 1].set_title('Validation Accuracy', fontweight='bold')\n",
    "        axes[0, 1].set_xlabel('Epoch')\n",
    "        axes[0, 1].set_ylabel('Accuracy')\n",
    "        axes[0, 1].legend()\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Learning Rateæ›²çº¿\n",
    "        axes[1, 0].plot(epochs, results['learning_rates'], 'purple', linewidth=2)\n",
    "        axes[1, 0].set_title('Learning Rate Schedule', fontweight='bold')\n",
    "        axes[1, 0].set_xlabel('Epoch')\n",
    "        axes[1, 0].set_ylabel('Learning Rate')\n",
    "        axes[1, 0].set_yscale('log')\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Validation Accuracyåˆ†å¸ƒ\n",
    "        axes[1, 1].hist(results['val_accuracies'], bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "        axes[1, 1].axvline(results['best_acc'], color='red', linestyle='--', linewidth=2, label=f'Best: {results[\"best_acc\"]:.4f}')\n",
    "        axes[1, 1].set_title('Validation Accuracy Distribution', fontweight='bold')\n",
    "        axes[1, 1].set_xlabel('Accuracy')\n",
    "        axes[1, 1].set_ylabel('Frequency')\n",
    "        axes[1, 1].legend()\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        save_path = os.path.join(save_dir, f'{model_key}_training_history.png')\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f\"âœ… Saved: {save_path}\")\n",
    "    \n",
    "    # 2. å¤šæ¨¡å‹å¯¹æ¯”å›¾ (å››çº¿å¯¹æ¯”)\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle('Multi-Model Training Comparison', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    colors = ['blue', 'red', 'green', 'orange', 'purple']\n",
    "    \n",
    "    # éªŒè¯Losså¯¹æ¯”\n",
    "    for i, (model_key, results) in enumerate(model_results.items()):\n",
    "        epochs = range(1, len(results['val_losses']) + 1)\n",
    "        axes[0, 0].plot(epochs, results['val_losses'], color=colors[i % len(colors)], \n",
    "                       label=MODEL_CONFIGS[model_key]['name'], linewidth=2)\n",
    "    axes[0, 0].set_title('Validation Loss Comparison', fontweight='bold')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Validation Loss')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # éªŒè¯Accuracyå¯¹æ¯”\n",
    "    for i, (model_key, results) in enumerate(model_results.items()):\n",
    "        epochs = range(1, len(results['val_accuracies']) + 1)\n",
    "        axes[0, 1].plot(epochs, results['val_accuracies'], color=colors[i % len(colors)], \n",
    "                       label=MODEL_CONFIGS[model_key]['name'], linewidth=2)\n",
    "    axes[0, 1].set_title('Validation Accuracy Comparison', fontweight='bold')\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('Validation Accuracy')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # è®­ç»ƒæ—¶é•¿å¯¹æ¯”\n",
    "    model_names = [MODEL_CONFIGS[key]['name'] for key in model_results.keys()]\n",
    "    training_times = [results['training_time'] for results in model_results.values()]\n",
    "    bars = axes[1, 0].bar(model_names, training_times, color=colors[:len(model_names)], alpha=0.7)\n",
    "    axes[1, 0].set_title('Training Time Comparison', fontweight='bold')\n",
    "    axes[1, 0].set_ylabel('Training Time (seconds)')\n",
    "    axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # æ·»åŠ æ•°å€¼æ ‡ç­¾\n",
    "    for bar, time_val in zip(bars, training_times):\n",
    "        height = bar.get_height()\n",
    "        axes[1, 0].text(bar.get_x() + bar.get_width()/2., height + height*0.01,\n",
    "                       f'{time_val:.1f}s', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # F1-Scoreå¯¹æ¯”\n",
    "    for i, (model_key, results) in enumerate(model_results.items()):\n",
    "        epochs = range(1, len(results['val_f1_scores']) + 1)\n",
    "        axes[1, 1].plot(epochs, results['val_f1_scores'], color=colors[i % len(colors)], \n",
    "                       label=MODEL_CONFIGS[model_key]['name'], linewidth=2)\n",
    "    axes[1, 1].set_title('F1-Score Comparison', fontweight='bold')\n",
    "    axes[1, 1].set_xlabel('Epoch')\n",
    "    axes[1, 1].set_ylabel('F1-Score')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    save_path = os.path.join(save_dir, 'multi_model_comparison.png')\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"âœ… Saved: {save_path}\")\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, title, save_name, save_dir=PLOTS_DIR):\n",
    "    \"\"\"ç»˜åˆ¶æ··æ·†çŸ©é˜µ\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", \n",
    "                xticklabels=['Real', 'Fake'], yticklabels=['Real', 'Fake'],\n",
    "                cbar_kws={'label': 'Count'})\n",
    "    plt.title(title, fontsize=14, fontweight='bold')\n",
    "    plt.ylabel('True Label', fontweight='bold')\n",
    "    plt.xlabel('Predicted Label', fontweight='bold')\n",
    "    \n",
    "    # æ·»åŠ å‡†ç¡®ç‡ä¿¡æ¯\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    plt.text(0.5, -0.1, f'Accuracy: {accuracy:.4f}', \n",
    "             transform=plt.gca().transAxes, ha='center', fontweight='bold')\n",
    "    \n",
    "    save_path = os.path.join(save_dir, f'{save_name}_confusion_matrix.png')\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"âœ… Saved: {save_path}\")\n",
    "\n",
    "def plot_ensemble_analysis(models_dict, data_loader, device):\n",
    "    \"\"\"ç»˜åˆ¶é›†æˆåˆ†æå¯è§†åŒ– - ä¿®å¤ç‰ˆæœ¬\"\"\"\n",
    "    print(\"ğŸ” ç”Ÿæˆé›†æˆåˆ†æå¯è§†åŒ–...\")\n",
    "    \n",
    "    # æ”¶é›†æ‰€æœ‰æ¨¡å‹çš„é¢„æµ‹ç»“æœ\n",
    "    model_predictions = {}\n",
    "    true_labels = []\n",
    "    \n",
    "    # åˆå§‹åŒ–æ¨¡å‹é¢„æµ‹å­—å…¸\n",
    "    for model_key in models_dict.keys():\n",
    "        model_predictions[model_key] = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in tqdm(data_loader, desc=\"æ”¶é›†æ¨¡å‹é¢„æµ‹\"):\n",
    "            imgs, labels = imgs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "            \n",
    "            # æ”¶é›†çœŸå®æ ‡ç­¾\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "            # æ”¶é›†æ¯ä¸ªæ¨¡å‹çš„é¢„æµ‹\n",
    "            for model_key, model in models_dict.items():\n",
    "                # AMPæ¨ç†\n",
    "                if USE_AMP:\n",
    "                    with autocast():\n",
    "                        outputs = model(imgs)\n",
    "                else:\n",
    "                    outputs = model(imgs)\n",
    "                \n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                model_predictions[model_key].extend(predicted.cpu().numpy())\n",
    "    \n",
    "    # è½¬æ¢ä¸ºnumpyæ•°ç»„\n",
    "    for key in model_predictions:\n",
    "        model_predictions[key] = np.array(model_predictions[key])\n",
    "    true_labels = np.array(true_labels)\n",
    "    \n",
    "    # éªŒè¯æ•°ç»„é•¿åº¦ä¸€è‡´æ€§\n",
    "    print(f\"ğŸ“Š æ•°æ®éªŒè¯: çœŸå®æ ‡ç­¾æ•°é‡={len(true_labels)}\")\n",
    "    for key, preds in model_predictions.items():\n",
    "        print(f\"ğŸ“Š {MODEL_CONFIGS[key]['name']} é¢„æµ‹æ•°é‡={len(preds)}\")\n",
    "        if len(preds) != len(true_labels):\n",
    "            print(f\"âš ï¸ è­¦å‘Š: {MODEL_CONFIGS[key]['name']} é¢„æµ‹æ•°é‡ä¸çœŸå®æ ‡ç­¾ä¸åŒ¹é…!\")\n",
    "            return\n",
    "    \n",
    "    # 1. æ¨¡å‹é¢„æµ‹ä¸€è‡´æ€§çƒ­å›¾\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle('Ensemble Analysis Dashboard', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # è®¡ç®—æ¨¡å‹é—´ä¸€è‡´æ€§\n",
    "    model_keys = list(model_predictions.keys())\n",
    "    n_models = len(model_keys)\n",
    "    consistency_matrix = np.zeros((n_models, n_models))\n",
    "    \n",
    "    for i, key1 in enumerate(model_keys):\n",
    "        for j, key2 in enumerate(model_keys):\n",
    "            if i == j:\n",
    "                consistency_matrix[i, j] = 1.0\n",
    "            else:\n",
    "                agreement = np.mean(model_predictions[key1] == model_predictions[key2])\n",
    "                consistency_matrix[i, j] = agreement\n",
    "    \n",
    "    # ç»˜åˆ¶ä¸€è‡´æ€§çƒ­å›¾\n",
    "    model_names = [MODEL_CONFIGS[key]['name'] for key in model_keys]\n",
    "    sns.heatmap(consistency_matrix, annot=True, fmt='.3f', cmap='RdYlBu_r',\n",
    "                xticklabels=model_names, yticklabels=model_names, ax=axes[0, 0])\n",
    "    axes[0, 0].set_title('Model Prediction Consistency', fontweight='bold')\n",
    "    \n",
    "    # 2. å„æ¨¡å‹å‡†ç¡®ç‡å¯¹æ¯”\n",
    "    accuracies = []\n",
    "    for key in model_keys:\n",
    "        acc = accuracy_score(true_labels, model_predictions[key])\n",
    "        accuracies.append(acc)\n",
    "        print(f\"ğŸ“Š {MODEL_CONFIGS[key]['name']} å‡†ç¡®ç‡: {acc:.4f}\")\n",
    "    \n",
    "    bars = axes[0, 1].bar(model_names, accuracies, color=['skyblue', 'lightcoral', 'lightgreen'][:len(model_names)])\n",
    "    axes[0, 1].set_title('Individual Model Accuracy', fontweight='bold')\n",
    "    axes[0, 1].set_ylabel('Accuracy')\n",
    "    axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # æ·»åŠ æ•°å€¼æ ‡ç­¾\n",
    "    for bar, acc in zip(bars, accuracies):\n",
    "        height = bar.get_height()\n",
    "        axes[0, 1].text(bar.get_x() + bar.get_width()/2., height + 0.005,\n",
    "                       f'{acc:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # 3. é›†æˆé¢„æµ‹ç½®ä¿¡åº¦åˆ†æ\n",
    "    print(\"ğŸ” è®¡ç®—é›†æˆé¢„æµ‹ç½®ä¿¡åº¦...\")\n",
    "    all_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in tqdm(data_loader, desc=\"è®¡ç®—ç½®ä¿¡åº¦\"):\n",
    "            imgs = imgs.to(device, non_blocking=True)\n",
    "            batch_probs = []\n",
    "            \n",
    "            for model_key, model in models_dict.items():\n",
    "                if USE_AMP:\n",
    "                    with autocast():\n",
    "                        outputs = model(imgs)\n",
    "                else:\n",
    "                    outputs = model(imgs)\n",
    "                probs = torch.softmax(outputs, dim=1)\n",
    "                batch_probs.append(probs.cpu().numpy())\n",
    "            \n",
    "            # å¹³å‡æ¦‚ç‡\n",
    "            ensemble_probs = np.mean(batch_probs, axis=0)\n",
    "            all_probs.extend(ensemble_probs)\n",
    "    \n",
    "    all_probs = np.array(all_probs)\n",
    "    ensemble_confidence = np.max(all_probs, axis=1)\n",
    "    ensemble_predictions = np.argmax(all_probs, axis=1)\n",
    "    \n",
    "    # æ­£ç¡®vsé”™è¯¯é¢„æµ‹çš„ç½®ä¿¡åº¦åˆ†å¸ƒ\n",
    "    correct_mask = ensemble_predictions == true_labels\n",
    "    correct_confidence = ensemble_confidence[correct_mask]\n",
    "    incorrect_confidence = ensemble_confidence[~correct_mask]\n",
    "    \n",
    "    axes[1, 0].hist(correct_confidence, bins=30, alpha=0.7, label='Correct', color='green', density=True)\n",
    "    axes[1, 0].hist(incorrect_confidence, bins=30, alpha=0.7, label='Incorrect', color='red', density=True)\n",
    "    axes[1, 0].set_title('Prediction Confidence Distribution', fontweight='bold')\n",
    "    axes[1, 0].set_xlabel('Confidence')\n",
    "    axes[1, 0].set_ylabel('Density')\n",
    "    axes[1, 0].legend()\n",
    "    \n",
    "    # 4. å„ç±»åˆ«F1-scoreå¯¹æ¯”\n",
    "    f1_scores = {}\n",
    "    for key in model_keys:\n",
    "        f1 = f1_score(true_labels, model_predictions[key], average=None)\n",
    "        f1_scores[key] = f1\n",
    "    \n",
    "    x = np.arange(len(classes))\n",
    "    width = 0.25\n",
    "    \n",
    "    for i, (key, f1) in enumerate(f1_scores.items()):\n",
    "        offset = (i - len(model_keys)/2 + 0.5) * width\n",
    "        axes[1, 1].bar(x + offset, f1, width, label=MODEL_CONFIGS[key]['name'])\n",
    "    \n",
    "    axes[1, 1].set_title('F1-Score by Class', fontweight='bold')\n",
    "    axes[1, 1].set_xlabel('Class')\n",
    "    axes[1, 1].set_ylabel('F1-Score')\n",
    "    axes[1, 1].set_xticks(x)\n",
    "    axes[1, 1].set_xticklabels(classes)\n",
    "    axes[1, 1].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    save_path = os.path.join(PLOTS_DIR, 'ensemble_analysis.png')\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"âœ… ä¿å­˜é›†æˆåˆ†æ: {save_path}\")\n",
    "    \n",
    "    # è¿”å›ç»Ÿè®¡ä¿¡æ¯\n",
    "    return {\n",
    "        'model_accuracies': dict(zip(model_keys, accuracies)),\n",
    "        'ensemble_accuracy': accuracy_score(true_labels, ensemble_predictions),\n",
    "        'consistency_matrix': consistency_matrix,\n",
    "        'model_names': model_names\n",
    "    }\n",
    "    \n",
    "def plot_interpretability_analysis(models_dict, data_loader, device):\n",
    "    \"\"\"ç»˜åˆ¶æ¨¡å‹è§£é‡Šæ€§åˆ†æï¼ˆå¦‚æœCaptumå¯ç”¨ï¼‰- ä¿®å¤ç‰ˆæœ¬\"\"\"\n",
    "    if not CAPTUM_AVAILABLE:\n",
    "        print(\"âš ï¸ Captumä¸å¯ç”¨ï¼Œè·³è¿‡è§£é‡Šæ€§åˆ†æ\")\n",
    "        return\n",
    "    \n",
    "    print(\"ğŸ” ç”Ÿæˆæ¨¡å‹è§£é‡Šæ€§åˆ†æ...\")\n",
    "    \n",
    "    try:\n",
    "        # è·å–ä¸€ä¸ªæ‰¹æ¬¡çš„æ•°æ®è¿›è¡Œåˆ†æ\n",
    "        data_iter = iter(data_loader)\n",
    "        imgs, labels = next(data_iter)\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        \n",
    "        # åªåˆ†æå‰4å¼ å›¾ç‰‡\n",
    "        sample_imgs = imgs[:4]\n",
    "        sample_labels = labels[:4]\n",
    "        \n",
    "        fig, axes = plt.subplots(len(models_dict), 4, figsize=(16, 4*len(models_dict)))\n",
    "        if len(models_dict) == 1:\n",
    "            axes = axes.reshape(1, -1)\n",
    "        \n",
    "        for model_idx, (model_key, model) in enumerate(models_dict.items()):\n",
    "            print(f\"ğŸ” åˆ†æ {MODEL_CONFIGS[model_key]['name']} çš„è§£é‡Šæ€§...\")\n",
    "            \n",
    "            try:\n",
    "                # åˆ›å»ºä¸€ä¸ªåŒ…è£…å™¨æ¥å¤„ç†å¤šGPUæ¨¡å‹\n",
    "                if hasattr(model, 'module'):\n",
    "                    # å¦‚æœæ˜¯DataParallelæ¨¡å‹ï¼Œä½¿ç”¨module\n",
    "                    analysis_model = model.module\n",
    "                else:\n",
    "                    analysis_model = model\n",
    "                \n",
    "                # ä½¿ç”¨Integrated Gradients\n",
    "                ig = IntegratedGradients(analysis_model)\n",
    "                \n",
    "                for img_idx in range(4):\n",
    "                    try:\n",
    "                        single_img = sample_imgs[img_idx:img_idx+1]\n",
    "                        single_label = sample_labels[img_idx:img_idx+1]\n",
    "                        \n",
    "                        # è®¡ç®—å½’å› \n",
    "                        if USE_AMP:\n",
    "                            # å¯¹äºè§£é‡Šæ€§åˆ†æï¼Œæš‚æ—¶ä¸ä½¿ç”¨AMPä»¥é¿å…å…¼å®¹æ€§é—®é¢˜\n",
    "                            attributions = ig.attribute(single_img, target=single_label)\n",
    "                        else:\n",
    "                            attributions = ig.attribute(single_img, target=single_label)\n",
    "                        \n",
    "                        # å¯è§†åŒ–\n",
    "                        attr_img = attributions.squeeze().cpu().detach().numpy()\n",
    "                        attr_img = np.transpose(attr_img, (1, 2, 0))\n",
    "                        \n",
    "                        # å½’ä¸€åŒ–åˆ°0-1\n",
    "                        if attr_img.max() != attr_img.min():\n",
    "                            attr_img = (attr_img - attr_img.min()) / (attr_img.max() - attr_img.min())\n",
    "                        else:\n",
    "                            attr_img = np.zeros_like(attr_img)\n",
    "                        \n",
    "                        axes[model_idx, img_idx].imshow(attr_img)\n",
    "                        axes[model_idx, img_idx].set_title(f'{MODEL_CONFIGS[model_key][\"name\"]}\\nSample {img_idx+1}')\n",
    "                        axes[model_idx, img_idx].axis('off')\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        print(f\"âš ï¸ æ ·æœ¬ {img_idx+1} åˆ†æå¤±è´¥: {e}\")\n",
    "                        axes[model_idx, img_idx].text(0.5, 0.5, f'Sample {img_idx+1}\\nFailed', \n",
    "                                                    ha='center', va='center', transform=axes[model_idx, img_idx].transAxes)\n",
    "                        axes[model_idx, img_idx].set_title(f'{MODEL_CONFIGS[model_key][\"name\"]}\\nSample {img_idx+1}')\n",
    "                        axes[model_idx, img_idx].axis('off')\n",
    "                        \n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ {MODEL_CONFIGS[model_key]['name']} æ•´ä½“è§£é‡Šæ€§åˆ†æå¤±è´¥: {e}\")\n",
    "                for img_idx in range(4):\n",
    "                    axes[model_idx, img_idx].text(0.5, 0.5, 'Analysis\\nFailed', \n",
    "                                                ha='center', va='center', transform=axes[model_idx, img_idx].transAxes)\n",
    "                    axes[model_idx, img_idx].set_title(f'{MODEL_CONFIGS[model_key][\"name\"]}\\nSample {img_idx+1}')\n",
    "                    axes[model_idx, img_idx].axis('off')\n",
    "        \n",
    "        plt.suptitle('Model Interpretability Analysis (Integrated Gradients)', fontsize=16, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        save_path = os.path.join(PLOTS_DIR, 'interpretability_analysis.png')\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f\"âœ… ä¿å­˜è§£é‡Šæ€§åˆ†æ: {save_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ è§£é‡Šæ€§åˆ†æå®Œå…¨å¤±è´¥: {e}\")\n",
    "        print(\"ğŸ’¡ å»ºè®®: æ£€æŸ¥Captumç‰ˆæœ¬æˆ–å°è¯•é‡æ–°å®‰è£…\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 8: é›†æˆé¢„æµ‹å‡½æ•° (æ”¯æŒAMP)\n",
    "def ensemble_predict(models_dict, data_loader, voting_type='soft', weights=None):\n",
    "    \"\"\"é›†æˆé¢„æµ‹å‡½æ•° - æ”¯æŒAMPæ··åˆç²¾åº¦\"\"\"\n",
    "    print(f\"ğŸ”® å¼€å§‹é›†æˆé¢„æµ‹ (æŠ•ç¥¨æ–¹å¼: {voting_type})\")\n",
    "    if USE_AMP:\n",
    "        print(\"âš¡ ä½¿ç”¨AMPåŠ é€Ÿé›†æˆé¢„æµ‹\")\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    model_outputs = {key: [] for key in models_dict.keys()}\n",
    "    \n",
    "    # å¦‚æœæ˜¯åŠ æƒæŠ•ç¥¨ä½†æ²¡æœ‰æä¾›æƒé‡ï¼Œåˆ™ä½¿ç”¨ç­‰æƒé‡\n",
    "    if voting_type == 'weighted' and weights is None:\n",
    "        weights = {key: 1.0 for key in models_dict.keys()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in tqdm(data_loader, desc=\"é›†æˆé¢„æµ‹\"):\n",
    "            imgs, labels = imgs.to(DEVICE, non_blocking=True), labels.to(DEVICE, non_blocking=True)\n",
    "            \n",
    "            # æ”¶é›†æ¯ä¸ªæ¨¡å‹çš„é¢„æµ‹\n",
    "            batch_predictions = []\n",
    "            for model_key, model in models_dict.items():\n",
    "                # AMPæ¨ç†\n",
    "                if USE_AMP:\n",
    "                    with autocast():\n",
    "                        outputs = model(imgs)\n",
    "                else:\n",
    "                    outputs = model(imgs)\n",
    "                \n",
    "                if voting_type in ['soft', 'weighted']:\n",
    "                    probs = torch.softmax(outputs, dim=1)\n",
    "                    batch_predictions.append(probs.cpu().numpy())\n",
    "                else:  # hard voting\n",
    "                    _, predicted = torch.max(outputs, 1)\n",
    "                    batch_predictions.append(predicted.cpu().numpy())\n",
    "                \n",
    "                model_outputs[model_key].extend(outputs.cpu().numpy())\n",
    "            \n",
    "            # é›†æˆé¢„æµ‹\n",
    "            if voting_type == 'soft':\n",
    "                # è½¯æŠ•ç¥¨ï¼šå¹³å‡æ¦‚ç‡\n",
    "                ensemble_probs = np.mean(batch_predictions, axis=0)\n",
    "                ensemble_pred = np.argmax(ensemble_probs, axis=1)\n",
    "            elif voting_type == 'weighted':\n",
    "                # åŠ æƒæŠ•ç¥¨ï¼šæ ¹æ®æƒé‡åŠ æƒå¹³å‡æ¦‚ç‡\n",
    "                weighted_probs = np.zeros_like(batch_predictions[0])\n",
    "                total_weight = 0\n",
    "                for i, (model_key, probs) in enumerate(zip(models_dict.keys(), batch_predictions)):\n",
    "                    weight = weights[model_key]\n",
    "                    weighted_probs += probs * weight\n",
    "                    total_weight += weight\n",
    "                ensemble_probs = weighted_probs / total_weight\n",
    "                ensemble_pred = np.argmax(ensemble_probs, axis=1)\n",
    "            else:\n",
    "                # ç¡¬æŠ•ç¥¨ï¼šå¤šæ•°æŠ•ç¥¨\n",
    "                batch_predictions = np.array(batch_predictions)\n",
    "                ensemble_pred = []\n",
    "                for i in range(batch_predictions.shape[1]):\n",
    "                    votes = batch_predictions[:, i]\n",
    "                    ensemble_pred.append(np.bincount(votes).argmax())\n",
    "                ensemble_pred = np.array(ensemble_pred)\n",
    "            \n",
    "            all_predictions.extend(ensemble_pred)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    return np.array(all_predictions), np.array(all_labels), model_outputs\n",
    "\n",
    "def calculate_model_weights(model_results, weight_method='accuracy'):\n",
    "    \"\"\"è®¡ç®—æ¨¡å‹æƒé‡\"\"\"\n",
    "    weights = {}\n",
    "    \n",
    "    if weight_method == 'accuracy':\n",
    "        # åŸºäºéªŒè¯å‡†ç¡®ç‡è®¡ç®—æƒé‡\n",
    "        accuracies = {key: results['best_acc'] for key, results in model_results.items()}\n",
    "        total_acc = sum(accuracies.values())\n",
    "        \n",
    "        for key, acc in accuracies.items():\n",
    "            weights[key] = acc / total_acc\n",
    "            \n",
    "    elif weight_method == 'softmax':\n",
    "        # ä½¿ç”¨softmaxå½’ä¸€åŒ–å‡†ç¡®ç‡ä½œä¸ºæƒé‡\n",
    "        accuracies = np.array([results['best_acc'] for results in model_results.values()])\n",
    "        softmax_weights = np.exp(accuracies * 10) / np.sum(np.exp(accuracies * 10))  # ä¹˜ä»¥10å¢å¼ºå·®å¼‚\n",
    "        \n",
    "        for i, key in enumerate(model_results.keys()):\n",
    "            weights[key] = softmax_weights[i]\n",
    "            \n",
    "    elif weight_method == 'rank':\n",
    "        # åŸºäºæ’åçš„æƒé‡åˆ†é…\n",
    "        sorted_models = sorted(model_results.items(), key=lambda x: x[1]['best_acc'], reverse=True)\n",
    "        n_models = len(sorted_models)\n",
    "        \n",
    "        for i, (key, _) in enumerate(sorted_models):\n",
    "            weights[key] = (n_models - i) / sum(range(1, n_models + 1))\n",
    "    \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åœ¨Cell 8åæ·»åŠ æ–°çš„Cell: æ¨¡å‹åŠ è½½å’Œè¾…åŠ©å‡½æ•°\n",
    "def load_trained_models(model_paths):\n",
    "    \"\"\"åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹\"\"\"\n",
    "    print(\"ğŸ“‚ åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹...\")\n",
    "    trained_models = {}\n",
    "    \n",
    "    for model_key, model_path in model_paths.items():\n",
    "        if os.path.exists(model_path):\n",
    "            # åˆ›å»ºæ¨¡å‹æ¶æ„\n",
    "            model = MODEL_CONFIGS[model_key]['create_fn']()\n",
    "            \n",
    "            # åŠ è½½æƒé‡\n",
    "            try:\n",
    "                model.load_state_dict(torch.load(model_path, map_location=DEVICE))\n",
    "                model = model.to(DEVICE)\n",
    "                model.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼\n",
    "                \n",
    "                # å¤šGPUæ”¯æŒ\n",
    "                if NUM_GPUS > 1:\n",
    "                    model = nn.DataParallel(model)\n",
    "                \n",
    "                trained_models[model_key] = model\n",
    "                print(f\"âœ… æˆåŠŸåŠ è½½ {MODEL_CONFIGS[model_key]['name']} ä» {model_path}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"âŒ åŠ è½½ {MODEL_CONFIGS[model_key]['name']} å¤±è´¥: {e}\")\n",
    "        else:\n",
    "            print(f\"âš ï¸ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}\")\n",
    "    \n",
    "    print(f\"ğŸ“Š æˆåŠŸåŠ è½½ {len(trained_models)}/{len(model_paths)} ä¸ªæ¨¡å‹\")\n",
    "    return trained_models\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, title, save_name):\n",
    "    \"\"\"ç»˜åˆ¶æ··æ·†çŸ©é˜µ\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=classes, yticklabels=classes)\n",
    "    plt.title(f'{title} - Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Predicted Label', fontweight='bold')\n",
    "    plt.ylabel('True Label', fontweight='bold')\n",
    "    \n",
    "    # æ·»åŠ å‡†ç¡®ç‡ä¿¡æ¯\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    plt.figtext(0.02, 0.02, f'Accuracy: {accuracy:.4f}', fontsize=12, \n",
    "                bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightblue\"))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    save_path = os.path.join(PLOTS_DIR, f'{save_name}_confusion_matrix.png')\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"âœ… ä¿å­˜æ··æ·†çŸ©é˜µ: {save_path}\")\n",
    "\n",
    "def plot_ensemble_analysis(models_dict, data_loader, device):\n",
    "    \"\"\"ç»˜åˆ¶é›†æˆåˆ†æå¯è§†åŒ–\"\"\"\n",
    "    print(\"ğŸ” ç”Ÿæˆé›†æˆåˆ†æå¯è§†åŒ–...\")\n",
    "    \n",
    "    # æ”¶é›†æ‰€æœ‰æ¨¡å‹çš„é¢„æµ‹ç»“æœ\n",
    "    model_predictions = {}\n",
    "    true_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in tqdm(data_loader, desc=\"æ”¶é›†æ¨¡å‹é¢„æµ‹\"):\n",
    "            imgs, labels = imgs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "            \n",
    "            if len(true_labels) == 0:  # åªåœ¨ç¬¬ä¸€æ¬¡æ”¶é›†çœŸå®æ ‡ç­¾\n",
    "                true_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "            for model_key, model in models_dict.items():\n",
    "                if model_key not in model_predictions:\n",
    "                    model_predictions[model_key] = []\n",
    "                \n",
    "                # AMPæ¨ç†\n",
    "                if USE_AMP:\n",
    "                    with autocast():\n",
    "                        outputs = model(imgs)\n",
    "                else:\n",
    "                    outputs = model(imgs)\n",
    "                \n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                model_predictions[model_key].extend(predicted.cpu().numpy())\n",
    "    \n",
    "    # è½¬æ¢ä¸ºnumpyæ•°ç»„\n",
    "    for key in model_predictions:\n",
    "        model_predictions[key] = np.array(model_predictions[key])\n",
    "    true_labels = np.array(true_labels)\n",
    "    \n",
    "    # 1. æ¨¡å‹é¢„æµ‹ä¸€è‡´æ€§çƒ­å›¾\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle('Ensemble Analysis Dashboard', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # è®¡ç®—æ¨¡å‹é—´ä¸€è‡´æ€§\n",
    "    model_keys = list(model_predictions.keys())\n",
    "    n_models = len(model_keys)\n",
    "    consistency_matrix = np.zeros((n_models, n_models))\n",
    "    \n",
    "    for i, key1 in enumerate(model_keys):\n",
    "        for j, key2 in enumerate(model_keys):\n",
    "            if i == j:\n",
    "                consistency_matrix[i, j] = 1.0\n",
    "            else:\n",
    "                agreement = np.mean(model_predictions[key1] == model_predictions[key2])\n",
    "                consistency_matrix[i, j] = agreement\n",
    "    \n",
    "    # ç»˜åˆ¶ä¸€è‡´æ€§çƒ­å›¾\n",
    "    model_names = [MODEL_CONFIGS[key]['name'] for key in model_keys]\n",
    "    sns.heatmap(consistency_matrix, annot=True, fmt='.3f', cmap='RdYlBu_r',\n",
    "                xticklabels=model_names, yticklabels=model_names, ax=axes[0, 0])\n",
    "    axes[0, 0].set_title('Model Prediction Consistency', fontweight='bold')\n",
    "    \n",
    "    # 2. å„æ¨¡å‹å‡†ç¡®ç‡å¯¹æ¯”\n",
    "    accuracies = []\n",
    "    for key in model_keys:\n",
    "        acc = accuracy_score(true_labels, model_predictions[key])\n",
    "        accuracies.append(acc)\n",
    "    \n",
    "    bars = axes[0, 1].bar(model_names, accuracies, color=['skyblue', 'lightcoral', 'lightgreen'][:len(model_names)])\n",
    "    axes[0, 1].set_title('Individual Model Accuracy', fontweight='bold')\n",
    "    axes[0, 1].set_ylabel('Accuracy')\n",
    "    axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # æ·»åŠ æ•°å€¼æ ‡ç­¾\n",
    "    for bar, acc in zip(bars, accuracies):\n",
    "        height = bar.get_height()\n",
    "        axes[0, 1].text(bar.get_x() + bar.get_width()/2., height + 0.005,\n",
    "                       f'{acc:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # 3. é›†æˆé¢„æµ‹ç½®ä¿¡åº¦åˆ†æ\n",
    "    # è®¡ç®—è½¯æŠ•ç¥¨ç»“æœ\n",
    "    all_probs = []\n",
    "    for imgs, labels in data_loader:\n",
    "        imgs = imgs.to(device, non_blocking=True)\n",
    "        batch_probs = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for model_key, model in models_dict.items():\n",
    "                if USE_AMP:\n",
    "                    with autocast():\n",
    "                        outputs = model(imgs)\n",
    "                else:\n",
    "                    outputs = model(imgs)\n",
    "                probs = torch.softmax(outputs, dim=1)\n",
    "                batch_probs.append(probs.cpu().numpy())\n",
    "        \n",
    "        # å¹³å‡æ¦‚ç‡\n",
    "        ensemble_probs = np.mean(batch_probs, axis=0)\n",
    "        all_probs.extend(ensemble_probs)\n",
    "    \n",
    "    all_probs = np.array(all_probs)\n",
    "    ensemble_confidence = np.max(all_probs, axis=1)\n",
    "    ensemble_predictions = np.argmax(all_probs, axis=1)\n",
    "    \n",
    "    # æ­£ç¡®vsé”™è¯¯é¢„æµ‹çš„ç½®ä¿¡åº¦åˆ†å¸ƒ\n",
    "    correct_mask = ensemble_predictions == true_labels\n",
    "    correct_confidence = ensemble_confidence[correct_mask]\n",
    "    incorrect_confidence = ensemble_confidence[~correct_mask]\n",
    "    \n",
    "    axes[1, 0].hist(correct_confidence, bins=30, alpha=0.7, label='Correct', color='green', density=True)\n",
    "    axes[1, 0].hist(incorrect_confidence, bins=30, alpha=0.7, label='Incorrect', color='red', density=True)\n",
    "    axes[1, 0].set_title('Prediction Confidence Distribution', fontweight='bold')\n",
    "    axes[1, 0].set_xlabel('Confidence')\n",
    "    axes[1, 0].set_ylabel('Density')\n",
    "    axes[1, 0].legend()\n",
    "    \n",
    "    # 4. å„ç±»åˆ«F1-scoreå¯¹æ¯”\n",
    "    f1_scores = {}\n",
    "    for key in model_keys:\n",
    "        f1 = f1_score(true_labels, model_predictions[key], average=None)\n",
    "        f1_scores[key] = f1\n",
    "    \n",
    "    x = np.arange(len(classes))\n",
    "    width = 0.25\n",
    "    \n",
    "    for i, (key, f1) in enumerate(f1_scores.items()):\n",
    "        offset = (i - len(model_keys)/2 + 0.5) * width\n",
    "        axes[1, 1].bar(x + offset, f1, width, label=MODEL_CONFIGS[key]['name'])\n",
    "    \n",
    "    axes[1, 1].set_title('F1-Score by Class', fontweight='bold')\n",
    "    axes[1, 1].set_xlabel('Class')\n",
    "    axes[1, 1].set_ylabel('F1-Score')\n",
    "    axes[1, 1].set_xticks(x)\n",
    "    axes[1, 1].set_xticklabels(classes)\n",
    "    axes[1, 1].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    save_path = os.path.join(PLOTS_DIR, 'ensemble_analysis.png')\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"âœ… ä¿å­˜é›†æˆåˆ†æ: {save_path}\")\n",
    "\n",
    "def plot_interpretability_analysis(models_dict, data_loader, device):\n",
    "    \"\"\"ç»˜åˆ¶æ¨¡å‹è§£é‡Šæ€§åˆ†æï¼ˆå¦‚æœCaptumå¯ç”¨ï¼‰\"\"\"\n",
    "    if not CAPTUM_AVAILABLE:\n",
    "        print(\"âš ï¸ Captumä¸å¯ç”¨ï¼Œè·³è¿‡è§£é‡Šæ€§åˆ†æ\")\n",
    "        return\n",
    "    \n",
    "    print(\"ğŸ” ç”Ÿæˆæ¨¡å‹è§£é‡Šæ€§åˆ†æ...\")\n",
    "    \n",
    "    # è·å–ä¸€ä¸ªæ‰¹æ¬¡çš„æ•°æ®è¿›è¡Œåˆ†æ\n",
    "    data_iter = iter(data_loader)\n",
    "    imgs, labels = next(data_iter)\n",
    "    imgs, labels = imgs.to(device), labels.to(device)\n",
    "    \n",
    "    # åªåˆ†æå‰4å¼ å›¾ç‰‡\n",
    "    sample_imgs = imgs[:4]\n",
    "    sample_labels = labels[:4]\n",
    "    \n",
    "    fig, axes = plt.subplots(len(models_dict), 4, figsize=(16, 4*len(models_dict)))\n",
    "    if len(models_dict) == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    for model_idx, (model_key, model) in enumerate(models_dict.items()):\n",
    "        try:\n",
    "            # ä½¿ç”¨Integrated Gradients\n",
    "            ig = IntegratedGradients(model)\n",
    "            \n",
    "            for img_idx in range(4):\n",
    "                single_img = sample_imgs[img_idx:img_idx+1]\n",
    "                single_label = sample_labels[img_idx:img_idx+1]\n",
    "                \n",
    "                # è®¡ç®—å½’å› \n",
    "                if USE_AMP:\n",
    "                    with autocast():\n",
    "                        attributions = ig.attribute(single_img, target=single_label)\n",
    "                else:\n",
    "                    attributions = ig.attribute(single_img, target=single_label)\n",
    "                \n",
    "                # å¯è§†åŒ–\n",
    "                attr_img = attributions.squeeze().cpu().detach().numpy()\n",
    "                attr_img = np.transpose(attr_img, (1, 2, 0))\n",
    "                \n",
    "                # å½’ä¸€åŒ–åˆ°0-1\n",
    "                attr_img = (attr_img - attr_img.min()) / (attr_img.max() - attr_img.min())\n",
    "                \n",
    "                axes[model_idx, img_idx].imshow(attr_img)\n",
    "                axes[model_idx, img_idx].set_title(f'{MODEL_CONFIGS[model_key][\"name\"]}\\nSample {img_idx+1}')\n",
    "                axes[model_idx, img_idx].axis('off')\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ {MODEL_CONFIGS[model_key]['name']} è§£é‡Šæ€§åˆ†æå¤±è´¥: {e}\")\n",
    "            for img_idx in range(4):\n",
    "                axes[model_idx, img_idx].text(0.5, 0.5, 'Analysis\\nFailed', \n",
    "                                            ha='center', va='center', transform=axes[model_idx, img_idx].transAxes)\n",
    "                axes[model_idx, img_idx].set_title(f'{MODEL_CONFIGS[model_key][\"name\"]}\\nSample {img_idx+1}')\n",
    "    \n",
    "    plt.suptitle('Model Interpretability Analysis (Integrated Gradients)', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    save_path = os.path.join(PLOTS_DIR, 'interpretability_analysis.png')\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"âœ… ä¿å­˜è§£é‡Šæ€§åˆ†æ: {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ åŠ è½½æ•°æ®é›†...\n",
      "è®­ç»ƒé›†å›¾ç‰‡æ•°: 140002\n",
      "è®­ç»ƒé›†ç±»åˆ«åˆ†å¸ƒ:\n",
      "  Real: 70001 (50.0%)\n",
      "  Fake: 70001 (50.0%)\n",
      "éªŒè¯é›†å›¾ç‰‡æ•°: 39428\n",
      "éªŒè¯é›†ç±»åˆ«åˆ†å¸ƒ:\n",
      "  Real: 19787 (50.2%)\n",
      "  Fake: 19641 (49.8%)\n",
      "âš ï¸ éªŒè¯é›†è¿‡å¤§ (39428 å¼ )ï¼Œéšæœºé‡‡æ · 12800 å¼ å›¾ç‰‡\n",
      "âœ… éªŒè¯é›†é‡‡æ ·å®Œæˆï¼Œå½“å‰å¤§å°: 12800\n",
      "éªŒè¯é›†ç±»åˆ«åˆ†å¸ƒ:\n",
      "  Real: 6400 (50.0%)\n",
      "  Fake: 6400 (50.0%)\n",
      "\n",
      "ğŸ“Š æ•°æ®é›†æ€»è§ˆ:\n",
      "è®­ç»ƒé›†æ€»æ•°: 140002\n",
      "éªŒè¯é›†æ€»æ•°: 12800\n",
      "éªŒè¯æ‰¹æ¬¡æ•°: 400\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: åŠ è½½æ•°æ®\n",
    "print(\"ğŸ“‚ åŠ è½½æ•°æ®é›†...\")\n",
    "train_df = create_dataframe(TRAIN_PATH, \"è®­ç»ƒ\")\n",
    "val_df = create_dataframe(VAL_PATH, \"éªŒè¯\")\n",
    "\n",
    "# é™åˆ¶éªŒè¯é›†å¤§å°ä¸º6400ä»¥å‡å°‘å†…å­˜ä½¿ç”¨\n",
    "MAX_VAL_SAMPLES = 12800\n",
    "if len(val_df) > MAX_VAL_SAMPLES:\n",
    "    print(f\"âš ï¸ éªŒè¯é›†è¿‡å¤§ ({len(val_df)} å¼ )ï¼Œéšæœºé‡‡æ · {MAX_VAL_SAMPLES} å¼ å›¾ç‰‡\")\n",
    "    # ä¿æŒç±»åˆ«å¹³è¡¡çš„éšæœºé‡‡æ ·\n",
    "    val_df = val_df.groupby('label', group_keys=False).apply(\n",
    "        lambda x: x.sample(min(len(x), MAX_VAL_SAMPLES//2), random_state=42)\n",
    "    ).reset_index(drop=True)\n",
    "    print(f\"âœ… éªŒè¯é›†é‡‡æ ·å®Œæˆï¼Œå½“å‰å¤§å°: {len(val_df)}\")\n",
    "    print(f\"éªŒè¯é›†ç±»åˆ«åˆ†å¸ƒ:\")\n",
    "    for idx, cls in enumerate(classes):\n",
    "        count = len(val_df[val_df['label'] == idx])\n",
    "        print(f\"  {cls}: {count} ({count/len(val_df)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nğŸ“Š æ•°æ®é›†æ€»è§ˆ:\")\n",
    "print(f\"è®­ç»ƒé›†æ€»æ•°: {len(train_df)}\")\n",
    "print(f\"éªŒè¯é›†æ€»æ•°: {len(val_df)}\")\n",
    "print(f\"éªŒè¯æ‰¹æ¬¡æ•°: {len(val_df) // BATCH_SIZE + (1 if len(val_df) % BATCH_SIZE > 0 else 0)}\")\n",
    "\n",
    "# åˆ›å»ºæ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨\n",
    "train_dataset = DeepfakeDataset(train_df, transform=train_transform)\n",
    "val_dataset = DeepfakeDataset(val_df, transform=val_transform)\n",
    "\n",
    "# ä½¿ç”¨åŠ¨æ€é…ç½®çš„num_workerså’Œpin_memory\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸš€ å¼€å§‹è®­ç»ƒå¤šä¸ªæ¨¡å‹...\n",
      "\n",
      "ğŸ”¥ Starting Training EfficientNet-B0\n",
      "âš¡ Using AMP (Automatic Mixed Precision) for faster training\n",
      "ğŸ” Initial GPU Memory: 0.02GB / 0.02GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/15 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4376/4376 [18:03<00:00,  4.04it/s]\n",
      "Epoch 1/15 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 400/400 [00:34<00:00, 11.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 0.0794, Val Loss: 0.0785, Val Acc: 0.9720, Val F1: 0.9720, LR: 0.000100 GPU Mem: 0.10GB/1.84GB\n",
      "âœ… Best model saved, validation accuracy: 0.9720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/15 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4376/4376 [15:39<00:00,  4.66it/s]\n",
      "Epoch 2/15 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 400/400 [00:20<00:00, 19.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss: 0.0444, Val Loss: 0.0575, Val Acc: 0.9798, Val F1: 0.9798, LR: 0.000100 GPU Mem: 0.10GB/1.84GB\n",
      "âœ… Best model saved, validation accuracy: 0.9798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/15 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4376/4376 [15:41<00:00,  4.65it/s]\n",
      "Epoch 3/15 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 400/400 [00:21<00:00, 18.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss: 0.0365, Val Loss: 0.0507, Val Acc: 0.9809, Val F1: 0.9809, LR: 0.000100 GPU Mem: 0.10GB/1.84GB\n",
      "âœ… Best model saved, validation accuracy: 0.9809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/15 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4376/4376 [15:35<00:00,  4.68it/s]\n",
      "Epoch 4/15 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 400/400 [00:20<00:00, 19.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Loss: 0.0336, Val Loss: 0.0632, Val Acc: 0.9778, Val F1: 0.9778, LR: 0.000100 GPU Mem: 0.10GB/1.84GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/15 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4376/4376 [15:42<00:00,  4.64it/s]\n",
      "Epoch 5/15 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 400/400 [00:21<00:00, 18.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Loss: 0.0323, Val Loss: 0.0488, Val Acc: 0.9822, Val F1: 0.9822, LR: 0.000100 GPU Mem: 0.10GB/1.84GB\n",
      "âœ… Best model saved, validation accuracy: 0.9822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/15 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4376/4376 [15:31<00:00,  4.70it/s]\n",
      "Epoch 6/15 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 400/400 [00:20<00:00, 19.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Loss: 0.0307, Val Loss: 0.0417, Val Acc: 0.9862, Val F1: 0.9862, LR: 0.000100 GPU Mem: 0.10GB/1.84GB\n",
      "âœ… Best model saved, validation accuracy: 0.9862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/15 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4376/4376 [15:30<00:00,  4.70it/s]\n",
      "Epoch 7/15 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 400/400 [00:20<00:00, 19.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Train Loss: 0.0293, Val Loss: 0.0527, Val Acc: 0.9809, Val F1: 0.9809, LR: 0.000100 GPU Mem: 0.10GB/1.84GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/15 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4376/4376 [15:35<00:00,  4.68it/s]\n",
      "Epoch 8/15 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 400/400 [00:20<00:00, 19.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Train Loss: 0.0290, Val Loss: 0.0382, Val Acc: 0.9869, Val F1: 0.9869, LR: 0.000100 GPU Mem: 0.10GB/1.84GB\n",
      "âœ… Best model saved, validation accuracy: 0.9869\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/15 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4376/4376 [15:45<00:00,  4.63it/s]\n",
      "Epoch 9/15 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 400/400 [00:21<00:00, 18.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Train Loss: 0.0277, Val Loss: 0.0514, Val Acc: 0.9835, Val F1: 0.9835, LR: 0.000100 GPU Mem: 0.10GB/1.84GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/15 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4376/4376 [15:44<00:00,  4.64it/s]\n",
      "Epoch 10/15 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 400/400 [00:20<00:00, 19.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Train Loss: 0.0274, Val Loss: 0.0526, Val Acc: 0.9816, Val F1: 0.9816, LR: 0.000100 GPU Mem: 0.10GB/1.84GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/15 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4376/4376 [15:41<00:00,  4.65it/s]\n",
      "Epoch 11/15 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 400/400 [00:20<00:00, 19.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: Train Loss: 0.0279, Val Loss: 0.0375, Val Acc: 0.9863, Val F1: 0.9863, LR: 0.000100 GPU Mem: 0.10GB/1.84GB\n",
      "â›” Early stopping triggered\n",
      "â±ï¸ Training completed in 10715.97 seconds\n",
      "ğŸ§¹ Final GPU Memory: 0.10GB\n",
      "âœ… EfficientNet-B0 è®­ç»ƒå®Œæˆï¼Œæœ€ä½³éªŒè¯å‡†ç¡®ç‡: 0.9869\n",
      "\n",
      "ğŸ”¥ Starting Training ResNet18\n",
      "âš¡ Using AMP (Automatic Mixed Precision) for faster training\n",
      "ğŸ” Initial GPU Memory: 0.06GB / 1.84GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/15 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4376/4376 [13:36<00:00,  5.36it/s]\n",
      "Epoch 1/15 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 400/400 [00:18<00:00, 21.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 0.0784, Val Loss: 0.0955, Val Acc: 0.9673, Val F1: 0.9673, LR: 0.000100 GPU Mem: 0.21GB/1.84GB\n",
      "âœ… Best model saved, validation accuracy: 0.9673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/15 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4376/4376 [13:39<00:00,  5.34it/s]\n",
      "Epoch 2/15 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 400/400 [00:19<00:00, 20.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss: 0.0500, Val Loss: 0.0900, Val Acc: 0.9710, Val F1: 0.9710, LR: 0.000100 GPU Mem: 0.21GB/1.84GB\n",
      "âœ… Best model saved, validation accuracy: 0.9710\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/15 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4376/4376 [13:42<00:00,  5.32it/s]\n",
      "Epoch 3/15 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 400/400 [00:18<00:00, 21.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss: 0.0436, Val Loss: 0.0779, Val Acc: 0.9727, Val F1: 0.9727, LR: 0.000100 GPU Mem: 0.21GB/1.84GB\n",
      "âœ… Best model saved, validation accuracy: 0.9727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/15 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4376/4376 [13:38<00:00,  5.34it/s]\n",
      "Epoch 4/15 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 400/400 [00:18<00:00, 21.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Loss: 0.0408, Val Loss: 0.0614, Val Acc: 0.9780, Val F1: 0.9780, LR: 0.000100 GPU Mem: 0.21GB/1.84GB\n",
      "âœ… Best model saved, validation accuracy: 0.9780\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/15 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4376/4376 [13:36<00:00,  5.36it/s]\n",
      "Epoch 5/15 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 400/400 [00:18<00:00, 21.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Loss: 0.0381, Val Loss: 0.0532, Val Acc: 0.9832, Val F1: 0.9832, LR: 0.000100 GPU Mem: 0.21GB/1.84GB\n",
      "âœ… Best model saved, validation accuracy: 0.9832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/15 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4376/4376 [13:36<00:00,  5.36it/s]\n",
      "Epoch 6/15 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 400/400 [00:18<00:00, 21.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Loss: 0.0362, Val Loss: 0.0674, Val Acc: 0.9780, Val F1: 0.9780, LR: 0.000100 GPU Mem: 0.21GB/1.84GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/15 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4376/4376 [13:37<00:00,  5.36it/s]\n",
      "Epoch 7/15 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 400/400 [00:19<00:00, 20.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Train Loss: 0.0338, Val Loss: 0.0550, Val Acc: 0.9812, Val F1: 0.9812, LR: 0.000100 GPU Mem: 0.21GB/1.84GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/15 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4376/4376 [13:39<00:00,  5.34it/s]\n",
      "Epoch 8/15 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 400/400 [00:18<00:00, 21.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Train Loss: 0.0327, Val Loss: 0.0493, Val Acc: 0.9833, Val F1: 0.9833, LR: 0.000100 GPU Mem: 0.21GB/1.84GB\n",
      "âœ… Best model saved, validation accuracy: 0.9833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/15 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4376/4376 [13:24<00:00,  5.44it/s]\n",
      "Epoch 9/15 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 400/400 [00:18<00:00, 21.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Train Loss: 0.0314, Val Loss: 0.0466, Val Acc: 0.9844, Val F1: 0.9844, LR: 0.000100 GPU Mem: 0.21GB/1.84GB\n",
      "âœ… Best model saved, validation accuracy: 0.9844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/15 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4376/4376 [13:29<00:00,  5.41it/s]\n",
      "Epoch 10/15 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 400/400 [00:18<00:00, 21.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Train Loss: 0.0305, Val Loss: 0.0490, Val Acc: 0.9818, Val F1: 0.9818, LR: 0.000100 GPU Mem: 0.21GB/1.84GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/15 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4376/4376 [13:28<00:00,  5.41it/s]\n",
      "Epoch 11/15 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 400/400 [00:18<00:00, 21.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: Train Loss: 0.0295, Val Loss: 0.0671, Val Acc: 0.9770, Val F1: 0.9770, LR: 0.000100 GPU Mem: 0.21GB/1.84GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/15 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4376/4376 [13:30<00:00,  5.40it/s]\n",
      "Epoch 12/15 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 400/400 [00:19<00:00, 20.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: Train Loss: 0.0287, Val Loss: 0.0530, Val Acc: 0.9814, Val F1: 0.9814, LR: 0.000100 GPU Mem: 0.21GB/1.84GB\n",
      "â›” Early stopping triggered\n",
      "â±ï¸ Training completed in 10005.60 seconds\n",
      "ğŸ§¹ Final GPU Memory: 0.21GB\n",
      "âœ… ResNet18 è®­ç»ƒå®Œæˆï¼Œæœ€ä½³éªŒè¯å‡†ç¡®ç‡: 0.9844\n",
      "\n",
      "ğŸ”¥ Starting Training ConvNeXt-Tiny\n",
      "âš¡ Using AMP (Automatic Mixed Precision) for faster training\n",
      "ğŸ” Initial GPU Memory: 0.12GB / 1.84GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/15 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4376/4376 [20:20<00:00,  3.59it/s]\n",
      "Epoch 1/15 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 400/400 [00:30<00:00, 13.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 0.0696, Val Loss: 0.0718, Val Acc: 0.9726, Val F1: 0.9726, LR: 0.000100 GPU Mem: 0.47GB/2.81GB\n",
      "âœ… Best model saved, validation accuracy: 0.9726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/15 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4376/4376 [20:23<00:00,  3.58it/s]\n",
      "Epoch 2/15 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 400/400 [00:30<00:00, 13.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss: 0.0463, Val Loss: 0.0520, Val Acc: 0.9812, Val F1: 0.9812, LR: 0.000100 GPU Mem: 0.47GB/2.81GB\n",
      "âœ… Best model saved, validation accuracy: 0.9812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/15 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4376/4376 [20:26<00:00,  3.57it/s]\n",
      "Epoch 3/15 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 400/400 [00:31<00:00, 12.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss: 0.0427, Val Loss: 0.0733, Val Acc: 0.9752, Val F1: 0.9752, LR: 0.000100 GPU Mem: 0.47GB/2.81GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/15 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4376/4376 [20:13<00:00,  3.61it/s]\n",
      "Epoch 4/15 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 400/400 [00:30<00:00, 13.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Loss: 0.0410, Val Loss: 0.0940, Val Acc: 0.9661, Val F1: 0.9661, LR: 0.000100 GPU Mem: 0.47GB/2.81GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/15 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4376/4376 [20:30<00:00,  3.56it/s]\n",
      "Epoch 5/15 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 400/400 [00:30<00:00, 13.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Loss: 0.0385, Val Loss: 0.0668, Val Acc: 0.9752, Val F1: 0.9752, LR: 0.000100 GPU Mem: 0.47GB/2.81GB\n",
      "â›” Early stopping triggered\n",
      "â±ï¸ Training completed in 6268.30 seconds\n",
      "ğŸ§¹ Final GPU Memory: 0.47GB\n",
      "âœ… ConvNeXt-Tiny è®­ç»ƒå®Œæˆï¼Œæœ€ä½³éªŒè¯å‡†ç¡®ç‡: 0.9812\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: è®­ç»ƒæ‰€æœ‰æ¨¡å‹\n",
    "print(\"\\nğŸš€ å¼€å§‹è®­ç»ƒå¤šä¸ªæ¨¡å‹...\")\n",
    "\n",
    "# é€‰æ‹©è¦è®­ç»ƒçš„æ¨¡å‹ï¼ˆå¯ä»¥æ ¹æ®éœ€è¦è°ƒæ•´ï¼‰\n",
    "selected_models = ['efficientnet_b0', 'resnet18', 'convnext_tiny']  # å‡å°‘æ¨¡å‹æ•°é‡ä»¥é€‚åº”Kaggleç¯å¢ƒ\n",
    "model_paths = {}\n",
    "model_results = {}\n",
    "\n",
    "for model_key in selected_models:\n",
    "    save_path = f\"best_{model_key}_model.pth\"\n",
    "    model_paths[model_key] = save_path\n",
    "    \n",
    "    # ä½¿ç”¨æ–°çš„è®­ç»ƒå‡½æ•°è¿”å›æ ¼å¼\n",
    "    model_results[model_key] = train_single_model(\n",
    "        model_key, train_loader, val_loader, save_path\n",
    "    )\n",
    "    \n",
    "    print(f\"âœ… {MODEL_CONFIGS[model_key]['name']} è®­ç»ƒå®Œæˆï¼Œæœ€ä½³éªŒè¯å‡†ç¡®ç‡: {model_results[model_key]['best_acc']:.4f}\")\n",
    "    \n",
    "    # æ¸…ç†GPUå†…å­˜\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š ç”Ÿæˆè®­ç»ƒå†å²å¯è§†åŒ–...\n",
      "ğŸ“Š Generating training history visualizations...\n",
      "âœ… Saved: ./works/plots\\efficientnet_b0_training_history.png\n",
      "âœ… Saved: ./works/plots\\resnet18_training_history.png\n",
      "âœ… Saved: ./works/plots\\convnext_tiny_training_history.png\n",
      "âœ… Saved: ./works/plots\\multi_model_comparison.png\n"
     ]
    }
   ],
   "source": [
    "# Cell 11: å¢å¼ºè®­ç»ƒå†å²å¯è§†åŒ–\n",
    "print(\"\\nğŸ“Š ç”Ÿæˆè®­ç»ƒå†å²å¯è§†åŒ–...\")\n",
    "plot_training_history(model_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”® å¼€å§‹é›†æˆé¢„æµ‹...\n",
      "ğŸ“‚ åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹...\n",
      "âœ… æˆåŠŸåŠ è½½ EfficientNet-B0 ä» best_efficientnet_b0_model.pth\n",
      "âœ… æˆåŠŸåŠ è½½ ResNet18 ä» best_resnet18_model.pth\n",
      "âœ… æˆåŠŸåŠ è½½ ConvNeXt-Tiny ä» best_convnext_tiny_model.pth\n",
      "ğŸ“Š æˆåŠŸåŠ è½½ 3/3 ä¸ªæ¨¡å‹\n",
      "\n",
      "âš–ï¸ è®¡ç®—æ¨¡å‹æƒé‡...\n",
      "æ¨¡å‹æƒé‡åˆ†é…:\n",
      "  EfficientNet-B0: 0.3343\n",
      "  ResNet18       : 0.3334\n",
      "  ConvNeXt-Tiny  : 0.3323\n",
      "\n",
      "ğŸ“Š è½¯æŠ•ç¥¨é›†æˆé¢„æµ‹:\n",
      "ğŸ”® å¼€å§‹é›†æˆé¢„æµ‹ (æŠ•ç¥¨æ–¹å¼: soft)\n",
      "âš¡ ä½¿ç”¨AMPåŠ é€Ÿé›†æˆé¢„æµ‹\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "é›†æˆé¢„æµ‹: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 400/400 [00:44<00:00,  8.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "è½¯æŠ•ç¥¨å‡†ç¡®ç‡: 0.9894\n",
      "\n",
      "ğŸ“Š ç¡¬æŠ•ç¥¨é›†æˆé¢„æµ‹:\n",
      "ğŸ”® å¼€å§‹é›†æˆé¢„æµ‹ (æŠ•ç¥¨æ–¹å¼: hard)\n",
      "âš¡ ä½¿ç”¨AMPåŠ é€Ÿé›†æˆé¢„æµ‹\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "é›†æˆé¢„æµ‹: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 400/400 [00:43<00:00,  9.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ç¡¬æŠ•ç¥¨å‡†ç¡®ç‡: 0.9889\n",
      "\n",
      "ğŸ“Š åŠ æƒæŠ•ç¥¨é›†æˆé¢„æµ‹:\n",
      "ğŸ”® å¼€å§‹é›†æˆé¢„æµ‹ (æŠ•ç¥¨æ–¹å¼: weighted)\n",
      "âš¡ ä½¿ç”¨AMPåŠ é€Ÿé›†æˆé¢„æµ‹\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "é›†æˆé¢„æµ‹: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 400/400 [00:43<00:00,  9.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åŠ æƒæŠ•ç¥¨å‡†ç¡®ç‡: 0.9894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 12: é›†æˆé¢„æµ‹å’Œè¯„ä¼°\n",
    "print(\"\\nğŸ”® å¼€å§‹é›†æˆé¢„æµ‹...\")\n",
    "\n",
    "# åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹\n",
    "trained_models = load_trained_models(model_paths)\n",
    "\n",
    "# è®¡ç®—æ¨¡å‹æƒé‡\n",
    "print(\"\\nâš–ï¸ è®¡ç®—æ¨¡å‹æƒé‡...\")\n",
    "model_weights = calculate_model_weights(model_results, weight_method='accuracy')\n",
    "print(\"æ¨¡å‹æƒé‡åˆ†é…:\")\n",
    "for model_key, weight in model_weights.items():\n",
    "    print(f\"  {MODEL_CONFIGS[model_key]['name']:15}: {weight:.4f}\")\n",
    "\n",
    "# è½¯æŠ•ç¥¨é¢„æµ‹\n",
    "print(\"\\nğŸ“Š è½¯æŠ•ç¥¨é›†æˆé¢„æµ‹:\")\n",
    "soft_predictions, true_labels, _ = ensemble_predict(trained_models, val_loader, voting_type='soft')\n",
    "soft_accuracy = accuracy_score(true_labels, soft_predictions)\n",
    "print(f\"è½¯æŠ•ç¥¨å‡†ç¡®ç‡: {soft_accuracy:.4f}\")\n",
    "\n",
    "# ç¡¬æŠ•ç¥¨é¢„æµ‹\n",
    "print(\"\\nğŸ“Š ç¡¬æŠ•ç¥¨é›†æˆé¢„æµ‹:\")\n",
    "hard_predictions, _, _ = ensemble_predict(trained_models, val_loader, voting_type='hard')\n",
    "hard_accuracy = accuracy_score(true_labels, hard_predictions)\n",
    "print(f\"ç¡¬æŠ•ç¥¨å‡†ç¡®ç‡: {hard_accuracy:.4f}\")\n",
    "\n",
    "# åŠ æƒæŠ•ç¥¨é¢„æµ‹\n",
    "print(\"\\nğŸ“Š åŠ æƒæŠ•ç¥¨é›†æˆé¢„æµ‹:\")\n",
    "weighted_predictions, _, _ = ensemble_predict(trained_models, val_loader, voting_type='weighted', weights=model_weights)\n",
    "weighted_accuracy = accuracy_score(true_labels, weighted_predictions)\n",
    "print(f\"åŠ æƒæŠ•ç¥¨å‡†ç¡®ç‡: {weighted_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“ˆ æ¨¡å‹æ€§èƒ½å¯¹æ¯”:\n",
      "==================================================\n",
      "EfficientNet-B0: 0.9869\n",
      "ResNet18       : 0.9844\n",
      "ConvNeXt-Tiny  : 0.9812\n",
      "è½¯æŠ•ç¥¨é›†æˆ          : 0.9894\n",
      "ç¡¬æŠ•ç¥¨é›†æˆ          : 0.9889\n",
      "åŠ æƒæŠ•ç¥¨é›†æˆ         : 0.9894\n",
      "\n",
      "ğŸ“Š ç”Ÿæˆæ··æ·†çŸ©é˜µå¯è§†åŒ–...\n",
      "âœ… ä¿å­˜æ··æ·†çŸ©é˜µ: ./works/plots\\soft_voting_confusion_matrix.png\n",
      "âœ… ä¿å­˜æ··æ·†çŸ©é˜µ: ./works/plots\\hard_voting_confusion_matrix.png\n",
      "âœ… ä¿å­˜æ··æ·†çŸ©é˜µ: ./works/plots\\weighted_voting_confusion_matrix.png\n"
     ]
    }
   ],
   "source": [
    "# Cell 13: ç»“æœå¯¹æ¯”å’Œå¯è§†åŒ–\n",
    "# å•æ¨¡å‹ç»“æœå¯¹æ¯”\n",
    "print(\"\\nğŸ“ˆ æ¨¡å‹æ€§èƒ½å¯¹æ¯”:\")\n",
    "print(\"=\"*50)\n",
    "for model_key in selected_models:\n",
    "    best_acc = model_results[model_key]['best_acc']\n",
    "    print(f\"{MODEL_CONFIGS[model_key]['name']:15}: {best_acc:.4f}\")\n",
    "\n",
    "print(f\"{'è½¯æŠ•ç¥¨é›†æˆ':15}: {soft_accuracy:.4f}\")\n",
    "print(f\"{'ç¡¬æŠ•ç¥¨é›†æˆ':15}: {hard_accuracy:.4f}\")\n",
    "print(f\"{'åŠ æƒæŠ•ç¥¨é›†æˆ':15}: {weighted_accuracy:.4f}\")\n",
    "\n",
    "# å¢å¼ºæ··æ·†çŸ©é˜µå¯è§†åŒ–\n",
    "print(\"\\nğŸ“Š ç”Ÿæˆæ··æ·†çŸ©é˜µå¯è§†åŒ–...\")\n",
    "plot_confusion_matrix(true_labels, soft_predictions, \"Soft Voting Ensemble\", \"soft_voting\")\n",
    "plot_confusion_matrix(true_labels, hard_predictions, \"Hard Voting Ensemble\", \"hard_voting\")\n",
    "plot_confusion_matrix(true_labels, weighted_predictions, \"Weighted Voting Ensemble\", \"weighted_voting\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“‹ è½¯æŠ•ç¥¨è¯¦ç»†åˆ†ç±»æŠ¥å‘Š:\n",
      "==================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real       0.99      0.99      0.99      6400\n",
      "        Fake       0.99      0.99      0.99      6400\n",
      "\n",
      "    accuracy                           0.99     12800\n",
      "   macro avg       0.99      0.99      0.99     12800\n",
      "weighted avg       0.99      0.99      0.99     12800\n",
      "\n",
      "\n",
      "ğŸ“‹ ç¡¬æŠ•ç¥¨è¯¦ç»†åˆ†ç±»æŠ¥å‘Š:\n",
      "==================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real       0.99      0.99      0.99      6400\n",
      "        Fake       0.99      0.99      0.99      6400\n",
      "\n",
      "    accuracy                           0.99     12800\n",
      "   macro avg       0.99      0.99      0.99     12800\n",
      "weighted avg       0.99      0.99      0.99     12800\n",
      "\n",
      "\n",
      "ğŸ“‹ åŠ æƒæŠ•ç¥¨è¯¦ç»†åˆ†ç±»æŠ¥å‘Š:\n",
      "==================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real       0.99      0.99      0.99      6400\n",
      "        Fake       0.99      0.99      0.99      6400\n",
      "\n",
      "    accuracy                           0.99     12800\n",
      "   macro avg       0.99      0.99      0.99     12800\n",
      "weighted avg       0.99      0.99      0.99     12800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 14: è¯¦ç»†åˆ†ç±»æŠ¥å‘Š\n",
    "print(\"\\nğŸ“‹ è½¯æŠ•ç¥¨è¯¦ç»†åˆ†ç±»æŠ¥å‘Š:\")\n",
    "print(\"=\"*50)\n",
    "print(classification_report(true_labels, soft_predictions, target_names=classes))\n",
    "\n",
    "print(\"\\nğŸ“‹ ç¡¬æŠ•ç¥¨è¯¦ç»†åˆ†ç±»æŠ¥å‘Š:\")\n",
    "print(\"=\"*50)\n",
    "print(classification_report(true_labels, hard_predictions, target_names=classes))\n",
    "\n",
    "print(\"\\nğŸ“‹ åŠ æƒæŠ•ç¥¨è¯¦ç»†åˆ†ç±»æŠ¥å‘Š:\")\n",
    "print(\"=\"*50)\n",
    "print(classification_report(true_labels, weighted_predictions, target_names=classes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š ç”Ÿæˆé›†æˆåˆ†æå¯è§†åŒ–...\n",
      "ğŸ” ç”Ÿæˆé›†æˆåˆ†æå¯è§†åŒ–...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "æ”¶é›†æ¨¡å‹é¢„æµ‹: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 400/400 [00:42<00:00,  9.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š æ•°æ®éªŒè¯: çœŸå®æ ‡ç­¾æ•°é‡=12800\n",
      "ğŸ“Š EfficientNet-B0 é¢„æµ‹æ•°é‡=12800\n",
      "ğŸ“Š ResNet18 é¢„æµ‹æ•°é‡=12800\n",
      "ğŸ“Š ConvNeXt-Tiny é¢„æµ‹æ•°é‡=12800\n",
      "ğŸ“Š EfficientNet-B0 å‡†ç¡®ç‡: 0.9869\n",
      "ğŸ“Š ResNet18 å‡†ç¡®ç‡: 0.9844\n",
      "ğŸ“Š ConvNeXt-Tiny å‡†ç¡®ç‡: 0.9812\n",
      "ğŸ” è®¡ç®—é›†æˆé¢„æµ‹ç½®ä¿¡åº¦...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "è®¡ç®—ç½®ä¿¡åº¦: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 400/400 [00:43<00:00,  9.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ä¿å­˜é›†æˆåˆ†æ: ./works/plots\\ensemble_analysis.png\n",
      "\n",
      "ğŸ“Š ç”Ÿæˆæ¨¡å‹è§£é‡Šæ€§åˆ†æ...\n",
      "ğŸ” ç”Ÿæˆæ¨¡å‹è§£é‡Šæ€§åˆ†æ...\n",
      "ğŸ” åˆ†æ EfficientNet-B0 çš„è§£é‡Šæ€§...\n",
      "ğŸ” åˆ†æ ResNet18 çš„è§£é‡Šæ€§...\n",
      "ğŸ” åˆ†æ ConvNeXt-Tiny çš„è§£é‡Šæ€§...\n",
      "âœ… ä¿å­˜è§£é‡Šæ€§åˆ†æ: ./works/plots\\interpretability_analysis.png\n"
     ]
    }
   ],
   "source": [
    "# Cell 15: é›†æˆåˆ†æå’Œè§£é‡Šæ€§å¯è§†åŒ–\n",
    "print(\"\\nğŸ“Š ç”Ÿæˆé›†æˆåˆ†æå¯è§†åŒ–...\")\n",
    "plot_ensemble_analysis(trained_models, val_loader, DEVICE)\n",
    "\n",
    "print(\"\\nğŸ“Š ç”Ÿæˆæ¨¡å‹è§£é‡Šæ€§åˆ†æ...\")\n",
    "plot_interpretability_analysis(trained_models, val_loader, DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ğŸ‰ å¤šæ¨¡å‹é›†æˆè®­ç»ƒå®Œæˆï¼\n",
      "============================================================\n",
      "è®­ç»ƒçš„æ¨¡å‹æ•°é‡: 3\n",
      "æœ€ä½³å•æ¨¡å‹å‡†ç¡®ç‡: 0.9869\n",
      "è½¯æŠ•ç¥¨é›†æˆå‡†ç¡®ç‡: 0.9894\n",
      "ç¡¬æŠ•ç¥¨é›†æˆå‡†ç¡®ç‡: 0.9889\n",
      "åŠ æƒæŠ•ç¥¨é›†æˆå‡†ç¡®ç‡: 0.9894\n",
      "è½¯æŠ•ç¥¨ç›¸å¯¹æå‡: +0.25%\n",
      "ç¡¬æŠ•ç¥¨ç›¸å¯¹æå‡: +0.20%\n",
      "åŠ æƒæŠ•ç¥¨ç›¸å¯¹æå‡: +0.25%\n",
      "\n",
      "ğŸ† æœ€ä½³é›†æˆæ–¹æ³•: è½¯æŠ•ç¥¨ (å‡†ç¡®ç‡: 0.9894)\n",
      "\n",
      "ğŸ’¾ ä¿å­˜çš„æ¨¡å‹æ–‡ä»¶:\n",
      "  EfficientNet-B0: best_efficientnet_b0_model.pth\n",
      "  ResNet18: best_resnet18_model.pth\n",
      "  ConvNeXt-Tiny: best_convnext_tiny_model.pth\n",
      "\n",
      "âš–ï¸ æ¨¡å‹æƒé‡åˆ†é…:\n",
      "  EfficientNet-B0: 0.3343\n",
      "  ResNet18: 0.3334\n",
      "  ConvNeXt-Tiny: 0.3323\n"
     ]
    }
   ],
   "source": [
    "# Cell 16: æœ€ç»ˆæ€»ç»“\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ‰ å¤šæ¨¡å‹é›†æˆè®­ç»ƒå®Œæˆï¼\")\n",
    "print(\"=\"*60)\n",
    "print(f\"è®­ç»ƒçš„æ¨¡å‹æ•°é‡: {len(selected_models)}\")\n",
    "print(f\"æœ€ä½³å•æ¨¡å‹å‡†ç¡®ç‡: {max([results['best_acc'] for results in model_results.values()]):.4f}\")\n",
    "print(f\"è½¯æŠ•ç¥¨é›†æˆå‡†ç¡®ç‡: {soft_accuracy:.4f}\")\n",
    "print(f\"ç¡¬æŠ•ç¥¨é›†æˆå‡†ç¡®ç‡: {hard_accuracy:.4f}\")\n",
    "print(f\"åŠ æƒæŠ•ç¥¨é›†æˆå‡†ç¡®ç‡: {weighted_accuracy:.4f}\")\n",
    "\n",
    "# è®¡ç®—æå‡å¹…åº¦\n",
    "best_single = max([results['best_acc'] for results in model_results.values()])\n",
    "soft_improvement = (soft_accuracy - best_single) * 100\n",
    "hard_improvement = (hard_accuracy - best_single) * 100\n",
    "weighted_improvement = (weighted_accuracy - best_single) * 100\n",
    "\n",
    "print(f\"è½¯æŠ•ç¥¨ç›¸å¯¹æå‡: {soft_improvement:+.2f}%\")\n",
    "print(f\"ç¡¬æŠ•ç¥¨ç›¸å¯¹æå‡: {hard_improvement:+.2f}%\")\n",
    "print(f\"åŠ æƒæŠ•ç¥¨ç›¸å¯¹æå‡: {weighted_improvement:+.2f}%\")\n",
    "\n",
    "# æ‰¾å‡ºæœ€ä½³é›†æˆæ–¹æ³•\n",
    "ensemble_results = {\n",
    "    'è½¯æŠ•ç¥¨': soft_accuracy,\n",
    "    'ç¡¬æŠ•ç¥¨': hard_accuracy,\n",
    "    'åŠ æƒæŠ•ç¥¨': weighted_accuracy\n",
    "}\n",
    "best_ensemble = max(ensemble_results, key=ensemble_results.get)\n",
    "print(f\"\\nğŸ† æœ€ä½³é›†æˆæ–¹æ³•: {best_ensemble} (å‡†ç¡®ç‡: {ensemble_results[best_ensemble]:.4f})\")\n",
    "\n",
    "print(f\"\\nğŸ’¾ ä¿å­˜çš„æ¨¡å‹æ–‡ä»¶:\")\n",
    "for model_key, path in model_paths.items():\n",
    "    print(f\"  {MODEL_CONFIGS[model_key]['name']}: {path}\")\n",
    "\n",
    "print(f\"\\nâš–ï¸ æ¨¡å‹æƒé‡åˆ†é…:\")\n",
    "for model_key, weight in model_weights.items():\n",
    "    print(f\"  {MODEL_CONFIGS[model_key]['name']}: {weight:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 1909705,
     "sourceId": 3134515,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
