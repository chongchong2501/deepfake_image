{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Kaggle Multi-Model Ensemble Deepfake Detection\n",
      "PyTorch Version: 2.7.1+cu118\n",
      "Captum Available: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MECHREUO\\AppData\\Roaming\\Python\\Python313\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: å¯¼å…¥ä¾èµ–å’Œç¯å¢ƒè®¾ç½®\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, f1_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import font_manager\n",
    "import matplotlib.patches as patches\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "import time\n",
    "from PIL import Image\n",
    "import gc\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# è§£é‡Šå·¥å…·å¯¼å…¥\n",
    "try:\n",
    "    from captum.attr import LayerGradCam, IntegratedGradients\n",
    "    from captum.attr import visualization as viz\n",
    "    CAPTUM_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"âš ï¸ Captum not available. Install with: pip install captum\")\n",
    "    CAPTUM_AVAILABLE = False\n",
    "\n",
    "# è®¾ç½®matplotlibä½¿ç”¨è‹±æ–‡å­—ä½“å’Œé«˜DPI\n",
    "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
    "plt.rcParams['font.size'] = 10\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "plt.rcParams['savefig.dpi'] = 300\n",
    "plt.rcParams['savefig.bbox'] = 'tight'\n",
    "\n",
    "# è®¾ç½®éšæœºç§å­\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"ğŸš€ Kaggle Multi-Model Ensemble Deepfake Detection\")\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"Captum Available: {CAPTUM_AVAILABLE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "GPU: NVIDIA GeForce RTX 4070 Laptop GPU\n",
      "GPU 0: NVIDIA GeForce RTX 4070 Laptop GPU\n",
      "GPU 0 Memory: 8.0GB\n",
      "Plots will be saved to: ./works/plots\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: å‚æ•°é…ç½®\n",
    "BASE_PATH = r'E:\\program\\deepfake_image\\Dataset'\n",
    "TRAIN_PATH = os.path.join(BASE_PATH, 'Train')\n",
    "VAL_PATH = os.path.join(BASE_PATH, 'Validation')\n",
    "\n",
    "# è®­ç»ƒå‚æ•°\n",
    "# å›¾åƒå¤§å°\n",
    "IMG_SIZE = 256\n",
    "\n",
    "# è®­ç»ƒæ‰¹æ¬¡å¤§å°\n",
    "BATCH_SIZE = 28\n",
    "\n",
    "# å­¦ä¹ ç‡\n",
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "# è®­ç»ƒè½®æ•°\n",
    "EPOCHS = 15\n",
    "\n",
    "# æƒé‡è¡°å‡ç³»æ•°\n",
    "WEIGHT_DECAY = 1e-4\n",
    "\n",
    "# æ—©åœè½®æ•°\n",
    "PATIENCE = 3\n",
    "\n",
    "# æ•°æ®åŠ è½½å™¨çš„å·¥ä½œè¿›ç¨‹æ•°é‡\n",
    "NUM_WORKERS = 0\n",
    "\n",
    "# è·å–å½“å‰è®¾å¤‡çš„GPUä¿¡æ¯\n",
    "NUM_GPUS = torch.cuda.device_count()\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {DEVICE}\")\n",
    "if torch.cuda.is_available():\n",
    "    if NUM_GPUS > 1:\n",
    "        print(f\"Multi-GPU Training: {[torch.cuda.get_device_name(i) for i in range(NUM_GPUS)]}\")\n",
    "        print(f\"GPU Count: {NUM_GPUS}\")\n",
    "        # NUM_WORKERS = 4  # å¤šGPUæ—¶å¢åŠ æ•°æ®åŠ è½½çº¿ç¨‹\n",
    "    else:\n",
    "        print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        # NUM_WORKERS = 2  # å•GPUæ—¶å‡å°‘æ•°æ®åŠ è½½çº¿ç¨‹\n",
    "    \n",
    "    for i in range(NUM_GPUS):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        print(f\"GPU {i} Memory: {torch.cuda.get_device_properties(i).total_memory / 1024**3:.1f}GB\")\n",
    "else:\n",
    "    NUM_WORKERS = 0\n",
    "    print(\"Using CPU Training\")\n",
    "\n",
    "# åˆ›å»ºè¾“å‡ºç›®å½•\n",
    "PLOTS_DIR = './works/plots'\n",
    "os.makedirs(PLOTS_DIR, exist_ok=True)\n",
    "print(f\"Plots will be saved to: {PLOTS_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 3: æ•°æ®åŠ è½½å‡½æ•°\n",
    "classes = ['Real', 'Fake']\n",
    "\n",
    "def create_dataframe(data_path, dataset_type):\n",
    "    \"\"\"åˆ›å»ºæ•°æ®é›†DataFrame\"\"\"\n",
    "    filepaths, labels = [], []\n",
    "    \n",
    "    for label_idx, cls in enumerate(classes):\n",
    "        folder = os.path.join(data_path, cls)\n",
    "        if os.path.exists(folder):\n",
    "            for img_name in os.listdir(folder):\n",
    "                if img_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                    filepaths.append(os.path.join(folder, img_name))\n",
    "                    labels.append(label_idx)\n",
    "    \n",
    "    df = pd.DataFrame({'filepath': filepaths, 'label': labels})\n",
    "    print(f\"{dataset_type}é›†å›¾ç‰‡æ•°: {len(df)}\")\n",
    "    if len(df) > 0:\n",
    "        print(f\"{dataset_type}é›†ç±»åˆ«åˆ†å¸ƒ:\")\n",
    "        for idx, cls in enumerate(classes):\n",
    "            count = len(df[df['label'] == idx])\n",
    "            print(f\"  {cls}: {count} ({count/len(df)*100:.1f}%)\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 4: æ•°æ®é¢„å¤„ç†å’Œå¢å¼º\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=10),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "class DeepfakeDataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.df.iloc[idx]['filepath']\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        label = self.df.iloc[idx]['label']\n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 5: æ¨¡å‹å®šä¹‰\n",
    "def create_efficientnet_b0():\n",
    "    \"\"\"åˆ›å»ºEfficientNet-B0æ¨¡å‹\"\"\"\n",
    "    model = models.efficientnet_b0(weights='IMAGENET1K_V1')\n",
    "    model.classifier[1] = nn.Linear(1280, 2)\n",
    "    return model\n",
    "\n",
    "def create_resnet18():\n",
    "    \"\"\"åˆ›å»ºResNet18æ¨¡å‹\"\"\"\n",
    "    model = models.resnet18(weights='IMAGENET1K_V1')\n",
    "    model.fc = nn.Linear(512, 2)\n",
    "    return model\n",
    "\n",
    "def create_convnext_tiny():\n",
    "    \"\"\"åˆ›å»ºConvNeXt-Tinyæ¨¡å‹\"\"\"\n",
    "    model = models.convnext_tiny(weights='IMAGENET1K_V1')\n",
    "    model.classifier[2] = nn.Linear(768, 2)\n",
    "    return model\n",
    "\n",
    "# æ¨¡å‹é…ç½®å­—å…¸\n",
    "MODEL_CONFIGS = {\n",
    "    'efficientnet_b0': {\n",
    "        'create_fn': create_efficientnet_b0,\n",
    "        'name': 'EfficientNet-B0'\n",
    "    },\n",
    "    'resnet18': {\n",
    "        'create_fn': create_resnet18,\n",
    "        'name': 'ResNet18'\n",
    "    },\n",
    "    'convnext_tiny': {\n",
    "        'create_fn': create_convnext_tiny,\n",
    "        'name': 'ConvNeXt-Tiny'\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 6: å•æ¨¡å‹è®­ç»ƒå‡½æ•°\n",
    "def train_single_model(model_key, train_loader, val_loader, save_path):\n",
    "    \"\"\"è®­ç»ƒå•ä¸ªæ¨¡å‹\"\"\"\n",
    "    print(f\"\\nğŸ”¥ Starting Training {MODEL_CONFIGS[model_key]['name']}\")\n",
    "    \n",
    "    # è®°å½•è®­ç»ƒå¼€å§‹æ—¶é—´\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # åˆ›å»ºæ¨¡å‹\n",
    "    model = MODEL_CONFIGS[model_key]['create_fn']()\n",
    "    model = model.to(DEVICE)\n",
    "    \n",
    "    # å¤šGPUæ”¯æŒ\n",
    "    if NUM_GPUS > 1:\n",
    "        model = nn.DataParallel(model)\n",
    "        print(f\"âœ… Model configured for multi-GPU training with {NUM_GPUS} GPUs\")\n",
    "    \n",
    "    # æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
    "    \n",
    "    # è®­ç»ƒè®°å½•\n",
    "    best_val_acc = 0\n",
    "    patience_counter = 0\n",
    "    train_losses, val_losses, val_accuracies, learning_rates = [], [], [], []\n",
    "    val_f1_scores = []\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        # è®­ç»ƒé˜¶æ®µ\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for imgs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} [Train]\"):\n",
    "            imgs, labels = imgs.to(DEVICE), labels.to(DEVICE)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        # éªŒè¯é˜¶æ®µ\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        all_val_preds = []\n",
    "        all_val_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for imgs, labels in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} [Val]\"):\n",
    "                imgs, labels = imgs.to(DEVICE), labels.to(DEVICE)\n",
    "                outputs = model(imgs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "                \n",
    "                all_val_preds.extend(predicted.cpu().numpy())\n",
    "                all_val_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        val_acc = correct / total\n",
    "        val_f1 = f1_score(all_val_labels, all_val_preds, average='weighted')\n",
    "        \n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_acc)\n",
    "        val_f1_scores.append(val_f1)\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        learning_rates.append(current_lr)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}: Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, Val F1: {val_f1:.4f}, LR: {current_lr:.6f}\")\n",
    "        \n",
    "        # ä¿å­˜æœ€ä½³æ¨¡å‹\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0\n",
    "            # ä¿å­˜æ¨¡å‹æ—¶å¤„ç†å¤šGPUæƒ…å†µ\n",
    "            if NUM_GPUS > 1:\n",
    "                torch.save(model.module.state_dict(), save_path)\n",
    "            else:\n",
    "                torch.save(model.state_dict(), save_path)\n",
    "            print(f\"âœ… Best model saved, validation accuracy: {best_val_acc:.4f}\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= PATIENCE:\n",
    "                print(\"â›” Early stopping triggered\")\n",
    "                break\n",
    "    \n",
    "    # è®¡ç®—è®­ç»ƒæ—¶é—´\n",
    "    training_time = time.time() - start_time\n",
    "    print(f\"â±ï¸ Training completed in {training_time:.2f} seconds\")\n",
    "    \n",
    "    return {\n",
    "         'best_acc': best_val_acc,\n",
    "         'train_losses': train_losses,\n",
    "         'val_losses': val_losses,\n",
    "         'val_accuracies': val_accuracies,\n",
    "         'val_f1_scores': val_f1_scores,\n",
    "         'learning_rates': learning_rates,\n",
    "         'training_time': training_time\n",
    "     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 7: å¯è§†åŒ–å‡½æ•°\n",
    "def plot_training_history(model_results, save_dir=PLOTS_DIR):\n",
    "    \"\"\"ç»˜åˆ¶è®­ç»ƒå†å²å¯è§†åŒ–\"\"\"\n",
    "    print(\"ğŸ“Š Generating training history visualizations...\")\n",
    "    \n",
    "    # 1. å•æ¨¡å‹è®­ç»ƒå†å² (2x2 å­å›¾)\n",
    "    for model_key, results in model_results.items():\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        fig.suptitle(f'{MODEL_CONFIGS[model_key][\"name\"]} Training History', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        epochs = range(1, len(results['train_losses']) + 1)\n",
    "        \n",
    "        # Lossæ›²çº¿\n",
    "        axes[0, 0].plot(epochs, results['train_losses'], 'b-', label='Train Loss', linewidth=2)\n",
    "        axes[0, 0].plot(epochs, results['val_losses'], 'r-', label='Validation Loss', linewidth=2)\n",
    "        axes[0, 0].set_title('Training & Validation Loss', fontweight='bold')\n",
    "        axes[0, 0].set_xlabel('Epoch')\n",
    "        axes[0, 0].set_ylabel('Loss')\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Accuracyæ›²çº¿\n",
    "        axes[0, 1].plot(epochs, results['val_accuracies'], 'g-', label='Validation Accuracy', linewidth=2)\n",
    "        axes[0, 1].set_title('Validation Accuracy', fontweight='bold')\n",
    "        axes[0, 1].set_xlabel('Epoch')\n",
    "        axes[0, 1].set_ylabel('Accuracy')\n",
    "        axes[0, 1].legend()\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Learning Rateæ›²çº¿\n",
    "        axes[1, 0].plot(epochs, results['learning_rates'], 'purple', linewidth=2)\n",
    "        axes[1, 0].set_title('Learning Rate Schedule', fontweight='bold')\n",
    "        axes[1, 0].set_xlabel('Epoch')\n",
    "        axes[1, 0].set_ylabel('Learning Rate')\n",
    "        axes[1, 0].set_yscale('log')\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Validation Accuracyåˆ†å¸ƒ\n",
    "        axes[1, 1].hist(results['val_accuracies'], bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "        axes[1, 1].axvline(results['best_acc'], color='red', linestyle='--', linewidth=2, label=f'Best: {results[\"best_acc\"]:.4f}')\n",
    "        axes[1, 1].set_title('Validation Accuracy Distribution', fontweight='bold')\n",
    "        axes[1, 1].set_xlabel('Accuracy')\n",
    "        axes[1, 1].set_ylabel('Frequency')\n",
    "        axes[1, 1].legend()\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        save_path = os.path.join(save_dir, f'{model_key}_training_history.png')\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f\"âœ… Saved: {save_path}\")\n",
    "    \n",
    "    # 2. å¤šæ¨¡å‹å¯¹æ¯”å›¾ (å››çº¿å¯¹æ¯”)\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle('Multi-Model Training Comparison', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    colors = ['blue', 'red', 'green', 'orange', 'purple']\n",
    "    \n",
    "    # éªŒè¯Losså¯¹æ¯”\n",
    "    for i, (model_key, results) in enumerate(model_results.items()):\n",
    "        epochs = range(1, len(results['val_losses']) + 1)\n",
    "        axes[0, 0].plot(epochs, results['val_losses'], color=colors[i % len(colors)], \n",
    "                       label=MODEL_CONFIGS[model_key]['name'], linewidth=2)\n",
    "    axes[0, 0].set_title('Validation Loss Comparison', fontweight='bold')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Validation Loss')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # éªŒè¯Accuracyå¯¹æ¯”\n",
    "    for i, (model_key, results) in enumerate(model_results.items()):\n",
    "        epochs = range(1, len(results['val_accuracies']) + 1)\n",
    "        axes[0, 1].plot(epochs, results['val_accuracies'], color=colors[i % len(colors)], \n",
    "                       label=MODEL_CONFIGS[model_key]['name'], linewidth=2)\n",
    "    axes[0, 1].set_title('Validation Accuracy Comparison', fontweight='bold')\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('Validation Accuracy')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # è®­ç»ƒæ—¶é•¿å¯¹æ¯”\n",
    "    model_names = [MODEL_CONFIGS[key]['name'] for key in model_results.keys()]\n",
    "    training_times = [results['training_time'] for results in model_results.values()]\n",
    "    bars = axes[1, 0].bar(model_names, training_times, color=colors[:len(model_names)], alpha=0.7)\n",
    "    axes[1, 0].set_title('Training Time Comparison', fontweight='bold')\n",
    "    axes[1, 0].set_ylabel('Training Time (seconds)')\n",
    "    axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # æ·»åŠ æ•°å€¼æ ‡ç­¾\n",
    "    for bar, time_val in zip(bars, training_times):\n",
    "        height = bar.get_height()\n",
    "        axes[1, 0].text(bar.get_x() + bar.get_width()/2., height + height*0.01,\n",
    "                       f'{time_val:.1f}s', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # F1-Scoreå¯¹æ¯”\n",
    "    for i, (model_key, results) in enumerate(model_results.items()):\n",
    "        epochs = range(1, len(results['val_f1_scores']) + 1)\n",
    "        axes[1, 1].plot(epochs, results['val_f1_scores'], color=colors[i % len(colors)], \n",
    "                       label=MODEL_CONFIGS[model_key]['name'], linewidth=2)\n",
    "    axes[1, 1].set_title('F1-Score Comparison', fontweight='bold')\n",
    "    axes[1, 1].set_xlabel('Epoch')\n",
    "    axes[1, 1].set_ylabel('F1-Score')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    save_path = os.path.join(save_dir, 'multi_model_comparison.png')\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"âœ… Saved: {save_path}\")\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, title, save_name, save_dir=PLOTS_DIR):\n",
    "    \"\"\"ç»˜åˆ¶æ··æ·†çŸ©é˜µ\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", \n",
    "                xticklabels=['Real', 'Fake'], yticklabels=['Real', 'Fake'],\n",
    "                cbar_kws={'label': 'Count'})\n",
    "    plt.title(title, fontsize=14, fontweight='bold')\n",
    "    plt.ylabel('True Label', fontweight='bold')\n",
    "    plt.xlabel('Predicted Label', fontweight='bold')\n",
    "    \n",
    "    # æ·»åŠ å‡†ç¡®ç‡ä¿¡æ¯\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    plt.text(0.5, -0.1, f'Accuracy: {accuracy:.4f}', \n",
    "             transform=plt.gca().transAxes, ha='center', fontweight='bold')\n",
    "    \n",
    "    save_path = os.path.join(save_dir, f'{save_name}_confusion_matrix.png')\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"âœ… Saved: {save_path}\")\n",
    "\n",
    "def plot_ensemble_analysis(models, val_loader, device, save_dir=PLOTS_DIR):\n",
    "    \"\"\"ç»˜åˆ¶é›†æˆåˆ†æå¯è§†åŒ–\"\"\"\n",
    "    print(\"ğŸ“Š Generating ensemble analysis visualizations...\")\n",
    "    \n",
    "    # æ”¶é›†æ‰€æœ‰æ¨¡å‹çš„é¢„æµ‹æ¦‚ç‡\n",
    "    all_probs = []\n",
    "    all_preds = []\n",
    "    y_true = []\n",
    "    \n",
    "    for model in models:\n",
    "        model.eval()\n",
    "        probs = []\n",
    "        preds = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                prob = F.softmax(outputs, dim=1)\n",
    "                probs.extend(prob.cpu().numpy())\n",
    "                preds.extend(torch.argmax(outputs, dim=1).cpu().numpy())\n",
    "                \n",
    "                if len(y_true) == 0:  # åªåœ¨ç¬¬ä¸€ä¸ªæ¨¡å‹æ—¶æ”¶é›†çœŸå®æ ‡ç­¾\n",
    "                    y_true.extend(labels.cpu().numpy())\n",
    "        \n",
    "        all_probs.append(np.array(probs))\n",
    "        all_preds.append(np.array(preds))\n",
    "    \n",
    "    all_probs = np.array(all_probs)  # shape: (n_models, n_samples, n_classes)\n",
    "    all_preds = np.array(all_preds)  # shape: (n_models, n_samples)\n",
    "    y_true = np.array(y_true)\n",
    "    \n",
    "    # è®¡ç®—é›†æˆé¢„æµ‹\n",
    "    ensemble_probs = np.mean(all_probs, axis=0)  # å¹³å‡æ¦‚ç‡\n",
    "    ensemble_preds = np.argmax(ensemble_probs, axis=1)\n",
    "    ensemble_confidence = np.max(ensemble_probs, axis=1)\n",
    "    \n",
    "    # åˆ›å»º2x2å­å›¾\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle('Ensemble Analysis', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. é¢„æµ‹æ¦‚ç‡ç›´æ–¹å›¾\n",
    "    for i, model_name in enumerate(['EfficientNet-B0', 'ResNet18', 'ConvNeXt-Tiny']):\n",
    "        if i < len(all_probs):\n",
    "            fake_probs = all_probs[i][:, 1]  # å‡å›¾ç‰‡çš„æ¦‚ç‡\n",
    "            axes[0, 0].hist(fake_probs, bins=30, alpha=0.6, label=model_name, density=True)\n",
    "    \n",
    "    axes[0, 0].set_title('Prediction Probability Distribution (Fake Class)', fontweight='bold')\n",
    "    axes[0, 0].set_xlabel('Probability')\n",
    "    axes[0, 0].set_ylabel('Density')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. æ¨¡å‹ä¸€è‡´æ€§çƒ­å›¾\n",
    "    n_models = len(all_preds)\n",
    "    consistency_matrix = np.zeros((n_models, n_models))\n",
    "    \n",
    "    for i in range(n_models):\n",
    "        for j in range(n_models):\n",
    "            consistency_matrix[i, j] = np.mean(all_preds[i] == all_preds[j])\n",
    "    \n",
    "    model_names = ['EfficientNet-B0', 'ResNet18', 'ConvNeXt-Tiny'][:n_models]\n",
    "    sns.heatmap(consistency_matrix, annot=True, fmt='.3f', cmap='YlOrRd',\n",
    "                xticklabels=model_names, yticklabels=model_names, ax=axes[0, 1])\n",
    "    axes[0, 1].set_title('Model Prediction Consistency', fontweight='bold')\n",
    "    \n",
    "    # 3. é›†æˆç½®ä¿¡åº¦å¯¹æ¯”ï¼ˆæ­£ç¡®vsé”™è¯¯é¢„æµ‹ï¼‰\n",
    "    correct_mask = ensemble_preds == y_true\n",
    "    correct_confidence = ensemble_confidence[correct_mask]\n",
    "    incorrect_confidence = ensemble_confidence[~correct_mask]\n",
    "    \n",
    "    axes[1, 0].hist(correct_confidence, bins=30, alpha=0.7, label='Correct Predictions', \n",
    "                   color='green', density=True)\n",
    "    axes[1, 0].hist(incorrect_confidence, bins=30, alpha=0.7, label='Incorrect Predictions', \n",
    "                   color='red', density=True)\n",
    "    axes[1, 0].set_title('Ensemble Prediction Confidence', fontweight='bold')\n",
    "    axes[1, 0].set_xlabel('Confidence')\n",
    "    axes[1, 0].set_ylabel('Density')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. å„ç±»åˆ«F1-scoreæŸ±çŠ¶å›¾\n",
    "    class_names = ['Real', 'Fake']\n",
    "    # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„F1åˆ†æ•°\n",
    "    f1_scores = f1_score(y_true, ensemble_preds, average=None)  # è¿”å›æ¯ä¸ªç±»åˆ«çš„F1åˆ†æ•°\n",
    "    \n",
    "    bars = axes[1, 1].bar(class_names, f1_scores, color=['skyblue', 'lightcoral'], alpha=0.8)\n",
    "    axes[1, 1].set_title('Per-Class F1-Score', fontweight='bold')\n",
    "    axes[1, 1].set_ylabel('F1-Score')\n",
    "    axes[1, 1].set_ylim(0, 1)\n",
    "    \n",
    "    # æ·»åŠ æ•°å€¼æ ‡ç­¾\n",
    "    for bar, score in zip(bars, f1_scores):\n",
    "        height = bar.get_height()\n",
    "        axes[1, 1].text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                       f'{score:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    save_path = os.path.join(save_dir, 'ensemble_analysis.png')\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"âœ… Saved: {save_path}\")\n",
    "\n",
    "def plot_interpretability_analysis(models, val_loader, device, save_dir=PLOTS_DIR, num_samples=4):\n",
    "    \"\"\"ç»˜åˆ¶æ¨¡å‹è§£é‡Šæ€§åˆ†æï¼ˆGrad-CAM + Integrated Gradientsï¼‰\"\"\"\n",
    "    print(\"ğŸ“Š Generating interpretability analysis...\")\n",
    "    \n",
    "    if not CAPTUM_AVAILABLE:\n",
    "        print(\"âš ï¸ Captum not available, skipping interpretability analysis\")\n",
    "        return\n",
    "    \n",
    "    # è·å–ä¸€äº›æ ·æœ¬è¿›è¡Œåˆ†æ\n",
    "    sample_images = []\n",
    "    sample_labels = []\n",
    "    sample_preds = []\n",
    "    \n",
    "    models[0].eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = models[0](inputs)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            \n",
    "            # é€‰æ‹©ä¸€äº›æœ‰è¶£çš„æ ·æœ¬ï¼ˆé¢„æµ‹æ­£ç¡®å’Œé”™è¯¯çš„ï¼‰\n",
    "            for i in range(min(num_samples, len(inputs))):\n",
    "                sample_images.append(inputs[i])\n",
    "                sample_labels.append(labels[i].item())\n",
    "                sample_preds.append(preds[i].item())\n",
    "            \n",
    "            if len(sample_images) >= num_samples:\n",
    "                break\n",
    "    \n",
    "    # ä¸ºæ¯ä¸ªæ¨¡å‹ç”Ÿæˆè§£é‡Š\n",
    "    for model_idx, model in enumerate(models):\n",
    "        model_name = ['EfficientNet-B0', 'ResNet18', 'ConvNeXt-Tiny'][model_idx]\n",
    "        \n",
    "        # åˆ›å»ºå­å›¾\n",
    "        fig, axes = plt.subplots(num_samples, 4, figsize=(16, 4*num_samples))\n",
    "        if num_samples == 1:\n",
    "            axes = axes.reshape(1, -1)\n",
    "        \n",
    "        fig.suptitle(f'{model_name} - Interpretability Analysis', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        for sample_idx in range(num_samples):\n",
    "            input_tensor = sample_images[sample_idx].unsqueeze(0)\n",
    "            true_label = sample_labels[sample_idx]\n",
    "            pred_label = sample_preds[sample_idx]\n",
    "            \n",
    "            # åŸå§‹å›¾åƒ\n",
    "            img_np = input_tensor.squeeze().cpu().numpy().transpose(1, 2, 0)\n",
    "            img_np = (img_np - img_np.min()) / (img_np.max() - img_np.min())  # å½’ä¸€åŒ–åˆ°[0,1]\n",
    "            axes[sample_idx, 0].imshow(img_np)\n",
    "            axes[sample_idx, 0].set_title(f'Original\\nTrue: {true_label}, Pred: {pred_label}')\n",
    "            axes[sample_idx, 0].axis('off')\n",
    "            \n",
    "            try:\n",
    "                # Grad-CAM\n",
    "                if hasattr(model, 'features'):  # EfficientNet/ResNet\n",
    "                    target_layer = model.features[-1]\n",
    "                elif hasattr(model, 'stages'):  # ConvNeXt\n",
    "                    target_layer = model.stages[-1]\n",
    "                else:\n",
    "                    # å°è¯•æ‰¾åˆ°æœ€åä¸€ä¸ªå·ç§¯å±‚\n",
    "                    target_layer = None\n",
    "                    for name, module in model.named_modules():\n",
    "                        if isinstance(module, torch.nn.Conv2d):\n",
    "                            target_layer = module\n",
    "                \n",
    "                if target_layer is not None:\n",
    "                    grad_cam = LayerGradCam(model, target_layer)\n",
    "                    attribution = grad_cam.attribute(input_tensor, target=pred_label)\n",
    "                    \n",
    "                    # æ˜¾ç¤ºGrad-CAM\n",
    "                    grad_cam_np = attribution.squeeze().cpu().numpy()\n",
    "                    axes[sample_idx, 1].imshow(grad_cam_np, cmap='jet', alpha=0.7)\n",
    "                    axes[sample_idx, 1].imshow(img_np, alpha=0.3)\n",
    "                    axes[sample_idx, 1].set_title('Grad-CAM')\n",
    "                    axes[sample_idx, 1].axis('off')\n",
    "                else:\n",
    "                    axes[sample_idx, 1].text(0.5, 0.5, 'Grad-CAM\\nNot Available', \n",
    "                                           ha='center', va='center', transform=axes[sample_idx, 1].transAxes)\n",
    "                    axes[sample_idx, 1].axis('off')\n",
    "                \n",
    "                # Integrated Gradients\n",
    "                ig = IntegratedGradients(model)\n",
    "                attribution = ig.attribute(input_tensor, target=pred_label, n_steps=50)\n",
    "                \n",
    "                # æ˜¾ç¤ºIntegrated Gradients\n",
    "                ig_np = attribution.squeeze().cpu().numpy()\n",
    "                ig_np = np.transpose(ig_np, (1, 2, 0))\n",
    "                ig_np = np.abs(ig_np).sum(axis=2)  # å–ç»å¯¹å€¼å¹¶æ±‚å’Œ\n",
    "                axes[sample_idx, 2].imshow(ig_np, cmap='hot')\n",
    "                axes[sample_idx, 2].set_title('Integrated Gradients')\n",
    "                axes[sample_idx, 2].axis('off')\n",
    "                \n",
    "                # å åŠ æ˜¾ç¤º\n",
    "                axes[sample_idx, 3].imshow(img_np, alpha=0.7)\n",
    "                axes[sample_idx, 3].imshow(ig_np, cmap='hot', alpha=0.3)\n",
    "                axes[sample_idx, 3].set_title('Overlay')\n",
    "                axes[sample_idx, 3].axis('off')\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ Error generating interpretability for sample {sample_idx}: {e}\")\n",
    "                for col in range(1, 4):\n",
    "                    axes[sample_idx, col].text(0.5, 0.5, f'Error:\\n{str(e)[:50]}...', \n",
    "                                             ha='center', va='center', transform=axes[sample_idx, col].transAxes)\n",
    "                    axes[sample_idx, col].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        save_path = os.path.join(save_dir, f'{model_name.lower().replace(\"-\", \"_\")}_interpretability.png')\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f\"âœ… Saved: {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 8: é›†æˆé¢„æµ‹å‡½æ•°\n",
    "def load_trained_models(model_paths):\n",
    "    \"\"\"åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹\"\"\"\n",
    "    models_dict = {}\n",
    "    for model_key, path in model_paths.items():\n",
    "        if os.path.exists(path):\n",
    "            model = MODEL_CONFIGS[model_key]['create_fn']()\n",
    "            model.load_state_dict(torch.load(path, map_location=DEVICE))\n",
    "            model = model.to(DEVICE)\n",
    "            \n",
    "            # å¤šGPUæ”¯æŒ\n",
    "            if NUM_GPUS > 1:\n",
    "                model = nn.DataParallel(model)\n",
    "                print(f\"âœ… å·²åŠ è½½ {MODEL_CONFIGS[model_key]['name']} (å¤šGPUæ¨¡å¼)\")\n",
    "            else:\n",
    "                print(f\"âœ… å·²åŠ è½½ {MODEL_CONFIGS[model_key]['name']}\")\n",
    "            \n",
    "            model.eval()\n",
    "            models_dict[model_key] = model\n",
    "        else:\n",
    "            print(f\"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {path}\")\n",
    "    return models_dict\n",
    "\n",
    "def ensemble_predict(models_dict, data_loader, voting_type='soft', weights=None):\n",
    "    \"\"\"é›†æˆé¢„æµ‹\"\"\"\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    model_outputs = {key: [] for key in models_dict.keys()}\n",
    "    \n",
    "    # å¦‚æœæ˜¯åŠ æƒæŠ•ç¥¨ä½†æ²¡æœ‰æä¾›æƒé‡ï¼Œåˆ™ä½¿ç”¨ç­‰æƒé‡\n",
    "    if voting_type == 'weighted' and weights is None:\n",
    "        weights = {key: 1.0 for key in models_dict.keys()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in tqdm(data_loader, desc=\"é›†æˆé¢„æµ‹\"):\n",
    "            imgs, labels = imgs.to(DEVICE), labels.to(DEVICE)\n",
    "            \n",
    "            # æ”¶é›†æ¯ä¸ªæ¨¡å‹çš„é¢„æµ‹\n",
    "            batch_predictions = []\n",
    "            for model_key, model in models_dict.items():\n",
    "                outputs = model(imgs)\n",
    "                if voting_type in ['soft', 'weighted']:\n",
    "                    probs = torch.softmax(outputs, dim=1)\n",
    "                    batch_predictions.append(probs.cpu().numpy())\n",
    "                else:  # hard voting\n",
    "                    _, predicted = torch.max(outputs, 1)\n",
    "                    batch_predictions.append(predicted.cpu().numpy())\n",
    "                \n",
    "                model_outputs[model_key].extend(outputs.cpu().numpy())\n",
    "            \n",
    "            # é›†æˆé¢„æµ‹\n",
    "            if voting_type == 'soft':\n",
    "                # è½¯æŠ•ç¥¨ï¼šå¹³å‡æ¦‚ç‡\n",
    "                ensemble_probs = np.mean(batch_predictions, axis=0)\n",
    "                ensemble_pred = np.argmax(ensemble_probs, axis=1)\n",
    "            elif voting_type == 'weighted':\n",
    "                # åŠ æƒæŠ•ç¥¨ï¼šæ ¹æ®æƒé‡åŠ æƒå¹³å‡æ¦‚ç‡\n",
    "                weighted_probs = np.zeros_like(batch_predictions[0])\n",
    "                total_weight = 0\n",
    "                for i, (model_key, probs) in enumerate(zip(models_dict.keys(), batch_predictions)):\n",
    "                    weight = weights[model_key]\n",
    "                    weighted_probs += probs * weight\n",
    "                    total_weight += weight\n",
    "                ensemble_probs = weighted_probs / total_weight\n",
    "                ensemble_pred = np.argmax(ensemble_probs, axis=1)\n",
    "            else:\n",
    "                # ç¡¬æŠ•ç¥¨ï¼šå¤šæ•°æŠ•ç¥¨\n",
    "                batch_predictions = np.array(batch_predictions)\n",
    "                ensemble_pred = []\n",
    "                for i in range(batch_predictions.shape[1]):\n",
    "                    votes = batch_predictions[:, i]\n",
    "                    ensemble_pred.append(np.bincount(votes).argmax())\n",
    "                ensemble_pred = np.array(ensemble_pred)\n",
    "            \n",
    "            all_predictions.extend(ensemble_pred)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    return np.array(all_predictions), np.array(all_labels), model_outputs\n",
    "\n",
    "def calculate_model_weights(model_results, weight_method='accuracy'):\n",
    "    \"\"\"è®¡ç®—æ¨¡å‹æƒé‡\"\"\"\n",
    "    weights = {}\n",
    "    \n",
    "    if weight_method == 'accuracy':\n",
    "        # åŸºäºéªŒè¯å‡†ç¡®ç‡è®¡ç®—æƒé‡\n",
    "        accuracies = {key: results['best_acc'] for key, results in model_results.items()}\n",
    "        total_acc = sum(accuracies.values())\n",
    "        \n",
    "        for key, acc in accuracies.items():\n",
    "            weights[key] = acc / total_acc\n",
    "            \n",
    "    elif weight_method == 'softmax':\n",
    "        # ä½¿ç”¨softmaxå½’ä¸€åŒ–å‡†ç¡®ç‡ä½œä¸ºæƒé‡\n",
    "        accuracies = np.array([results['best_acc'] for results in model_results.values()])\n",
    "        softmax_weights = np.exp(accuracies * 10) / np.sum(np.exp(accuracies * 10))  # ä¹˜ä»¥10å¢å¼ºå·®å¼‚\n",
    "        \n",
    "        for i, key in enumerate(model_results.keys()):\n",
    "            weights[key] = softmax_weights[i]\n",
    "            \n",
    "    elif weight_method == 'rank':\n",
    "        # åŸºäºæ’åçš„æƒé‡åˆ†é…\n",
    "        sorted_models = sorted(model_results.items(), key=lambda x: x[1]['best_acc'], reverse=True)\n",
    "        n_models = len(sorted_models)\n",
    "        \n",
    "        for i, (key, _) in enumerate(sorted_models):\n",
    "            weights[key] = (n_models - i) / sum(range(1, n_models + 1))\n",
    "    \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ åŠ è½½æ•°æ®é›†...\n",
      "è®­ç»ƒé›†å›¾ç‰‡æ•°: 140002\n",
      "è®­ç»ƒé›†ç±»åˆ«åˆ†å¸ƒ:\n",
      "  Real: 70001 (50.0%)\n",
      "  Fake: 70001 (50.0%)\n",
      "éªŒè¯é›†å›¾ç‰‡æ•°: 39428\n",
      "éªŒè¯é›†ç±»åˆ«åˆ†å¸ƒ:\n",
      "  Real: 19787 (50.2%)\n",
      "  Fake: 19641 (49.8%)\n",
      "âš ï¸ éªŒè¯é›†è¿‡å¤§ (39428 å¼ )ï¼Œéšæœºé‡‡æ · 6400 å¼ å›¾ç‰‡\n",
      "âœ… éªŒè¯é›†é‡‡æ ·å®Œæˆï¼Œå½“å‰å¤§å°: 6400\n",
      "éªŒè¯é›†ç±»åˆ«åˆ†å¸ƒ:\n",
      "  Real: 3200 (50.0%)\n",
      "  Fake: 3200 (50.0%)\n",
      "\n",
      "ğŸ“Š æ•°æ®é›†æ€»è§ˆ:\n",
      "è®­ç»ƒé›†æ€»æ•°: 140002\n",
      "éªŒè¯é›†æ€»æ•°: 6400\n",
      "éªŒè¯æ‰¹æ¬¡æ•°: 200\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: åŠ è½½æ•°æ®\n",
    "print(\"ğŸ“‚ åŠ è½½æ•°æ®é›†...\")\n",
    "train_df = create_dataframe(TRAIN_PATH, \"è®­ç»ƒ\")\n",
    "val_df = create_dataframe(VAL_PATH, \"éªŒè¯\")\n",
    "\n",
    "# é™åˆ¶éªŒè¯é›†å¤§å°ä¸º6400ä»¥å‡å°‘å†…å­˜ä½¿ç”¨\n",
    "MAX_VAL_SAMPLES = 12800\n",
    "if len(val_df) > MAX_VAL_SAMPLES:\n",
    "    print(f\"âš ï¸ éªŒè¯é›†è¿‡å¤§ ({len(val_df)} å¼ )ï¼Œéšæœºé‡‡æ · {MAX_VAL_SAMPLES} å¼ å›¾ç‰‡\")\n",
    "    # ä¿æŒç±»åˆ«å¹³è¡¡çš„éšæœºé‡‡æ ·\n",
    "    val_df = val_df.groupby('label', group_keys=False).apply(\n",
    "        lambda x: x.sample(min(len(x), MAX_VAL_SAMPLES//2), random_state=42)\n",
    "    ).reset_index(drop=True)\n",
    "    print(f\"âœ… éªŒè¯é›†é‡‡æ ·å®Œæˆï¼Œå½“å‰å¤§å°: {len(val_df)}\")\n",
    "    print(f\"éªŒè¯é›†ç±»åˆ«åˆ†å¸ƒ:\")\n",
    "    for idx, cls in enumerate(classes):\n",
    "        count = len(val_df[val_df['label'] == idx])\n",
    "        print(f\"  {cls}: {count} ({count/len(val_df)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nğŸ“Š æ•°æ®é›†æ€»è§ˆ:\")\n",
    "print(f\"è®­ç»ƒé›†æ€»æ•°: {len(train_df)}\")\n",
    "print(f\"éªŒè¯é›†æ€»æ•°: {len(val_df)}\")\n",
    "print(f\"éªŒè¯æ‰¹æ¬¡æ•°: {len(val_df) // BATCH_SIZE + (1 if len(val_df) % BATCH_SIZE > 0 else 0)}\")\n",
    "\n",
    "# åˆ›å»ºæ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨\n",
    "train_dataset = DeepfakeDataset(train_df, transform=train_transform)\n",
    "val_dataset = DeepfakeDataset(val_df, transform=val_transform)\n",
    "\n",
    "# ä½¿ç”¨åŠ¨æ€é…ç½®çš„num_workerså’Œpin_memory\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸš€ å¼€å§‹è®­ç»ƒå¤šä¸ªæ¨¡å‹...\n",
      "\n",
      "ğŸ”¥ Starting Training EfficientNet-B0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 [Train]:   8%|â–Š         | 345/4376 [01:48<21:03,  3.19it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     11\u001b[39m model_paths[model_key] = save_path\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# ä½¿ç”¨æ–°çš„è®­ç»ƒå‡½æ•°è¿”å›æ ¼å¼\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m model_results[model_key] = \u001b[43mtrain_single_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_path\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mâœ… \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMODEL_CONFIGS[model_key][\u001b[33m'\u001b[39m\u001b[33mname\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m è®­ç»ƒå®Œæˆï¼Œæœ€ä½³éªŒè¯å‡†ç¡®ç‡: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_results[model_key][\u001b[33m'\u001b[39m\u001b[33mbest_acc\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# æ¸…ç†GPUå†…å­˜\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 33\u001b[39m, in \u001b[36mtrain_single_model\u001b[39m\u001b[34m(model_key, train_loader, val_loader, save_path)\u001b[39m\n\u001b[32m     31\u001b[39m model.train()\n\u001b[32m     32\u001b[39m train_loss = \u001b[32m0\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mEpoch \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mepoch\u001b[49m\u001b[43m+\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mEPOCHS\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m [Train]\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\tqdm\\std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\utils\\data\\dataloader.py:733\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    731\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    732\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    736\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    739\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\utils\\data\\dataloader.py:789\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    787\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    788\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m789\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    790\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    791\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 33\u001b[39m, in \u001b[36mDeepfakeDataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m     30\u001b[39m img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transform:\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m     img = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m label = \u001b[38;5;28mself\u001b[39m.df.iloc[idx][\u001b[33m'\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m img, label\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[39m, in \u001b[36mCompose.__call__\u001b[39m\u001b[34m(self, img)\u001b[39m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[32m     94\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transforms:\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m         img = \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\torchvision\\transforms\\transforms.py:1280\u001b[39m, in \u001b[36mColorJitter.forward\u001b[39m\u001b[34m(self, img)\u001b[39m\n\u001b[32m   1278\u001b[39m         img = F.adjust_saturation(img, saturation_factor)\n\u001b[32m   1279\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m fn_id == \u001b[32m3\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m hue_factor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1280\u001b[39m         img = \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43madjust_hue\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhue_factor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1282\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\torchvision\\transforms\\functional.py:968\u001b[39m, in \u001b[36madjust_hue\u001b[39m\u001b[34m(img, hue_factor)\u001b[39m\n\u001b[32m    966\u001b[39m     _log_api_usage_once(adjust_hue)\n\u001b[32m    967\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch.Tensor):\n\u001b[32m--> \u001b[39m\u001b[32m968\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_pil\u001b[49m\u001b[43m.\u001b[49m\u001b[43madjust_hue\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhue_factor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    970\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m F_t.adjust_hue(img, hue_factor)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\torchvision\\transforms\\_functional_pil.py:109\u001b[39m, in \u001b[36madjust_hue\u001b[39m\u001b[34m(img, hue_factor)\u001b[39m\n\u001b[32m    106\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m input_mode \u001b[38;5;129;01min\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33mL\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m1\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mI\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mF\u001b[39m\u001b[33m\"\u001b[39m}:\n\u001b[32m    107\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n\u001b[32m--> \u001b[39m\u001b[32m109\u001b[39m h, s, v = \u001b[43mimg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mHSV\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m.split()\n\u001b[32m    111\u001b[39m np_h = np.array(h, dtype=np.uint8)\n\u001b[32m    112\u001b[39m \u001b[38;5;66;03m# This will over/underflow, as desired\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\PIL\\Image.py:1145\u001b[39m, in \u001b[36mImage.convert\u001b[39m\u001b[34m(self, mode, matrix, dither, palette, colors)\u001b[39m\n\u001b[32m   1142\u001b[39m     dither = Dither.FLOYDSTEINBERG\n\u001b[32m   1144\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1145\u001b[39m     im = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mim\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdither\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1146\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[32m   1147\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1148\u001b[39m         \u001b[38;5;66;03m# normalize source image and try again\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Cell 10: è®­ç»ƒæ‰€æœ‰æ¨¡å‹\n",
    "print(\"\\nğŸš€ å¼€å§‹è®­ç»ƒå¤šä¸ªæ¨¡å‹...\")\n",
    "\n",
    "# é€‰æ‹©è¦è®­ç»ƒçš„æ¨¡å‹ï¼ˆå¯ä»¥æ ¹æ®éœ€è¦è°ƒæ•´ï¼‰\n",
    "selected_models = ['efficientnet_b0', 'resnet18', 'convnext_tiny']  # å‡å°‘æ¨¡å‹æ•°é‡ä»¥é€‚åº”Kaggleç¯å¢ƒ\n",
    "model_paths = {}\n",
    "model_results = {}\n",
    "\n",
    "for model_key in selected_models:\n",
    "    save_path = f\"best_{model_key}_model.pth\"\n",
    "    model_paths[model_key] = save_path\n",
    "    \n",
    "    # ä½¿ç”¨æ–°çš„è®­ç»ƒå‡½æ•°è¿”å›æ ¼å¼\n",
    "    model_results[model_key] = train_single_model(\n",
    "        model_key, train_loader, val_loader, save_path\n",
    "    )\n",
    "    \n",
    "    print(f\"âœ… {MODEL_CONFIGS[model_key]['name']} è®­ç»ƒå®Œæˆï¼Œæœ€ä½³éªŒè¯å‡†ç¡®ç‡: {model_results[model_key]['best_acc']:.4f}\")\n",
    "    \n",
    "    # æ¸…ç†GPUå†…å­˜\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 11: å¢å¼ºè®­ç»ƒå†å²å¯è§†åŒ–\n",
    "print(\"\\nğŸ“Š ç”Ÿæˆè®­ç»ƒå†å²å¯è§†åŒ–...\")\n",
    "plot_training_history(model_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 12: é›†æˆé¢„æµ‹å’Œè¯„ä¼°\n",
    "print(\"\\nğŸ”® å¼€å§‹é›†æˆé¢„æµ‹...\")\n",
    "\n",
    "# åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹\n",
    "trained_models = load_trained_models(model_paths)\n",
    "\n",
    "# è®¡ç®—æ¨¡å‹æƒé‡\n",
    "print(\"\\nâš–ï¸ è®¡ç®—æ¨¡å‹æƒé‡...\")\n",
    "model_weights = calculate_model_weights(model_results, weight_method='accuracy')\n",
    "print(\"æ¨¡å‹æƒé‡åˆ†é…:\")\n",
    "for model_key, weight in model_weights.items():\n",
    "    print(f\"  {MODEL_CONFIGS[model_key]['name']:15}: {weight:.4f}\")\n",
    "\n",
    "# è½¯æŠ•ç¥¨é¢„æµ‹\n",
    "print(\"\\nğŸ“Š è½¯æŠ•ç¥¨é›†æˆé¢„æµ‹:\")\n",
    "soft_predictions, true_labels, _ = ensemble_predict(trained_models, val_loader, voting_type='soft')\n",
    "soft_accuracy = accuracy_score(true_labels, soft_predictions)\n",
    "print(f\"è½¯æŠ•ç¥¨å‡†ç¡®ç‡: {soft_accuracy:.4f}\")\n",
    "\n",
    "# ç¡¬æŠ•ç¥¨é¢„æµ‹\n",
    "print(\"\\nğŸ“Š ç¡¬æŠ•ç¥¨é›†æˆé¢„æµ‹:\")\n",
    "hard_predictions, _, _ = ensemble_predict(trained_models, val_loader, voting_type='hard')\n",
    "hard_accuracy = accuracy_score(true_labels, hard_predictions)\n",
    "print(f\"ç¡¬æŠ•ç¥¨å‡†ç¡®ç‡: {hard_accuracy:.4f}\")\n",
    "\n",
    "# åŠ æƒæŠ•ç¥¨é¢„æµ‹\n",
    "print(\"\\nğŸ“Š åŠ æƒæŠ•ç¥¨é›†æˆé¢„æµ‹:\")\n",
    "weighted_predictions, _, _ = ensemble_predict(trained_models, val_loader, voting_type='weighted', weights=model_weights)\n",
    "weighted_accuracy = accuracy_score(true_labels, weighted_predictions)\n",
    "print(f\"åŠ æƒæŠ•ç¥¨å‡†ç¡®ç‡: {weighted_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 13: ç»“æœå¯¹æ¯”å’Œå¯è§†åŒ–\n",
    "# å•æ¨¡å‹ç»“æœå¯¹æ¯”\n",
    "print(\"\\nğŸ“ˆ æ¨¡å‹æ€§èƒ½å¯¹æ¯”:\")\n",
    "print(\"=\"*50)\n",
    "for model_key in selected_models:\n",
    "    best_acc = model_results[model_key]['best_acc']\n",
    "    print(f\"{MODEL_CONFIGS[model_key]['name']:15}: {best_acc:.4f}\")\n",
    "\n",
    "print(f\"{'è½¯æŠ•ç¥¨é›†æˆ':15}: {soft_accuracy:.4f}\")\n",
    "print(f\"{'ç¡¬æŠ•ç¥¨é›†æˆ':15}: {hard_accuracy:.4f}\")\n",
    "print(f\"{'åŠ æƒæŠ•ç¥¨é›†æˆ':15}: {weighted_accuracy:.4f}\")\n",
    "\n",
    "# å¢å¼ºæ··æ·†çŸ©é˜µå¯è§†åŒ–\n",
    "print(\"\\nğŸ“Š ç”Ÿæˆæ··æ·†çŸ©é˜µå¯è§†åŒ–...\")\n",
    "plot_confusion_matrix(true_labels, soft_predictions, \"Soft Voting Ensemble\", \"soft_voting\")\n",
    "plot_confusion_matrix(true_labels, hard_predictions, \"Hard Voting Ensemble\", \"hard_voting\")\n",
    "plot_confusion_matrix(true_labels, weighted_predictions, \"Weighted Voting Ensemble\", \"weighted_voting\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 14: è¯¦ç»†åˆ†ç±»æŠ¥å‘Š\n",
    "print(\"\\nğŸ“‹ è½¯æŠ•ç¥¨è¯¦ç»†åˆ†ç±»æŠ¥å‘Š:\")\n",
    "print(\"=\"*50)\n",
    "print(classification_report(true_labels, soft_predictions, target_names=classes))\n",
    "\n",
    "print(\"\\nğŸ“‹ ç¡¬æŠ•ç¥¨è¯¦ç»†åˆ†ç±»æŠ¥å‘Š:\")\n",
    "print(\"=\"*50)\n",
    "print(classification_report(true_labels, hard_predictions, target_names=classes))\n",
    "\n",
    "print(\"\\nğŸ“‹ åŠ æƒæŠ•ç¥¨è¯¦ç»†åˆ†ç±»æŠ¥å‘Š:\")\n",
    "print(\"=\"*50)\n",
    "print(classification_report(true_labels, weighted_predictions, target_names=classes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 15: é›†æˆåˆ†æå’Œè§£é‡Šæ€§å¯è§†åŒ–\n",
    "print(\"\\nğŸ“Š ç”Ÿæˆé›†æˆåˆ†æå¯è§†åŒ–...\")\n",
    "plot_ensemble_analysis(trained_models, val_loader, device)\n",
    "\n",
    "print(\"\\nğŸ“Š ç”Ÿæˆæ¨¡å‹è§£é‡Šæ€§åˆ†æ...\")\n",
    "plot_interpretability_analysis(trained_models, val_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 16: æœ€ç»ˆæ€»ç»“\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ‰ å¤šæ¨¡å‹é›†æˆè®­ç»ƒå®Œæˆï¼\")\n",
    "print(\"=\"*60)\n",
    "print(f\"è®­ç»ƒçš„æ¨¡å‹æ•°é‡: {len(selected_models)}\")\n",
    "print(f\"æœ€ä½³å•æ¨¡å‹å‡†ç¡®ç‡: {max([results['best_acc'] for results in model_results.values()]):.4f}\")\n",
    "print(f\"è½¯æŠ•ç¥¨é›†æˆå‡†ç¡®ç‡: {soft_accuracy:.4f}\")\n",
    "print(f\"ç¡¬æŠ•ç¥¨é›†æˆå‡†ç¡®ç‡: {hard_accuracy:.4f}\")\n",
    "print(f\"åŠ æƒæŠ•ç¥¨é›†æˆå‡†ç¡®ç‡: {weighted_accuracy:.4f}\")\n",
    "\n",
    "# è®¡ç®—æå‡å¹…åº¦\n",
    "best_single = max([results['best_acc'] for results in model_results.values()])\n",
    "soft_improvement = (soft_accuracy - best_single) * 100\n",
    "hard_improvement = (hard_accuracy - best_single) * 100\n",
    "weighted_improvement = (weighted_accuracy - best_single) * 100\n",
    "\n",
    "print(f\"è½¯æŠ•ç¥¨ç›¸å¯¹æå‡: {soft_improvement:+.2f}%\")\n",
    "print(f\"ç¡¬æŠ•ç¥¨ç›¸å¯¹æå‡: {hard_improvement:+.2f}%\")\n",
    "print(f\"åŠ æƒæŠ•ç¥¨ç›¸å¯¹æå‡: {weighted_improvement:+.2f}%\")\n",
    "\n",
    "# æ‰¾å‡ºæœ€ä½³é›†æˆæ–¹æ³•\n",
    "ensemble_results = {\n",
    "    'è½¯æŠ•ç¥¨': soft_accuracy,\n",
    "    'ç¡¬æŠ•ç¥¨': hard_accuracy,\n",
    "    'åŠ æƒæŠ•ç¥¨': weighted_accuracy\n",
    "}\n",
    "best_ensemble = max(ensemble_results, key=ensemble_results.get)\n",
    "print(f\"\\nğŸ† æœ€ä½³é›†æˆæ–¹æ³•: {best_ensemble} (å‡†ç¡®ç‡: {ensemble_results[best_ensemble]:.4f})\")\n",
    "\n",
    "print(f\"\\nğŸ’¾ ä¿å­˜çš„æ¨¡å‹æ–‡ä»¶:\")\n",
    "for model_key, path in model_paths.items():\n",
    "    print(f\"  {MODEL_CONFIGS[model_key]['name']}: {path}\")\n",
    "\n",
    "print(f\"\\nâš–ï¸ æ¨¡å‹æƒé‡åˆ†é…:\")\n",
    "for model_key, weight in model_weights.items():\n",
    "    print(f\"  {MODEL_CONFIGS[model_key]['name']}: {weight:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 1909705,
     "sourceId": 3134515,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
