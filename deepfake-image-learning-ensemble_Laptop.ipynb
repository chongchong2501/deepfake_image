{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Kaggle Multi-Model Ensemble Deepfake Detection\n",
      "PyTorch Version: 2.7.1+cu118\n",
      "Captum Available: True\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: 导入依赖和环境设置\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "# 添加AMP混合精度训练支持\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, f1_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import font_manager\n",
    "import matplotlib.patches as patches\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "import time\n",
    "from PIL import Image\n",
    "import gc\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 解释工具导入\n",
    "try:\n",
    "    from captum.attr import LayerGradCam, IntegratedGradients\n",
    "    from captum.attr import visualization as viz\n",
    "    CAPTUM_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"⚠️ Captum not available. Install with: pip install captum\")\n",
    "    CAPTUM_AVAILABLE = False\n",
    "\n",
    "# 设置matplotlib使用英文字体和高DPI\n",
    "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
    "plt.rcParams['font.size'] = 10\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "plt.rcParams['savefig.dpi'] = 300\n",
    "plt.rcParams['savefig.bbox'] = 'tight'\n",
    "\n",
    "# 设置随机种子\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"🚀 Kaggle Multi-Model Ensemble Deepfake Detection with AMP\")\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"Captum Available: {CAPTUM_AVAILABLE}\")\n",
    "print(f\"AMP Available: {torch.cuda.is_available() and hasattr(torch.cuda.amp, 'autocast')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "GPU: NVIDIA GeForce RTX 4070 Laptop GPU\n",
      "GPU 0: NVIDIA GeForce RTX 4070 Laptop GPU\n",
      "GPU 0 Memory: 8.0GB\n",
      "Plots will be saved to: ./works/plots\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: 参数配置\n",
    "BASE_PATH = r'E:\\program\\deepfake_image\\Dataset'\n",
    "TRAIN_PATH = os.path.join(BASE_PATH, 'Train')\n",
    "VAL_PATH = os.path.join(BASE_PATH, 'Validation')\n",
    "\n",
    "# 训练参数\n",
    "# 图像大小\n",
    "IMG_SIZE = 256\n",
    "\n",
    "# 训练批次大小 (AMP可以支持更大的批次)\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# 学习率\n",
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "# 训练轮数\n",
    "EPOCHS = 15\n",
    "\n",
    "# 权重衰减系数\n",
    "WEIGHT_DECAY = 1e-4\n",
    "\n",
    "# 早停轮数\n",
    "PATIENCE = 3\n",
    "\n",
    "# 数据加载器的工作进程数量\n",
    "NUM_WORKERS = 0\n",
    "\n",
    "# AMP混合精度训练开关\n",
    "USE_AMP = True  # 启用混合精度训练\n",
    "\n",
    "# 获取当前设备的GPU信息\n",
    "NUM_GPUS = torch.cuda.device_count()\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {DEVICE}\")\n",
    "\n",
    "# AMP支持检查\n",
    "if torch.cuda.is_available() and USE_AMP:\n",
    "    print(\"✅ AMP (Automatic Mixed Precision) enabled\")\n",
    "    print(\"📈 Expected benefits: ~30% faster training, ~40% memory reduction\")\n",
    "elif not torch.cuda.is_available():\n",
    "    USE_AMP = False\n",
    "    print(\"⚠️ CUDA not available, AMP disabled\")\n",
    "else:\n",
    "    print(\"ℹ️ AMP disabled by configuration\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    if NUM_GPUS > 1:\n",
    "        print(f\"Multi-GPU Training: {[torch.cuda.get_device_name(i) for i in range(NUM_GPUS)]}\")\n",
    "        print(f\"GPU Count: {NUM_GPUS}\")\n",
    "    else:\n",
    "        print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    \n",
    "    for i in range(NUM_GPUS):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        print(f\"GPU {i} Memory: {torch.cuda.get_device_properties(i).total_memory / 1024**3:.1f}GB\")\n",
    "else:\n",
    "    NUM_WORKERS = 0\n",
    "    print(\"Using CPU Training\")\n",
    "\n",
    "# 创建输出目录\n",
    "PLOTS_DIR = './works/plots'\n",
    "os.makedirs(PLOTS_DIR, exist_ok=True)\n",
    "print(f\"Plots will be saved to: {PLOTS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 3: 数据加载函数\n",
    "classes = ['Real', 'Fake']\n",
    "\n",
    "def create_dataframe(data_path, dataset_type):\n",
    "    \"\"\"创建数据集DataFrame\"\"\"\n",
    "    filepaths, labels = [], []\n",
    "    \n",
    "    for label_idx, cls in enumerate(classes):\n",
    "        folder = os.path.join(data_path, cls)\n",
    "        if os.path.exists(folder):\n",
    "            for img_name in os.listdir(folder):\n",
    "                if img_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                    filepaths.append(os.path.join(folder, img_name))\n",
    "                    labels.append(label_idx)\n",
    "    \n",
    "    df = pd.DataFrame({'filepath': filepaths, 'label': labels})\n",
    "    print(f\"{dataset_type}集图片数: {len(df)}\")\n",
    "    if len(df) > 0:\n",
    "        print(f\"{dataset_type}集类别分布:\")\n",
    "        for idx, cls in enumerate(classes):\n",
    "            count = len(df[df['label'] == idx])\n",
    "            print(f\"  {cls}: {count} ({count/len(df)*100:.1f}%)\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 4: 数据预处理和增强\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=10),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "class DeepfakeDataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.df.iloc[idx]['filepath']\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        label = self.df.iloc[idx]['label']\n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 5: 模型定义\n",
    "def create_efficientnet_b0():\n",
    "    \"\"\"创建EfficientNet-B0模型\"\"\"\n",
    "    model = models.efficientnet_b0(weights='IMAGENET1K_V1')\n",
    "    model.classifier[1] = nn.Linear(1280, 2)\n",
    "    return model\n",
    "\n",
    "def create_resnet18():\n",
    "    \"\"\"创建ResNet18模型\"\"\"\n",
    "    model = models.resnet18(weights='IMAGENET1K_V1')\n",
    "    model.fc = nn.Linear(512, 2)\n",
    "    return model\n",
    "\n",
    "def create_convnext_tiny():\n",
    "    \"\"\"创建ConvNeXt-Tiny模型\"\"\"\n",
    "    model = models.convnext_tiny(weights='IMAGENET1K_V1')\n",
    "    model.classifier[2] = nn.Linear(768, 2)\n",
    "    return model\n",
    "\n",
    "# 模型配置字典\n",
    "MODEL_CONFIGS = {\n",
    "    'efficientnet_b0': {\n",
    "        'create_fn': create_efficientnet_b0,\n",
    "        'name': 'EfficientNet-B0'\n",
    "    },\n",
    "    'resnet18': {\n",
    "        'create_fn': create_resnet18,\n",
    "        'name': 'ResNet18'\n",
    "    },\n",
    "    'convnext_tiny': {\n",
    "        'create_fn': create_convnext_tiny,\n",
    "        'name': 'ConvNeXt-Tiny'\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 6: 单模型训练函数 (支持AMP)\n",
    "def train_single_model(model_key, train_loader, val_loader, save_path):\n",
    "    \"\"\"训练单个模型 - 支持AMP混合精度训练\"\"\"\n",
    "    print(f\"\\n🔥 Starting Training {MODEL_CONFIGS[model_key]['name']}\")\n",
    "    if USE_AMP:\n",
    "        print(\"⚡ Using AMP (Automatic Mixed Precision) for faster training\")\n",
    "    \n",
    "    # 记录训练开始时间\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # 创建模型\n",
    "    model = MODEL_CONFIGS[model_key]['create_fn']()\n",
    "    model = model.to(DEVICE)\n",
    "    \n",
    "    # 多GPU支持\n",
    "    if NUM_GPUS > 1:\n",
    "        model = nn.DataParallel(model)\n",
    "        print(f\"✅ Model configured for multi-GPU training with {NUM_GPUS} GPUs\")\n",
    "    \n",
    "    # 损失函数和优化器\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
    "    \n",
    "    # AMP梯度缩放器\n",
    "    scaler = GradScaler() if USE_AMP else None\n",
    "    \n",
    "    # 训练记录\n",
    "    best_val_acc = 0\n",
    "    patience_counter = 0\n",
    "    train_losses, val_losses, val_accuracies, learning_rates = [], [], [], []\n",
    "    val_f1_scores = []\n",
    "    \n",
    "    # 显存使用监控\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"🔍 Initial GPU Memory: {torch.cuda.memory_allocated()/1024**3:.2f}GB / {torch.cuda.max_memory_allocated()/1024**3:.2f}GB\")\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        # 训练阶段\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        \n",
    "        for imgs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} [Train]\"):\n",
    "            imgs, labels = imgs.to(DEVICE, non_blocking=True), labels.to(DEVICE, non_blocking=True)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # AMP前向传播\n",
    "            if USE_AMP:\n",
    "                with autocast():\n",
    "                    outputs = model(imgs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                \n",
    "                # AMP反向传播\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                # 标准训练\n",
    "                outputs = model(imgs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        # 验证阶段\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        all_val_preds = []\n",
    "        all_val_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for imgs, labels in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} [Val]\"):\n",
    "                imgs, labels = imgs.to(DEVICE, non_blocking=True), labels.to(DEVICE, non_blocking=True)\n",
    "                \n",
    "                # AMP验证\n",
    "                if USE_AMP:\n",
    "                    with autocast():\n",
    "                        outputs = model(imgs)\n",
    "                        loss = criterion(outputs, labels)\n",
    "                else:\n",
    "                    outputs = model(imgs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "                \n",
    "                all_val_preds.extend(predicted.cpu().numpy())\n",
    "                all_val_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        val_acc = correct / total\n",
    "        val_f1 = f1_score(all_val_labels, all_val_preds, average='weighted')\n",
    "        \n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_acc)\n",
    "        val_f1_scores.append(val_f1)\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        learning_rates.append(current_lr)\n",
    "        \n",
    "        # 显存监控\n",
    "        if torch.cuda.is_available():\n",
    "            current_memory = torch.cuda.memory_allocated() / 1024**3\n",
    "            max_memory = torch.cuda.max_memory_allocated() / 1024**3\n",
    "            memory_info = f\"GPU Mem: {current_memory:.2f}GB/{max_memory:.2f}GB\"\n",
    "        else:\n",
    "            memory_info = \"\"\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}: Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, \"\n",
    "              f\"Val Acc: {val_acc:.4f}, Val F1: {val_f1:.4f}, LR: {current_lr:.6f} {memory_info}\")\n",
    "        \n",
    "        # 保存最佳模型\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0\n",
    "            # 保存模型时处理多GPU情况\n",
    "            if NUM_GPUS > 1:\n",
    "                torch.save(model.module.state_dict(), save_path)\n",
    "            else:\n",
    "                torch.save(model.state_dict(), save_path)\n",
    "            print(f\"✅ Best model saved, validation accuracy: {best_val_acc:.4f}\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= PATIENCE:\n",
    "                print(\"⛔ Early stopping triggered\")\n",
    "                break\n",
    "        \n",
    "        # 每个epoch后清理显存\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    # 计算训练时间\n",
    "    training_time = time.time() - start_time\n",
    "    print(f\"⏱️ Training completed in {training_time:.2f} seconds\")\n",
    "    \n",
    "    # 最终显存清理\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        print(f\"🧹 Final GPU Memory: {torch.cuda.memory_allocated()/1024**3:.2f}GB\")\n",
    "    \n",
    "    return {\n",
    "         'best_acc': best_val_acc,\n",
    "         'train_losses': train_losses,\n",
    "         'val_losses': val_losses,\n",
    "         'val_accuracies': val_accuracies,\n",
    "         'val_f1_scores': val_f1_scores,\n",
    "         'learning_rates': learning_rates,\n",
    "         'training_time': training_time,\n",
    "         'amp_enabled': USE_AMP\n",
    "     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 7: 可视化函数\n",
    "def plot_training_history(model_results, save_dir=PLOTS_DIR):\n",
    "    \"\"\"绘制训练历史可视化\"\"\"\n",
    "    print(\"📊 Generating training history visualizations...\")\n",
    "    \n",
    "    # 1. 单模型训练历史 (2x2 子图)\n",
    "    for model_key, results in model_results.items():\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        fig.suptitle(f'{MODEL_CONFIGS[model_key][\"name\"]} Training History', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        epochs = range(1, len(results['train_losses']) + 1)\n",
    "        \n",
    "        # Loss曲线\n",
    "        axes[0, 0].plot(epochs, results['train_losses'], 'b-', label='Train Loss', linewidth=2)\n",
    "        axes[0, 0].plot(epochs, results['val_losses'], 'r-', label='Validation Loss', linewidth=2)\n",
    "        axes[0, 0].set_title('Training & Validation Loss', fontweight='bold')\n",
    "        axes[0, 0].set_xlabel('Epoch')\n",
    "        axes[0, 0].set_ylabel('Loss')\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Accuracy曲线\n",
    "        axes[0, 1].plot(epochs, results['val_accuracies'], 'g-', label='Validation Accuracy', linewidth=2)\n",
    "        axes[0, 1].set_title('Validation Accuracy', fontweight='bold')\n",
    "        axes[0, 1].set_xlabel('Epoch')\n",
    "        axes[0, 1].set_ylabel('Accuracy')\n",
    "        axes[0, 1].legend()\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Learning Rate曲线\n",
    "        axes[1, 0].plot(epochs, results['learning_rates'], 'purple', linewidth=2)\n",
    "        axes[1, 0].set_title('Learning Rate Schedule', fontweight='bold')\n",
    "        axes[1, 0].set_xlabel('Epoch')\n",
    "        axes[1, 0].set_ylabel('Learning Rate')\n",
    "        axes[1, 0].set_yscale('log')\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Validation Accuracy分布\n",
    "        axes[1, 1].hist(results['val_accuracies'], bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "        axes[1, 1].axvline(results['best_acc'], color='red', linestyle='--', linewidth=2, label=f'Best: {results[\"best_acc\"]:.4f}')\n",
    "        axes[1, 1].set_title('Validation Accuracy Distribution', fontweight='bold')\n",
    "        axes[1, 1].set_xlabel('Accuracy')\n",
    "        axes[1, 1].set_ylabel('Frequency')\n",
    "        axes[1, 1].legend()\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        save_path = os.path.join(save_dir, f'{model_key}_training_history.png')\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f\"✅ Saved: {save_path}\")\n",
    "    \n",
    "    # 2. 多模型对比图 (四线对比)\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle('Multi-Model Training Comparison', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    colors = ['blue', 'red', 'green', 'orange', 'purple']\n",
    "    \n",
    "    # 验证Loss对比\n",
    "    for i, (model_key, results) in enumerate(model_results.items()):\n",
    "        epochs = range(1, len(results['val_losses']) + 1)\n",
    "        axes[0, 0].plot(epochs, results['val_losses'], color=colors[i % len(colors)], \n",
    "                       label=MODEL_CONFIGS[model_key]['name'], linewidth=2)\n",
    "    axes[0, 0].set_title('Validation Loss Comparison', fontweight='bold')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Validation Loss')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 验证Accuracy对比\n",
    "    for i, (model_key, results) in enumerate(model_results.items()):\n",
    "        epochs = range(1, len(results['val_accuracies']) + 1)\n",
    "        axes[0, 1].plot(epochs, results['val_accuracies'], color=colors[i % len(colors)], \n",
    "                       label=MODEL_CONFIGS[model_key]['name'], linewidth=2)\n",
    "    axes[0, 1].set_title('Validation Accuracy Comparison', fontweight='bold')\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('Validation Accuracy')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 训练时长对比\n",
    "    model_names = [MODEL_CONFIGS[key]['name'] for key in model_results.keys()]\n",
    "    training_times = [results['training_time'] for results in model_results.values()]\n",
    "    bars = axes[1, 0].bar(model_names, training_times, color=colors[:len(model_names)], alpha=0.7)\n",
    "    axes[1, 0].set_title('Training Time Comparison', fontweight='bold')\n",
    "    axes[1, 0].set_ylabel('Training Time (seconds)')\n",
    "    axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 添加数值标签\n",
    "    for bar, time_val in zip(bars, training_times):\n",
    "        height = bar.get_height()\n",
    "        axes[1, 0].text(bar.get_x() + bar.get_width()/2., height + height*0.01,\n",
    "                       f'{time_val:.1f}s', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # F1-Score对比\n",
    "    for i, (model_key, results) in enumerate(model_results.items()):\n",
    "        epochs = range(1, len(results['val_f1_scores']) + 1)\n",
    "        axes[1, 1].plot(epochs, results['val_f1_scores'], color=colors[i % len(colors)], \n",
    "                       label=MODEL_CONFIGS[model_key]['name'], linewidth=2)\n",
    "    axes[1, 1].set_title('F1-Score Comparison', fontweight='bold')\n",
    "    axes[1, 1].set_xlabel('Epoch')\n",
    "    axes[1, 1].set_ylabel('F1-Score')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    save_path = os.path.join(save_dir, 'multi_model_comparison.png')\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"✅ Saved: {save_path}\")\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, title, save_name, save_dir=PLOTS_DIR):\n",
    "    \"\"\"绘制混淆矩阵\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", \n",
    "                xticklabels=['Real', 'Fake'], yticklabels=['Real', 'Fake'],\n",
    "                cbar_kws={'label': 'Count'})\n",
    "    plt.title(title, fontsize=14, fontweight='bold')\n",
    "    plt.ylabel('True Label', fontweight='bold')\n",
    "    plt.xlabel('Predicted Label', fontweight='bold')\n",
    "    \n",
    "    # 添加准确率信息\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    plt.text(0.5, -0.1, f'Accuracy: {accuracy:.4f}', \n",
    "             transform=plt.gca().transAxes, ha='center', fontweight='bold')\n",
    "    \n",
    "    save_path = os.path.join(save_dir, f'{save_name}_confusion_matrix.png')\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"✅ Saved: {save_path}\")\n",
    "\n",
    "def plot_ensemble_analysis(models, val_loader, device, save_dir=PLOTS_DIR):\n",
    "    \"\"\"绘制集成分析可视化\"\"\"\n",
    "    print(\"📊 Generating ensemble analysis visualizations...\")\n",
    "    \n",
    "    # 收集所有模型的预测概率\n",
    "    all_probs = []\n",
    "    all_preds = []\n",
    "    y_true = []\n",
    "    \n",
    "    for model in models:\n",
    "        model.eval()\n",
    "        probs = []\n",
    "        preds = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                prob = F.softmax(outputs, dim=1)\n",
    "                probs.extend(prob.cpu().numpy())\n",
    "                preds.extend(torch.argmax(outputs, dim=1).cpu().numpy())\n",
    "                \n",
    "                if len(y_true) == 0:  # 只在第一个模型时收集真实标签\n",
    "                    y_true.extend(labels.cpu().numpy())\n",
    "        \n",
    "        all_probs.append(np.array(probs))\n",
    "        all_preds.append(np.array(preds))\n",
    "    \n",
    "    all_probs = np.array(all_probs)  # shape: (n_models, n_samples, n_classes)\n",
    "    all_preds = np.array(all_preds)  # shape: (n_models, n_samples)\n",
    "    y_true = np.array(y_true)\n",
    "    \n",
    "    # 计算集成预测\n",
    "    ensemble_probs = np.mean(all_probs, axis=0)  # 平均概率\n",
    "    ensemble_preds = np.argmax(ensemble_probs, axis=1)\n",
    "    ensemble_confidence = np.max(ensemble_probs, axis=1)\n",
    "    \n",
    "    # 创建2x2子图\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle('Ensemble Analysis', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. 预测概率直方图\n",
    "    for i, model_name in enumerate(['EfficientNet-B0', 'ResNet18', 'ConvNeXt-Tiny']):\n",
    "        if i < len(all_probs):\n",
    "            fake_probs = all_probs[i][:, 1]  # 假图片的概率\n",
    "            axes[0, 0].hist(fake_probs, bins=30, alpha=0.6, label=model_name, density=True)\n",
    "    \n",
    "    axes[0, 0].set_title('Prediction Probability Distribution (Fake Class)', fontweight='bold')\n",
    "    axes[0, 0].set_xlabel('Probability')\n",
    "    axes[0, 0].set_ylabel('Density')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. 模型一致性热图\n",
    "    n_models = len(all_preds)\n",
    "    consistency_matrix = np.zeros((n_models, n_models))\n",
    "    \n",
    "    for i in range(n_models):\n",
    "        for j in range(n_models):\n",
    "            consistency_matrix[i, j] = np.mean(all_preds[i] == all_preds[j])\n",
    "    \n",
    "    model_names = ['EfficientNet-B0', 'ResNet18', 'ConvNeXt-Tiny'][:n_models]\n",
    "    sns.heatmap(consistency_matrix, annot=True, fmt='.3f', cmap='YlOrRd',\n",
    "                xticklabels=model_names, yticklabels=model_names, ax=axes[0, 1])\n",
    "    axes[0, 1].set_title('Model Prediction Consistency', fontweight='bold')\n",
    "    \n",
    "    # 3. 集成置信度对比（正确vs错误预测）\n",
    "    correct_mask = ensemble_preds == y_true\n",
    "    correct_confidence = ensemble_confidence[correct_mask]\n",
    "    incorrect_confidence = ensemble_confidence[~correct_mask]\n",
    "    \n",
    "    axes[1, 0].hist(correct_confidence, bins=30, alpha=0.7, label='Correct Predictions', \n",
    "                   color='green', density=True)\n",
    "    axes[1, 0].hist(incorrect_confidence, bins=30, alpha=0.7, label='Incorrect Predictions', \n",
    "                   color='red', density=True)\n",
    "    axes[1, 0].set_title('Ensemble Prediction Confidence', fontweight='bold')\n",
    "    axes[1, 0].set_xlabel('Confidence')\n",
    "    axes[1, 0].set_ylabel('Density')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. 各类别F1-score柱状图\n",
    "    class_names = ['Real', 'Fake']\n",
    "    # 计算每个类别的F1分数\n",
    "    f1_scores = f1_score(y_true, ensemble_preds, average=None)  # 返回每个类别的F1分数\n",
    "    \n",
    "    bars = axes[1, 1].bar(class_names, f1_scores, color=['skyblue', 'lightcoral'], alpha=0.8)\n",
    "    axes[1, 1].set_title('Per-Class F1-Score', fontweight='bold')\n",
    "    axes[1, 1].set_ylabel('F1-Score')\n",
    "    axes[1, 1].set_ylim(0, 1)\n",
    "    \n",
    "    # 添加数值标签\n",
    "    for bar, score in zip(bars, f1_scores):\n",
    "        height = bar.get_height()\n",
    "        axes[1, 1].text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                       f'{score:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    save_path = os.path.join(save_dir, 'ensemble_analysis.png')\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"✅ Saved: {save_path}\")\n",
    "\n",
    "def plot_interpretability_analysis(models, val_loader, device, save_dir=PLOTS_DIR, num_samples=4):\n",
    "    \"\"\"绘制模型解释性分析（Grad-CAM + Integrated Gradients）\"\"\"\n",
    "    print(\"📊 Generating interpretability analysis...\")\n",
    "    \n",
    "    if not CAPTUM_AVAILABLE:\n",
    "        print(\"⚠️ Captum not available, skipping interpretability analysis\")\n",
    "        return\n",
    "    \n",
    "    # 获取一些样本进行分析\n",
    "    sample_images = []\n",
    "    sample_labels = []\n",
    "    sample_preds = []\n",
    "    \n",
    "    models[0].eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = models[0](inputs)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            \n",
    "            # 选择一些有趣的样本（预测正确和错误的）\n",
    "            for i in range(min(num_samples, len(inputs))):\n",
    "                sample_images.append(inputs[i])\n",
    "                sample_labels.append(labels[i].item())\n",
    "                sample_preds.append(preds[i].item())\n",
    "            \n",
    "            if len(sample_images) >= num_samples:\n",
    "                break\n",
    "    \n",
    "    # 为每个模型生成解释\n",
    "    for model_idx, model in enumerate(models):\n",
    "        model_name = ['EfficientNet-B0', 'ResNet18', 'ConvNeXt-Tiny'][model_idx]\n",
    "        \n",
    "        # 创建子图\n",
    "        fig, axes = plt.subplots(num_samples, 4, figsize=(16, 4*num_samples))\n",
    "        if num_samples == 1:\n",
    "            axes = axes.reshape(1, -1)\n",
    "        \n",
    "        fig.suptitle(f'{model_name} - Interpretability Analysis', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        for sample_idx in range(num_samples):\n",
    "            input_tensor = sample_images[sample_idx].unsqueeze(0)\n",
    "            true_label = sample_labels[sample_idx]\n",
    "            pred_label = sample_preds[sample_idx]\n",
    "            \n",
    "            # 原始图像\n",
    "            img_np = input_tensor.squeeze().cpu().numpy().transpose(1, 2, 0)\n",
    "            img_np = (img_np - img_np.min()) / (img_np.max() - img_np.min())  # 归一化到[0,1]\n",
    "            axes[sample_idx, 0].imshow(img_np)\n",
    "            axes[sample_idx, 0].set_title(f'Original\\nTrue: {true_label}, Pred: {pred_label}')\n",
    "            axes[sample_idx, 0].axis('off')\n",
    "            \n",
    "            try:\n",
    "                # Grad-CAM\n",
    "                if hasattr(model, 'features'):  # EfficientNet/ResNet\n",
    "                    target_layer = model.features[-1]\n",
    "                elif hasattr(model, 'stages'):  # ConvNeXt\n",
    "                    target_layer = model.stages[-1]\n",
    "                else:\n",
    "                    # 尝试找到最后一个卷积层\n",
    "                    target_layer = None\n",
    "                    for name, module in model.named_modules():\n",
    "                        if isinstance(module, torch.nn.Conv2d):\n",
    "                            target_layer = module\n",
    "                \n",
    "                if target_layer is not None:\n",
    "                    grad_cam = LayerGradCam(model, target_layer)\n",
    "                    attribution = grad_cam.attribute(input_tensor, target=pred_label)\n",
    "                    \n",
    "                    # 显示Grad-CAM\n",
    "                    grad_cam_np = attribution.squeeze().cpu().numpy()\n",
    "                    axes[sample_idx, 1].imshow(grad_cam_np, cmap='jet', alpha=0.7)\n",
    "                    axes[sample_idx, 1].imshow(img_np, alpha=0.3)\n",
    "                    axes[sample_idx, 1].set_title('Grad-CAM')\n",
    "                    axes[sample_idx, 1].axis('off')\n",
    "                else:\n",
    "                    axes[sample_idx, 1].text(0.5, 0.5, 'Grad-CAM\\nNot Available', \n",
    "                                           ha='center', va='center', transform=axes[sample_idx, 1].transAxes)\n",
    "                    axes[sample_idx, 1].axis('off')\n",
    "                \n",
    "                # Integrated Gradients\n",
    "                ig = IntegratedGradients(model)\n",
    "                attribution = ig.attribute(input_tensor, target=pred_label, n_steps=50)\n",
    "                \n",
    "                # 显示Integrated Gradients\n",
    "                ig_np = attribution.squeeze().cpu().numpy()\n",
    "                ig_np = np.transpose(ig_np, (1, 2, 0))\n",
    "                ig_np = np.abs(ig_np).sum(axis=2)  # 取绝对值并求和\n",
    "                axes[sample_idx, 2].imshow(ig_np, cmap='hot')\n",
    "                axes[sample_idx, 2].set_title('Integrated Gradients')\n",
    "                axes[sample_idx, 2].axis('off')\n",
    "                \n",
    "                # 叠加显示\n",
    "                axes[sample_idx, 3].imshow(img_np, alpha=0.7)\n",
    "                axes[sample_idx, 3].imshow(ig_np, cmap='hot', alpha=0.3)\n",
    "                axes[sample_idx, 3].set_title('Overlay')\n",
    "                axes[sample_idx, 3].axis('off')\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Error generating interpretability for sample {sample_idx}: {e}\")\n",
    "                for col in range(1, 4):\n",
    "                    axes[sample_idx, col].text(0.5, 0.5, f'Error:\\n{str(e)[:50]}...', \n",
    "                                             ha='center', va='center', transform=axes[sample_idx, col].transAxes)\n",
    "                    axes[sample_idx, col].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        save_path = os.path.join(save_dir, f'{model_name.lower().replace(\"-\", \"_\")}_interpretability.png')\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f\"✅ Saved: {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 8: 集成预测函数 (支持AMP)\n",
    "def ensemble_predict(models_dict, data_loader, voting_type='soft', weights=None):\n",
    "    \"\"\"集成预测函数 - 支持AMP混合精度\"\"\"\n",
    "    print(f\"🔮 开始集成预测 (投票方式: {voting_type})\")\n",
    "    if USE_AMP:\n",
    "        print(\"⚡ 使用AMP加速集成预测\")\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    model_outputs = {key: [] for key in models_dict.keys()}\n",
    "    \n",
    "    # 如果是加权投票但没有提供权重，则使用等权重\n",
    "    if voting_type == 'weighted' and weights is None:\n",
    "        weights = {key: 1.0 for key in models_dict.keys()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in tqdm(data_loader, desc=\"集成预测\"):\n",
    "            imgs, labels = imgs.to(DEVICE, non_blocking=True), labels.to(DEVICE, non_blocking=True)\n",
    "            \n",
    "            # 收集每个模型的预测\n",
    "            batch_predictions = []\n",
    "            for model_key, model in models_dict.items():\n",
    "                # AMP推理\n",
    "                if USE_AMP:\n",
    "                    with autocast():\n",
    "                        outputs = model(imgs)\n",
    "                else:\n",
    "                    outputs = model(imgs)\n",
    "                \n",
    "                if voting_type in ['soft', 'weighted']:\n",
    "                    probs = torch.softmax(outputs, dim=1)\n",
    "                    batch_predictions.append(probs.cpu().numpy())\n",
    "                else:  # hard voting\n",
    "                    _, predicted = torch.max(outputs, 1)\n",
    "                    batch_predictions.append(predicted.cpu().numpy())\n",
    "                \n",
    "                model_outputs[model_key].extend(outputs.cpu().numpy())\n",
    "            \n",
    "            # 集成预测\n",
    "            if voting_type == 'soft':\n",
    "                # 软投票：平均概率\n",
    "                ensemble_probs = np.mean(batch_predictions, axis=0)\n",
    "                ensemble_pred = np.argmax(ensemble_probs, axis=1)\n",
    "            elif voting_type == 'weighted':\n",
    "                # 加权投票：根据权重加权平均概率\n",
    "                weighted_probs = np.zeros_like(batch_predictions[0])\n",
    "                total_weight = 0\n",
    "                for i, (model_key, probs) in enumerate(zip(models_dict.keys(), batch_predictions)):\n",
    "                    weight = weights[model_key]\n",
    "                    weighted_probs += probs * weight\n",
    "                    total_weight += weight\n",
    "                ensemble_probs = weighted_probs / total_weight\n",
    "                ensemble_pred = np.argmax(ensemble_probs, axis=1)\n",
    "            else:\n",
    "                # 硬投票：多数投票\n",
    "                batch_predictions = np.array(batch_predictions)\n",
    "                ensemble_pred = []\n",
    "                for i in range(batch_predictions.shape[1]):\n",
    "                    votes = batch_predictions[:, i]\n",
    "                    ensemble_pred.append(np.bincount(votes).argmax())\n",
    "                ensemble_pred = np.array(ensemble_pred)\n",
    "            \n",
    "            all_predictions.extend(ensemble_pred)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    return np.array(all_predictions), np.array(all_labels), model_outputs\n",
    "\n",
    "def calculate_model_weights(model_results, weight_method='accuracy'):\n",
    "    \"\"\"计算模型权重\"\"\"\n",
    "    weights = {}\n",
    "    \n",
    "    if weight_method == 'accuracy':\n",
    "        # 基于验证准确率计算权重\n",
    "        accuracies = {key: results['best_acc'] for key, results in model_results.items()}\n",
    "        total_acc = sum(accuracies.values())\n",
    "        \n",
    "        for key, acc in accuracies.items():\n",
    "            weights[key] = acc / total_acc\n",
    "            \n",
    "    elif weight_method == 'softmax':\n",
    "        # 使用softmax归一化准确率作为权重\n",
    "        accuracies = np.array([results['best_acc'] for results in model_results.values()])\n",
    "        softmax_weights = np.exp(accuracies * 10) / np.sum(np.exp(accuracies * 10))  # 乘以10增强差异\n",
    "        \n",
    "        for i, key in enumerate(model_results.keys()):\n",
    "            weights[key] = softmax_weights[i]\n",
    "            \n",
    "    elif weight_method == 'rank':\n",
    "        # 基于排名的权重分配\n",
    "        sorted_models = sorted(model_results.items(), key=lambda x: x[1]['best_acc'], reverse=True)\n",
    "        n_models = len(sorted_models)\n",
    "        \n",
    "        for i, (key, _) in enumerate(sorted_models):\n",
    "            weights[key] = (n_models - i) / sum(range(1, n_models + 1))\n",
    "    \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📂 加载数据集...\n",
      "训练集图片数: 140002\n",
      "训练集类别分布:\n",
      "  Real: 70001 (50.0%)\n",
      "  Fake: 70001 (50.0%)\n",
      "验证集图片数: 39428\n",
      "验证集类别分布:\n",
      "  Real: 19787 (50.2%)\n",
      "  Fake: 19641 (49.8%)\n",
      "⚠️ 验证集过大 (39428 张)，随机采样 12800 张图片\n",
      "✅ 验证集采样完成，当前大小: 12800\n",
      "验证集类别分布:\n",
      "  Real: 6400 (50.0%)\n",
      "  Fake: 6400 (50.0%)\n",
      "\n",
      "📊 数据集总览:\n",
      "训练集总数: 140002\n",
      "验证集总数: 12800\n",
      "验证批次数: 458\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: 加载数据\n",
    "print(\"📂 加载数据集...\")\n",
    "train_df = create_dataframe(TRAIN_PATH, \"训练\")\n",
    "val_df = create_dataframe(VAL_PATH, \"验证\")\n",
    "\n",
    "# 限制验证集大小为6400以减少内存使用\n",
    "MAX_VAL_SAMPLES = 12800\n",
    "if len(val_df) > MAX_VAL_SAMPLES:\n",
    "    print(f\"⚠️ 验证集过大 ({len(val_df)} 张)，随机采样 {MAX_VAL_SAMPLES} 张图片\")\n",
    "    # 保持类别平衡的随机采样\n",
    "    val_df = val_df.groupby('label', group_keys=False).apply(\n",
    "        lambda x: x.sample(min(len(x), MAX_VAL_SAMPLES//2), random_state=42)\n",
    "    ).reset_index(drop=True)\n",
    "    print(f\"✅ 验证集采样完成，当前大小: {len(val_df)}\")\n",
    "    print(f\"验证集类别分布:\")\n",
    "    for idx, cls in enumerate(classes):\n",
    "        count = len(val_df[val_df['label'] == idx])\n",
    "        print(f\"  {cls}: {count} ({count/len(val_df)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\n📊 数据集总览:\")\n",
    "print(f\"训练集总数: {len(train_df)}\")\n",
    "print(f\"验证集总数: {len(val_df)}\")\n",
    "print(f\"验证批次数: {len(val_df) // BATCH_SIZE + (1 if len(val_df) % BATCH_SIZE > 0 else 0)}\")\n",
    "\n",
    "# 创建数据集和数据加载器\n",
    "train_dataset = DeepfakeDataset(train_df, transform=train_transform)\n",
    "val_dataset = DeepfakeDataset(val_df, transform=val_transform)\n",
    "\n",
    "# 使用动态配置的num_workers和pin_memory\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚀 开始训练多个模型...\n",
      "\n",
      "🔥 Starting Training EfficientNet-B0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/15 [Train]:   0%|          | 0/5001 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# Cell 10: 训练所有模型\n",
    "print(\"\\n🚀 开始训练多个模型...\")\n",
    "\n",
    "# 选择要训练的模型（可以根据需要调整）\n",
    "selected_models = ['efficientnet_b0', 'resnet18', 'convnext_tiny']  # 减少模型数量以适应Kaggle环境\n",
    "model_paths = {}\n",
    "model_results = {}\n",
    "\n",
    "for model_key in selected_models:\n",
    "    save_path = f\"best_{model_key}_model.pth\"\n",
    "    model_paths[model_key] = save_path\n",
    "    \n",
    "    # 使用新的训练函数返回格式\n",
    "    model_results[model_key] = train_single_model(\n",
    "        model_key, train_loader, val_loader, save_path\n",
    "    )\n",
    "    \n",
    "    print(f\"✅ {MODEL_CONFIGS[model_key]['name']} 训练完成，最佳验证准确率: {model_results[model_key]['best_acc']:.4f}\")\n",
    "    \n",
    "    # 清理GPU内存\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 11: 增强训练历史可视化\n",
    "print(\"\\n📊 生成训练历史可视化...\")\n",
    "plot_training_history(model_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 12: 集成预测和评估\n",
    "print(\"\\n🔮 开始集成预测...\")\n",
    "\n",
    "# 加载训练好的模型\n",
    "trained_models = load_trained_models(model_paths)\n",
    "\n",
    "# 计算模型权重\n",
    "print(\"\\n⚖️ 计算模型权重...\")\n",
    "model_weights = calculate_model_weights(model_results, weight_method='accuracy')\n",
    "print(\"模型权重分配:\")\n",
    "for model_key, weight in model_weights.items():\n",
    "    print(f\"  {MODEL_CONFIGS[model_key]['name']:15}: {weight:.4f}\")\n",
    "\n",
    "# 软投票预测\n",
    "print(\"\\n📊 软投票集成预测:\")\n",
    "soft_predictions, true_labels, _ = ensemble_predict(trained_models, val_loader, voting_type='soft')\n",
    "soft_accuracy = accuracy_score(true_labels, soft_predictions)\n",
    "print(f\"软投票准确率: {soft_accuracy:.4f}\")\n",
    "\n",
    "# 硬投票预测\n",
    "print(\"\\n📊 硬投票集成预测:\")\n",
    "hard_predictions, _, _ = ensemble_predict(trained_models, val_loader, voting_type='hard')\n",
    "hard_accuracy = accuracy_score(true_labels, hard_predictions)\n",
    "print(f\"硬投票准确率: {hard_accuracy:.4f}\")\n",
    "\n",
    "# 加权投票预测\n",
    "print(\"\\n📊 加权投票集成预测:\")\n",
    "weighted_predictions, _, _ = ensemble_predict(trained_models, val_loader, voting_type='weighted', weights=model_weights)\n",
    "weighted_accuracy = accuracy_score(true_labels, weighted_predictions)\n",
    "print(f\"加权投票准确率: {weighted_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 13: 结果对比和可视化\n",
    "# 单模型结果对比\n",
    "print(\"\\n📈 模型性能对比:\")\n",
    "print(\"=\"*50)\n",
    "for model_key in selected_models:\n",
    "    best_acc = model_results[model_key]['best_acc']\n",
    "    print(f\"{MODEL_CONFIGS[model_key]['name']:15}: {best_acc:.4f}\")\n",
    "\n",
    "print(f\"{'软投票集成':15}: {soft_accuracy:.4f}\")\n",
    "print(f\"{'硬投票集成':15}: {hard_accuracy:.4f}\")\n",
    "print(f\"{'加权投票集成':15}: {weighted_accuracy:.4f}\")\n",
    "\n",
    "# 增强混淆矩阵可视化\n",
    "print(\"\\n📊 生成混淆矩阵可视化...\")\n",
    "plot_confusion_matrix(true_labels, soft_predictions, \"Soft Voting Ensemble\", \"soft_voting\")\n",
    "plot_confusion_matrix(true_labels, hard_predictions, \"Hard Voting Ensemble\", \"hard_voting\")\n",
    "plot_confusion_matrix(true_labels, weighted_predictions, \"Weighted Voting Ensemble\", \"weighted_voting\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 14: 详细分类报告\n",
    "print(\"\\n📋 软投票详细分类报告:\")\n",
    "print(\"=\"*50)\n",
    "print(classification_report(true_labels, soft_predictions, target_names=classes))\n",
    "\n",
    "print(\"\\n📋 硬投票详细分类报告:\")\n",
    "print(\"=\"*50)\n",
    "print(classification_report(true_labels, hard_predictions, target_names=classes))\n",
    "\n",
    "print(\"\\n📋 加权投票详细分类报告:\")\n",
    "print(\"=\"*50)\n",
    "print(classification_report(true_labels, weighted_predictions, target_names=classes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 15: 集成分析和解释性可视化\n",
    "print(\"\\n📊 生成集成分析可视化...\")\n",
    "plot_ensemble_analysis(trained_models, val_loader, device)\n",
    "\n",
    "print(\"\\n📊 生成模型解释性分析...\")\n",
    "plot_interpretability_analysis(trained_models, val_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 16: 最终总结\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"🎉 多模型集成训练完成！\")\n",
    "print(\"=\"*60)\n",
    "print(f\"训练的模型数量: {len(selected_models)}\")\n",
    "print(f\"最佳单模型准确率: {max([results['best_acc'] for results in model_results.values()]):.4f}\")\n",
    "print(f\"软投票集成准确率: {soft_accuracy:.4f}\")\n",
    "print(f\"硬投票集成准确率: {hard_accuracy:.4f}\")\n",
    "print(f\"加权投票集成准确率: {weighted_accuracy:.4f}\")\n",
    "\n",
    "# 计算提升幅度\n",
    "best_single = max([results['best_acc'] for results in model_results.values()])\n",
    "soft_improvement = (soft_accuracy - best_single) * 100\n",
    "hard_improvement = (hard_accuracy - best_single) * 100\n",
    "weighted_improvement = (weighted_accuracy - best_single) * 100\n",
    "\n",
    "print(f\"软投票相对提升: {soft_improvement:+.2f}%\")\n",
    "print(f\"硬投票相对提升: {hard_improvement:+.2f}%\")\n",
    "print(f\"加权投票相对提升: {weighted_improvement:+.2f}%\")\n",
    "\n",
    "# 找出最佳集成方法\n",
    "ensemble_results = {\n",
    "    '软投票': soft_accuracy,\n",
    "    '硬投票': hard_accuracy,\n",
    "    '加权投票': weighted_accuracy\n",
    "}\n",
    "best_ensemble = max(ensemble_results, key=ensemble_results.get)\n",
    "print(f\"\\n🏆 最佳集成方法: {best_ensemble} (准确率: {ensemble_results[best_ensemble]:.4f})\")\n",
    "\n",
    "print(f\"\\n💾 保存的模型文件:\")\n",
    "for model_key, path in model_paths.items():\n",
    "    print(f\"  {MODEL_CONFIGS[model_key]['name']}: {path}\")\n",
    "\n",
    "print(f\"\\n⚖️ 模型权重分配:\")\n",
    "for model_key, weight in model_weights.items():\n",
    "    print(f\"  {MODEL_CONFIGS[model_key]['name']}: {weight:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 1909705,
     "sourceId": 3134515,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
