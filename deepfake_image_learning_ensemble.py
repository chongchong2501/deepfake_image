# =====================
# Kaggleæ·±åº¦ä¼ªé€ å›¾åƒæ£€æµ‹ - å¤šæ¨¡å‹é›†æˆæŠ•ç¥¨ç‰ˆæœ¬
# =====================

# Cell 1: å¯¼å…¥ä¾èµ–å’Œç¯å¢ƒè®¾ç½®
import os
import cv2
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms, models
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score
import seaborn as sns
import matplotlib.pyplot as plt
from tqdm.auto import tqdm
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®éšæœºç§å­
torch.manual_seed(42)
np.random.seed(42)

print("ğŸš€ Kaggleå¤šæ¨¡å‹é›†æˆæ·±åº¦ä¼ªé€ æ£€æµ‹")
print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")

# Cell 2: å‚æ•°é…ç½®
# Kaggleç¯å¢ƒè·¯å¾„
BASE_PATH = '/kaggle/input/deepfake-and-real-images/Dataset'
TRAIN_PATH = os.path.join(BASE_PATH, 'Train')
VAL_PATH = os.path.join(BASE_PATH, 'Validation')

# è®­ç»ƒå‚æ•°
IMG_SIZE = 256
BATCH_SIZE = 32
LEARNING_RATE = 1e-4
EPOCHS = 15
WEIGHT_DECAY = 1e-4
PATIENCE = 8

# è®¾å¤‡é…ç½®
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"ä½¿ç”¨è®¾å¤‡: {DEVICE}")
if torch.cuda.is_available():
    print(f"GPU: {torch.cuda.get_device_name(0)}")
    print(f"GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")

# Cell 3: æ•°æ®åŠ è½½å‡½æ•°
classes = ['Real', 'Fake']

def create_dataframe(data_path, dataset_type):
    """åˆ›å»ºæ•°æ®é›†DataFrame"""
    filepaths, labels = [], []
    
    for label_idx, cls in enumerate(classes):
        folder = os.path.join(data_path, cls)
        if os.path.exists(folder):
            for img_name in os.listdir(folder):
                if img_name.lower().endswith(('.png', '.jpg', '.jpeg')):
                    filepaths.append(os.path.join(folder, img_name))
                    labels.append(label_idx)
    
    df = pd.DataFrame({'filepath': filepaths, 'label': labels})
    print(f"{dataset_type}é›†å›¾ç‰‡æ•°: {len(df)}")
    if len(df) > 0:
        print(f"{dataset_type}é›†ç±»åˆ«åˆ†å¸ƒ:")
        for idx, cls in enumerate(classes):
            count = len(df[df['label'] == idx])
            print(f"  {cls}: {count} ({count/len(df)*100:.1f}%)")
    return df

# Cell 4: æ•°æ®é¢„å¤„ç†å’Œå¢å¼º
train_transform = transforms.Compose([
    transforms.ToPILImage(),
    transforms.Resize((IMG_SIZE, IMG_SIZE)),
    transforms.RandomHorizontalFlip(p=0.5),
    transforms.RandomRotation(degrees=10),
    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

val_transform = transforms.Compose([
    transforms.ToPILImage(),
    transforms.Resize((IMG_SIZE, IMG_SIZE)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

class DeepfakeDataset(Dataset):
    def __init__(self, df, transform=None):
        self.df = df
        self.transform = transform
        
    def __len__(self):
        return len(self.df)
    
    def __getitem__(self, idx):
        img_path = self.df.iloc[idx]['filepath']
        img = cv2.imread(img_path)
        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        
        if self.transform:
            img = self.transform(img)
        
        label = self.df.iloc[idx]['label']
        return img, label

# Cell 5: æ¨¡å‹å®šä¹‰
def create_efficientnet_b0():
    """åˆ›å»ºEfficientNet-B0æ¨¡å‹"""
    model = models.efficientnet_b0(weights='IMAGENET1K_V1')
    model.classifier[1] = nn.Linear(1280, 2)
    return model

def create_efficientnet_b1():
    """åˆ›å»ºEfficientNet-B1æ¨¡å‹"""
    model = models.efficientnet_b1(weights='IMAGENET1K_V1')
    model.classifier[1] = nn.Linear(1280, 2)
    return model

def create_resnet50():
    """åˆ›å»ºResNet50æ¨¡å‹"""
    model = models.resnet50(weights='IMAGENET1K_V1')
    model.fc = nn.Linear(2048, 2)
    return model

def create_convnext_tiny():
    """åˆ›å»ºConvNeXt-Tinyæ¨¡å‹"""
    model = models.convnext_tiny(weights='IMAGENET1K_V1')
    model.classifier[2] = nn.Linear(768, 2)
    return model

# æ¨¡å‹é…ç½®å­—å…¸
MODEL_CONFIGS = {
    'efficientnet_b0': {
        'create_fn': create_efficientnet_b0,
        'name': 'EfficientNet-B0'
    },
    'efficientnet_b1': {
        'create_fn': create_efficientnet_b1,
        'name': 'EfficientNet-B1'
    },
    'resnet50': {
        'create_fn': create_resnet50,
        'name': 'ResNet50'
    },
    'convnext_tiny': {
        'create_fn': create_convnext_tiny,
        'name': 'ConvNeXt-Tiny'
    }
}

# Cell 6: å•æ¨¡å‹è®­ç»ƒå‡½æ•°
def train_single_model(model_key, train_loader, val_loader, save_path):
    """è®­ç»ƒå•ä¸ªæ¨¡å‹"""
    print(f"\nğŸ”¥ å¼€å§‹è®­ç»ƒ {MODEL_CONFIGS[model_key]['name']}")
    
    # åˆ›å»ºæ¨¡å‹
    model = MODEL_CONFIGS[model_key]['create_fn']()
    model = model.to(DEVICE)
    
    # æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)
    
    # è®­ç»ƒè®°å½•
    best_val_acc = 0
    patience_counter = 0
    train_losses, val_losses, val_accuracies = [], [], []
    
    for epoch in range(EPOCHS):
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        train_loss = 0
        for imgs, labels in tqdm(train_loader, desc=f"Epoch {epoch+1}/{EPOCHS} [Train]"):
            imgs, labels = imgs.to(DEVICE), labels.to(DEVICE)
            
            optimizer.zero_grad()
            outputs = model(imgs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            train_loss += loss.item()
        
        train_loss /= len(train_loader)
        train_losses.append(train_loss)
        
        # éªŒè¯é˜¶æ®µ
        model.eval()
        val_loss = 0
        correct = 0
        total = 0
        with torch.no_grad():
            for imgs, labels in tqdm(val_loader, desc=f"Epoch {epoch+1}/{EPOCHS} [Val]"):
                imgs, labels = imgs.to(DEVICE), labels.to(DEVICE)
                outputs = model(imgs)
                loss = criterion(outputs, labels)
                val_loss += loss.item()
                
                _, predicted = torch.max(outputs, 1)
                correct += (predicted == labels).sum().item()
                total += labels.size(0)
        
        val_loss /= len(val_loader)
        val_acc = correct / total
        val_losses.append(val_loss)
        val_accuracies.append(val_acc)
        
        scheduler.step(val_loss)
        current_lr = optimizer.param_groups[0]['lr']
        
        print(f"Epoch {epoch+1}: Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, LR: {current_lr:.6f}")
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            patience_counter = 0
            torch.save(model.state_dict(), save_path)
            print(f"âœ… æœ€ä½³æ¨¡å‹å·²ä¿å­˜ï¼ŒéªŒè¯å‡†ç¡®ç‡: {best_val_acc:.4f}")
        else:
            patience_counter += 1
            if patience_counter >= PATIENCE:
                print("â›” Early stopping triggered")
                break
    
    return best_val_acc, train_losses, val_losses, val_accuracies

# Cell 7: é›†æˆé¢„æµ‹å‡½æ•°
def load_trained_models(model_paths):
    """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
    models_dict = {}
    for model_key, path in model_paths.items():
        if os.path.exists(path):
            model = MODEL_CONFIGS[model_key]['create_fn']()
            model.load_state_dict(torch.load(path, map_location=DEVICE))
            model = model.to(DEVICE)
            model.eval()
            models_dict[model_key] = model
            print(f"âœ… å·²åŠ è½½ {MODEL_CONFIGS[model_key]['name']}")
        else:
            print(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {path}")
    return models_dict

def ensemble_predict(models_dict, data_loader, voting_type='soft', weights=None):
    """é›†æˆé¢„æµ‹"""
    all_predictions = []
    all_labels = []
    model_outputs = {key: [] for key in models_dict.keys()}
    
    # å¦‚æœæ˜¯åŠ æƒæŠ•ç¥¨ä½†æ²¡æœ‰æä¾›æƒé‡ï¼Œåˆ™ä½¿ç”¨ç­‰æƒé‡
    if voting_type == 'weighted' and weights is None:
        weights = {key: 1.0 for key in models_dict.keys()}
    
    with torch.no_grad():
        for imgs, labels in tqdm(data_loader, desc="é›†æˆé¢„æµ‹"):
            imgs, labels = imgs.to(DEVICE), labels.to(DEVICE)
            
            # æ”¶é›†æ¯ä¸ªæ¨¡å‹çš„é¢„æµ‹
            batch_predictions = []
            for model_key, model in models_dict.items():
                outputs = model(imgs)
                if voting_type in ['soft', 'weighted']:
                    probs = torch.softmax(outputs, dim=1)
                    batch_predictions.append(probs.cpu().numpy())
                else:  # hard voting
                    _, predicted = torch.max(outputs, 1)
                    batch_predictions.append(predicted.cpu().numpy())
                
                model_outputs[model_key].extend(outputs.cpu().numpy())
            
            # é›†æˆé¢„æµ‹
            if voting_type == 'soft':
                # è½¯æŠ•ç¥¨ï¼šå¹³å‡æ¦‚ç‡
                ensemble_probs = np.mean(batch_predictions, axis=0)
                ensemble_pred = np.argmax(ensemble_probs, axis=1)
            elif voting_type == 'weighted':
                # åŠ æƒæŠ•ç¥¨ï¼šæ ¹æ®æƒé‡åŠ æƒå¹³å‡æ¦‚ç‡
                weighted_probs = np.zeros_like(batch_predictions[0])
                total_weight = 0
                for i, (model_key, probs) in enumerate(zip(models_dict.keys(), batch_predictions)):
                    weight = weights[model_key]
                    weighted_probs += probs * weight
                    total_weight += weight
                ensemble_probs = weighted_probs / total_weight
                ensemble_pred = np.argmax(ensemble_probs, axis=1)
            else:
                # ç¡¬æŠ•ç¥¨ï¼šå¤šæ•°æŠ•ç¥¨
                batch_predictions = np.array(batch_predictions)
                ensemble_pred = []
                for i in range(batch_predictions.shape[1]):
                    votes = batch_predictions[:, i]
                    ensemble_pred.append(np.bincount(votes).argmax())
                ensemble_pred = np.array(ensemble_pred)
            
            all_predictions.extend(ensemble_pred)
            all_labels.extend(labels.cpu().numpy())
    
    return np.array(all_predictions), np.array(all_labels), model_outputs

def calculate_model_weights(model_results, weight_method='accuracy'):
    """è®¡ç®—æ¨¡å‹æƒé‡"""
    weights = {}
    
    if weight_method == 'accuracy':
        # åŸºäºéªŒè¯å‡†ç¡®ç‡è®¡ç®—æƒé‡
        accuracies = {key: results['best_acc'] for key, results in model_results.items()}
        total_acc = sum(accuracies.values())
        
        for key, acc in accuracies.items():
            weights[key] = acc / total_acc
            
    elif weight_method == 'softmax':
        # ä½¿ç”¨softmaxå½’ä¸€åŒ–å‡†ç¡®ç‡ä½œä¸ºæƒé‡
        accuracies = np.array([results['best_acc'] for results in model_results.values()])
        softmax_weights = np.exp(accuracies * 10) / np.sum(np.exp(accuracies * 10))  # ä¹˜ä»¥10å¢å¼ºå·®å¼‚
        
        for i, key in enumerate(model_results.keys()):
            weights[key] = softmax_weights[i]
            
    elif weight_method == 'rank':
        # åŸºäºæ’åçš„æƒé‡åˆ†é…
        sorted_models = sorted(model_results.items(), key=lambda x: x[1]['best_acc'], reverse=True)
        n_models = len(sorted_models)
        
        for i, (key, _) in enumerate(sorted_models):
            weights[key] = (n_models - i) / sum(range(1, n_models + 1))
    
    return weights

# Cell 8: åŠ è½½æ•°æ®
print("ğŸ“‚ åŠ è½½æ•°æ®é›†...")
train_df = create_dataframe(TRAIN_PATH, "è®­ç»ƒ")
val_df = create_dataframe(VAL_PATH, "éªŒè¯")

print(f"\nğŸ“Š æ•°æ®é›†æ€»è§ˆ:")
print(f"è®­ç»ƒé›†æ€»æ•°: {len(train_df)}")
print(f"éªŒè¯é›†æ€»æ•°: {len(val_df)}")

# åˆ›å»ºæ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨
train_dataset = DeepfakeDataset(train_df, transform=train_transform)
val_dataset = DeepfakeDataset(val_df, transform=val_transform)

# ä¿®å¤å¤šè¿›ç¨‹é”™è¯¯ï¼šè®¾ç½®num_workers=0é¿å…è¿›ç¨‹ç®¡ç†é—®é¢˜
train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, pin_memory=True)
val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=True)

# Cell 9: è®­ç»ƒæ‰€æœ‰æ¨¡å‹
print("\nğŸš€ å¼€å§‹è®­ç»ƒå¤šä¸ªæ¨¡å‹...")

# é€‰æ‹©è¦è®­ç»ƒçš„æ¨¡å‹ï¼ˆå¯ä»¥æ ¹æ®éœ€è¦è°ƒæ•´ï¼‰
selected_models = ['efficientnet_b0', 'resnet50', 'convnext_tiny']  # å‡å°‘æ¨¡å‹æ•°é‡ä»¥é€‚åº”Kaggleç¯å¢ƒ
model_paths = {}
model_results = {}

for model_key in selected_models:
    save_path = f"best_{model_key}_model.pth"
    model_paths[model_key] = save_path
    
    best_acc, train_losses, val_losses, val_accs = train_single_model(
        model_key, train_loader, val_loader, save_path
    )
    
    model_results[model_key] = {
        'best_acc': best_acc,
        'train_losses': train_losses,
        'val_losses': val_losses,
        'val_accuracies': val_accs
    }
    
    print(f"âœ… {MODEL_CONFIGS[model_key]['name']} è®­ç»ƒå®Œæˆï¼Œæœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_acc:.4f}")

# Cell 10: å¯è§†åŒ–è®­ç»ƒè¿‡ç¨‹
fig, axes = plt.subplots(2, 2, figsize=(15, 10))
fig.suptitle('å¤šæ¨¡å‹è®­ç»ƒè¿‡ç¨‹', fontsize=16)

for idx, (model_key, results) in enumerate(model_results.items()):
    row = idx // 2
    col = idx % 2
    
    ax = axes[row, col]
    epochs = range(1, len(results['train_losses']) + 1)
    
    ax.plot(epochs, results['train_losses'], 'b-', label='Train Loss')
    ax.plot(epochs, results['val_losses'], 'r-', label='Val Loss')
    ax.set_title(f"{MODEL_CONFIGS[model_key]['name']}")
    ax.set_xlabel('Epoch')
    ax.set_ylabel('Loss')
    ax.legend()
    ax.grid(True)

# å¦‚æœæ¨¡å‹æ•°é‡å°‘äº4ä¸ªï¼Œéšè—å¤šä½™çš„å­å›¾
for idx in range(len(model_results), 4):
    row = idx // 2
    col = idx % 2
    axes[row, col].set_visible(False)

plt.tight_layout()
plt.show()

# Cell 11: é›†æˆé¢„æµ‹å’Œè¯„ä¼°
print("\nğŸ”® å¼€å§‹é›†æˆé¢„æµ‹...")

# åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹
trained_models = load_trained_models(model_paths)

# è®¡ç®—æ¨¡å‹æƒé‡
print("\nâš–ï¸ è®¡ç®—æ¨¡å‹æƒé‡...")
model_weights = calculate_model_weights(model_results, weight_method='accuracy')
print("æ¨¡å‹æƒé‡åˆ†é…:")
for model_key, weight in model_weights.items():
    print(f"  {MODEL_CONFIGS[model_key]['name']:15}: {weight:.4f}")

# è½¯æŠ•ç¥¨é¢„æµ‹
print("\nğŸ“Š è½¯æŠ•ç¥¨é›†æˆé¢„æµ‹:")
soft_predictions, true_labels, _ = ensemble_predict(trained_models, val_loader, voting_type='soft')
soft_accuracy = accuracy_score(true_labels, soft_predictions)
print(f"è½¯æŠ•ç¥¨å‡†ç¡®ç‡: {soft_accuracy:.4f}")

# ç¡¬æŠ•ç¥¨é¢„æµ‹
print("\nğŸ“Š ç¡¬æŠ•ç¥¨é›†æˆé¢„æµ‹:")
hard_predictions, _, _ = ensemble_predict(trained_models, val_loader, voting_type='hard')
hard_accuracy = accuracy_score(true_labels, hard_predictions)
print(f"ç¡¬æŠ•ç¥¨å‡†ç¡®ç‡: {hard_accuracy:.4f}")

# åŠ æƒæŠ•ç¥¨é¢„æµ‹
print("\nğŸ“Š åŠ æƒæŠ•ç¥¨é›†æˆé¢„æµ‹:")
weighted_predictions, _, _ = ensemble_predict(trained_models, val_loader, voting_type='weighted', weights=model_weights)
weighted_accuracy = accuracy_score(true_labels, weighted_predictions)
print(f"åŠ æƒæŠ•ç¥¨å‡†ç¡®ç‡: {weighted_accuracy:.4f}")

# Cell 12: ç»“æœå¯¹æ¯”å’Œå¯è§†åŒ–
# å•æ¨¡å‹ç»“æœå¯¹æ¯”
print("\nğŸ“ˆ æ¨¡å‹æ€§èƒ½å¯¹æ¯”:")
print("="*50)
for model_key in selected_models:
    best_acc = model_results[model_key]['best_acc']
    print(f"{MODEL_CONFIGS[model_key]['name']:15}: {best_acc:.4f}")

print(f"{'è½¯æŠ•ç¥¨é›†æˆ':15}: {soft_accuracy:.4f}")
print(f"{'ç¡¬æŠ•ç¥¨é›†æˆ':15}: {hard_accuracy:.4f}")
print(f"{'åŠ æƒæŠ•ç¥¨é›†æˆ':15}: {weighted_accuracy:.4f}")

# æ··æ·†çŸ©é˜µå¯è§†åŒ–
fig, axes = plt.subplots(1, 3, figsize=(18, 5))

# è½¯æŠ•ç¥¨æ··æ·†çŸ©é˜µ
cm_soft = confusion_matrix(true_labels, soft_predictions)
sns.heatmap(cm_soft, annot=True, fmt="d", cmap="Blues", 
            xticklabels=classes, yticklabels=classes, ax=axes[0])
axes[0].set_title(f"è½¯æŠ•ç¥¨æ··æ·†çŸ©é˜µ\nå‡†ç¡®ç‡: {soft_accuracy:.4f}")
axes[0].set_ylabel("çœŸå®æ ‡ç­¾")
axes[0].set_xlabel("é¢„æµ‹æ ‡ç­¾")

# ç¡¬æŠ•ç¥¨æ··æ·†çŸ©é˜µ
cm_hard = confusion_matrix(true_labels, hard_predictions)
sns.heatmap(cm_hard, annot=True, fmt="d", cmap="Greens", 
            xticklabels=classes, yticklabels=classes, ax=axes[1])
axes[1].set_title(f"ç¡¬æŠ•ç¥¨æ··æ·†çŸ©é˜µ\nå‡†ç¡®ç‡: {hard_accuracy:.4f}")
axes[1].set_ylabel("çœŸå®æ ‡ç­¾")
axes[1].set_xlabel("é¢„æµ‹æ ‡ç­¾")

# åŠ æƒæŠ•ç¥¨æ··æ·†çŸ©é˜µ
cm_weighted = confusion_matrix(true_labels, weighted_predictions)
sns.heatmap(cm_weighted, annot=True, fmt="d", cmap="Oranges", 
            xticklabels=classes, yticklabels=classes, ax=axes[2])
axes[2].set_title(f"åŠ æƒæŠ•ç¥¨æ··æ·†çŸ©é˜µ\nå‡†ç¡®ç‡: {weighted_accuracy:.4f}")
axes[2].set_ylabel("çœŸå®æ ‡ç­¾")
axes[2].set_xlabel("é¢„æµ‹æ ‡ç­¾")

plt.tight_layout()
plt.show()

# Cell 13: è¯¦ç»†åˆ†ç±»æŠ¥å‘Š
print("\nğŸ“‹ è½¯æŠ•ç¥¨è¯¦ç»†åˆ†ç±»æŠ¥å‘Š:")
print("="*50)
print(classification_report(true_labels, soft_predictions, target_names=classes))

print("\nğŸ“‹ ç¡¬æŠ•ç¥¨è¯¦ç»†åˆ†ç±»æŠ¥å‘Š:")
print("="*50)
print(classification_report(true_labels, hard_predictions, target_names=classes))

print("\nğŸ“‹ åŠ æƒæŠ•ç¥¨è¯¦ç»†åˆ†ç±»æŠ¥å‘Š:")
print("="*50)
print(classification_report(true_labels, weighted_predictions, target_names=classes))

# Cell 14: æœ€ç»ˆæ€»ç»“
print("\n" + "="*60)
print("ğŸ‰ å¤šæ¨¡å‹é›†æˆè®­ç»ƒå®Œæˆï¼")
print("="*60)
print(f"è®­ç»ƒçš„æ¨¡å‹æ•°é‡: {len(selected_models)}")
print(f"æœ€ä½³å•æ¨¡å‹å‡†ç¡®ç‡: {max([results['best_acc'] for results in model_results.values()]):.4f}")
print(f"è½¯æŠ•ç¥¨é›†æˆå‡†ç¡®ç‡: {soft_accuracy:.4f}")
print(f"ç¡¬æŠ•ç¥¨é›†æˆå‡†ç¡®ç‡: {hard_accuracy:.4f}")
print(f"åŠ æƒæŠ•ç¥¨é›†æˆå‡†ç¡®ç‡: {weighted_accuracy:.4f}")

# è®¡ç®—æå‡å¹…åº¦
best_single = max([results['best_acc'] for results in model_results.values()])
soft_improvement = (soft_accuracy - best_single) * 100
hard_improvement = (hard_accuracy - best_single) * 100
weighted_improvement = (weighted_accuracy - best_single) * 100

print(f"è½¯æŠ•ç¥¨ç›¸å¯¹æå‡: {soft_improvement:+.2f}%")
print(f"ç¡¬æŠ•ç¥¨ç›¸å¯¹æå‡: {hard_improvement:+.2f}%")
print(f"åŠ æƒæŠ•ç¥¨ç›¸å¯¹æå‡: {weighted_improvement:+.2f}%")

# æ‰¾å‡ºæœ€ä½³é›†æˆæ–¹æ³•
ensemble_results = {
    'è½¯æŠ•ç¥¨': soft_accuracy,
    'ç¡¬æŠ•ç¥¨': hard_accuracy,
    'åŠ æƒæŠ•ç¥¨': weighted_accuracy
}
best_ensemble = max(ensemble_results, key=ensemble_results.get)
print(f"\nğŸ† æœ€ä½³é›†æˆæ–¹æ³•: {best_ensemble} (å‡†ç¡®ç‡: {ensemble_results[best_ensemble]:.4f})")

print(f"\nğŸ’¾ ä¿å­˜çš„æ¨¡å‹æ–‡ä»¶:")
for model_key, path in model_paths.items():
    print(f"  {MODEL_CONFIGS[model_key]['name']}: {path}")

print(f"\nâš–ï¸ æ¨¡å‹æƒé‡åˆ†é…:")
for model_key, weight in model_weights.items():
    print(f"  {MODEL_CONFIGS[model_key]['name']}: {weight:.4f}")