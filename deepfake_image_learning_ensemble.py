# =====================
# Kaggleæ·±åº¦ä¼ªé€ å›¾åƒæ£€æµ‹ - å¤šæ¨¡å‹é›†æˆæŠ•ç¥¨ç‰ˆæœ¬
# =====================

# Cell 1: å¯¼å…¥ä¾èµ–å’Œç¯å¢ƒè®¾ç½®
import os
import cv2
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms, models
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, f1_score
import seaborn as sns
import matplotlib.pyplot as plt
from matplotlib import font_manager
import matplotlib.patches as patches
from tqdm.auto import tqdm
import warnings
import time
from PIL import Image
import gc
warnings.filterwarnings('ignore')

# è§£é‡Šå·¥å…·å¯¼å…¥
try:
    from captum.attr import LayerGradCam, IntegratedGradients
    from captum.attr import visualization as viz
    CAPTUM_AVAILABLE = True
except ImportError:
    print("âš ï¸ Captum not available. Install with: pip install captum")
    CAPTUM_AVAILABLE = False

# è®¾ç½®matplotlibä½¿ç”¨è‹±æ–‡å­—ä½“å’Œé«˜DPI
plt.rcParams['font.family'] = 'DejaVu Sans'
plt.rcParams['font.size'] = 10
plt.rcParams['figure.dpi'] = 300
plt.rcParams['savefig.dpi'] = 300
plt.rcParams['savefig.bbox'] = 'tight'

# è®¾ç½®éšæœºç§å­
torch.manual_seed(42)
np.random.seed(42)

print("ğŸš€ Kaggle Multi-Model Ensemble Deepfake Detection")
print(f"PyTorch Version: {torch.__version__}")
print(f"Captum Available: {CAPTUM_AVAILABLE}")

# Cell 2: å‚æ•°é…ç½®
BASE_PATH = '/kaggle/input/deepfake-and-real-images/Dataset'
TRAIN_PATH = os.path.join(BASE_PATH, 'Train')
VAL_PATH = os.path.join(BASE_PATH, 'Validation')

# è®­ç»ƒå‚æ•°
# å›¾åƒå¤§å°
IMG_SIZE = 256

# è®­ç»ƒæ‰¹æ¬¡å¤§å°
BATCH_SIZE = 32

# å­¦ä¹ ç‡
LEARNING_RATE = 1e-4

# è®­ç»ƒè½®æ•°
EPOCHS = 30

# æƒé‡è¡°å‡ç³»æ•°
WEIGHT_DECAY = 1e-4

# æ—©åœè½®æ•°
PATIENCE = 5

# æ•°æ®åŠ è½½å™¨çš„å·¥ä½œè¿›ç¨‹æ•°é‡
NUM_WORKERS = 4

# å¤šGPUè®¾ç½®
NUM_GPUS = torch.cuda.device_count()
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Device: {DEVICE}")
if torch.cuda.is_available():
    if NUM_GPUS > 1:
        print(f"Multi-GPU Training: {[torch.cuda.get_device_name(i) for i in range(NUM_GPUS)]}")
        print(f"GPU Count: {NUM_GPUS}")
        NUM_WORKERS = 4  # å¤šGPUæ—¶å¢åŠ æ•°æ®åŠ è½½çº¿ç¨‹
    else:
        print(f"GPU: {torch.cuda.get_device_name(0)}")
        NUM_WORKERS = 2  # å•GPUæ—¶å‡å°‘æ•°æ®åŠ è½½çº¿ç¨‹
    
    for i in range(NUM_GPUS):
        print(f"GPU {i}: {torch.cuda.get_device_name(i)}")
        print(f"GPU {i} Memory: {torch.cuda.get_device_properties(i).total_memory / 1024**3:.1f}GB")
else:
    NUM_WORKERS = 0
    print("Using CPU Training")

# åˆ›å»ºè¾“å‡ºç›®å½•
PLOTS_DIR = './works/plots'
os.makedirs(PLOTS_DIR, exist_ok=True)
print(f"Plots will be saved to: {PLOTS_DIR}")



# Cell 3: æ•°æ®åŠ è½½å‡½æ•°
classes = ['Real', 'Fake']

def create_dataframe(data_path, dataset_type):
    """åˆ›å»ºæ•°æ®é›†DataFrame"""
    filepaths, labels = [], []
    
    for label_idx, cls in enumerate(classes):
        folder = os.path.join(data_path, cls)
        if os.path.exists(folder):
            for img_name in os.listdir(folder):
                if img_name.lower().endswith(('.png', '.jpg', '.jpeg')):
                    filepaths.append(os.path.join(folder, img_name))
                    labels.append(label_idx)
    
    df = pd.DataFrame({'filepath': filepaths, 'label': labels})
    print(f"{dataset_type}é›†å›¾ç‰‡æ•°: {len(df)}")
    if len(df) > 0:
        print(f"{dataset_type}é›†ç±»åˆ«åˆ†å¸ƒ:")
        for idx, cls in enumerate(classes):
            count = len(df[df['label'] == idx])
            print(f"  {cls}: {count} ({count/len(df)*100:.1f}%)")
    return df

# Cell 4: æ•°æ®é¢„å¤„ç†å’Œå¢å¼º
train_transform = transforms.Compose([
    transforms.ToPILImage(),
    transforms.Resize((IMG_SIZE, IMG_SIZE)),
    transforms.RandomHorizontalFlip(p=0.5),
    transforms.RandomRotation(degrees=10),
    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

val_transform = transforms.Compose([
    transforms.ToPILImage(),
    transforms.Resize((IMG_SIZE, IMG_SIZE)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

class DeepfakeDataset(Dataset):
    def __init__(self, df, transform=None):
        self.df = df
        self.transform = transform
        
    def __len__(self):
        return len(self.df)
    
    def __getitem__(self, idx):
        img_path = self.df.iloc[idx]['filepath']
        img = cv2.imread(img_path)
        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        
        if self.transform:
            img = self.transform(img)
        
        label = self.df.iloc[idx]['label']
        return img, label

# Cell 5: æ¨¡å‹å®šä¹‰
def create_efficientnet_b0():
    """åˆ›å»ºEfficientNet-B0æ¨¡å‹"""
    model = models.efficientnet_b0(weights='IMAGENET1K_V1')
    model.classifier[1] = nn.Linear(1280, 2)
    return model

def create_resnet18():
    """åˆ›å»ºResNet18æ¨¡å‹"""
    model = models.resnet18(weights='IMAGENET1K_V1')
    model.fc = nn.Linear(512, 2)
    return model

def create_convnext_tiny():
    """åˆ›å»ºConvNeXt-Tinyæ¨¡å‹"""
    model = models.convnext_tiny(weights='IMAGENET1K_V1')
    model.classifier[2] = nn.Linear(768, 2)
    return model

# æ¨¡å‹é…ç½®å­—å…¸
MODEL_CONFIGS = {
    'efficientnet_b0': {
        'create_fn': create_efficientnet_b0,
        'name': 'EfficientNet-B0'
    },
    'resnet18': {
        'create_fn': create_resnet18,
        'name': 'ResNet18'
    },
    'convnext_tiny': {
        'create_fn': create_convnext_tiny,
        'name': 'ConvNeXt-Tiny'
    }
}

# Cell 6: å•æ¨¡å‹è®­ç»ƒå‡½æ•°
def train_single_model(model_key, train_loader, val_loader, save_path):
    """è®­ç»ƒå•ä¸ªæ¨¡å‹"""
    print(f"\nğŸ”¥ Starting Training {MODEL_CONFIGS[model_key]['name']}")
    
    # è®°å½•è®­ç»ƒå¼€å§‹æ—¶é—´
    start_time = time.time()
    
    # åˆ›å»ºæ¨¡å‹
    model = MODEL_CONFIGS[model_key]['create_fn']()
    model = model.to(DEVICE)
    
    # å¤šGPUæ”¯æŒ
    if NUM_GPUS > 1:
        model = nn.DataParallel(model)
        print(f"âœ… Model configured for multi-GPU training with {NUM_GPUS} GPUs")
    
    # æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)
    
    # è®­ç»ƒè®°å½•
    best_val_acc = 0
    patience_counter = 0
    train_losses, val_losses, val_accuracies, learning_rates = [], [], [], []
    val_f1_scores = []
    
    for epoch in range(EPOCHS):
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        train_loss = 0
        for imgs, labels in tqdm(train_loader, desc=f"Epoch {epoch+1}/{EPOCHS} [Train]"):
            imgs, labels = imgs.to(DEVICE), labels.to(DEVICE)
            
            optimizer.zero_grad()
            outputs = model(imgs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            train_loss += loss.item()
        
        train_loss /= len(train_loader)
        train_losses.append(train_loss)
        
        # éªŒè¯é˜¶æ®µ
        model.eval()
        val_loss = 0
        correct = 0
        total = 0
        all_val_preds = []
        all_val_labels = []
        
        with torch.no_grad():
            for imgs, labels in tqdm(val_loader, desc=f"Epoch {epoch+1}/{EPOCHS} [Val]"):
                imgs, labels = imgs.to(DEVICE), labels.to(DEVICE)
                outputs = model(imgs)
                loss = criterion(outputs, labels)
                val_loss += loss.item()
                
                _, predicted = torch.max(outputs, 1)
                correct += (predicted == labels).sum().item()
                total += labels.size(0)
                
                all_val_preds.extend(predicted.cpu().numpy())
                all_val_labels.extend(labels.cpu().numpy())
        
        val_loss /= len(val_loader)
        val_acc = correct / total
        val_f1 = f1_score(all_val_labels, all_val_preds, average='weighted')
        
        val_losses.append(val_loss)
        val_accuracies.append(val_acc)
        val_f1_scores.append(val_f1)
        
        scheduler.step(val_loss)
        current_lr = optimizer.param_groups[0]['lr']
        learning_rates.append(current_lr)
        
        print(f"Epoch {epoch+1}: Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, Val F1: {val_f1:.4f}, LR: {current_lr:.6f}")
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            patience_counter = 0
            # ä¿å­˜æ¨¡å‹æ—¶å¤„ç†å¤šGPUæƒ…å†µ
            if NUM_GPUS > 1:
                torch.save(model.module.state_dict(), save_path)
            else:
                torch.save(model.state_dict(), save_path)
            print(f"âœ… Best model saved, validation accuracy: {best_val_acc:.4f}")
        else:
            patience_counter += 1
            if patience_counter >= PATIENCE:
                print("â›” Early stopping triggered")
                break
    
    # è®¡ç®—è®­ç»ƒæ—¶é—´
    training_time = time.time() - start_time
    print(f"â±ï¸ Training completed in {training_time:.2f} seconds")
    
    return {
         'best_acc': best_val_acc,
         'train_losses': train_losses,
         'val_losses': val_losses,
         'val_accuracies': val_accuracies,
         'val_f1_scores': val_f1_scores,
         'learning_rates': learning_rates,
         'training_time': training_time
     }

# Cell 7: å¯è§†åŒ–å‡½æ•°
def plot_training_history(model_results, save_dir=PLOTS_DIR):
    """ç»˜åˆ¶è®­ç»ƒå†å²å¯è§†åŒ–"""
    print("ğŸ“Š Generating training history visualizations...")
    
    # 1. å•æ¨¡å‹è®­ç»ƒå†å² (2x2 å­å›¾)
    for model_key, results in model_results.items():
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        fig.suptitle(f'{MODEL_CONFIGS[model_key]["name"]} Training History', fontsize=16, fontweight='bold')
        
        epochs = range(1, len(results['train_losses']) + 1)
        
        # Lossæ›²çº¿
        axes[0, 0].plot(epochs, results['train_losses'], 'b-', label='Train Loss', linewidth=2)
        axes[0, 0].plot(epochs, results['val_losses'], 'r-', label='Validation Loss', linewidth=2)
        axes[0, 0].set_title('Training & Validation Loss', fontweight='bold')
        axes[0, 0].set_xlabel('Epoch')
        axes[0, 0].set_ylabel('Loss')
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)
        
        # Accuracyæ›²çº¿
        axes[0, 1].plot(epochs, results['val_accuracies'], 'g-', label='Validation Accuracy', linewidth=2)
        axes[0, 1].set_title('Validation Accuracy', fontweight='bold')
        axes[0, 1].set_xlabel('Epoch')
        axes[0, 1].set_ylabel('Accuracy')
        axes[0, 1].legend()
        axes[0, 1].grid(True, alpha=0.3)
        
        # Learning Rateæ›²çº¿
        axes[1, 0].plot(epochs, results['learning_rates'], 'purple', linewidth=2)
        axes[1, 0].set_title('Learning Rate Schedule', fontweight='bold')
        axes[1, 0].set_xlabel('Epoch')
        axes[1, 0].set_ylabel('Learning Rate')
        axes[1, 0].set_yscale('log')
        axes[1, 0].grid(True, alpha=0.3)
        
        # Validation Accuracyåˆ†å¸ƒ
        axes[1, 1].hist(results['val_accuracies'], bins=20, alpha=0.7, color='skyblue', edgecolor='black')
        axes[1, 1].axvline(results['best_acc'], color='red', linestyle='--', linewidth=2, label=f'Best: {results["best_acc"]:.4f}')
        axes[1, 1].set_title('Validation Accuracy Distribution', fontweight='bold')
        axes[1, 1].set_xlabel('Accuracy')
        axes[1, 1].set_ylabel('Frequency')
        axes[1, 1].legend()
        axes[1, 1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        save_path = os.path.join(save_dir, f'{model_key}_training_history.png')
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()
        print(f"âœ… Saved: {save_path}")
    
    # 2. å¤šæ¨¡å‹å¯¹æ¯”å›¾ (å››çº¿å¯¹æ¯”)
    fig, axes = plt.subplots(2, 2, figsize=(16, 12))
    fig.suptitle('Multi-Model Training Comparison', fontsize=16, fontweight='bold')
    
    colors = ['blue', 'red', 'green', 'orange', 'purple']
    
    # éªŒè¯Losså¯¹æ¯”
    for i, (model_key, results) in enumerate(model_results.items()):
        epochs = range(1, len(results['val_losses']) + 1)
        axes[0, 0].plot(epochs, results['val_losses'], color=colors[i % len(colors)], 
                       label=MODEL_CONFIGS[model_key]['name'], linewidth=2)
    axes[0, 0].set_title('Validation Loss Comparison', fontweight='bold')
    axes[0, 0].set_xlabel('Epoch')
    axes[0, 0].set_ylabel('Validation Loss')
    axes[0, 0].legend()
    axes[0, 0].grid(True, alpha=0.3)
    
    # éªŒè¯Accuracyå¯¹æ¯”
    for i, (model_key, results) in enumerate(model_results.items()):
        epochs = range(1, len(results['val_accuracies']) + 1)
        axes[0, 1].plot(epochs, results['val_accuracies'], color=colors[i % len(colors)], 
                       label=MODEL_CONFIGS[model_key]['name'], linewidth=2)
    axes[0, 1].set_title('Validation Accuracy Comparison', fontweight='bold')
    axes[0, 1].set_xlabel('Epoch')
    axes[0, 1].set_ylabel('Validation Accuracy')
    axes[0, 1].legend()
    axes[0, 1].grid(True, alpha=0.3)
    
    # è®­ç»ƒæ—¶é•¿å¯¹æ¯”
    model_names = [MODEL_CONFIGS[key]['name'] for key in model_results.keys()]
    training_times = [results['training_time'] for results in model_results.values()]
    bars = axes[1, 0].bar(model_names, training_times, color=colors[:len(model_names)], alpha=0.7)
    axes[1, 0].set_title('Training Time Comparison', fontweight='bold')
    axes[1, 0].set_ylabel('Training Time (seconds)')
    axes[1, 0].tick_params(axis='x', rotation=45)
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for bar, time_val in zip(bars, training_times):
        height = bar.get_height()
        axes[1, 0].text(bar.get_x() + bar.get_width()/2., height + height*0.01,
                       f'{time_val:.1f}s', ha='center', va='bottom', fontweight='bold')
    
    # F1-Scoreå¯¹æ¯”
    for i, (model_key, results) in enumerate(model_results.items()):
        epochs = range(1, len(results['val_f1_scores']) + 1)
        axes[1, 1].plot(epochs, results['val_f1_scores'], color=colors[i % len(colors)], 
                       label=MODEL_CONFIGS[model_key]['name'], linewidth=2)
    axes[1, 1].set_title('F1-Score Comparison', fontweight='bold')
    axes[1, 1].set_xlabel('Epoch')
    axes[1, 1].set_ylabel('F1-Score')
    axes[1, 1].legend()
    axes[1, 1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    save_path = os.path.join(save_dir, 'multi_model_comparison.png')
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"âœ… Saved: {save_path}")

def plot_confusion_matrix(y_true, y_pred, title, save_name, save_dir=PLOTS_DIR):
    """ç»˜åˆ¶æ··æ·†çŸ©é˜µ"""
    cm = confusion_matrix(y_true, y_pred)
    
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", 
                xticklabels=['Real', 'Fake'], yticklabels=['Real', 'Fake'],
                cbar_kws={'label': 'Count'})
    plt.title(title, fontsize=14, fontweight='bold')
    plt.ylabel('True Label', fontweight='bold')
    plt.xlabel('Predicted Label', fontweight='bold')
    
    # æ·»åŠ å‡†ç¡®ç‡ä¿¡æ¯
    accuracy = accuracy_score(y_true, y_pred)
    plt.text(0.5, -0.1, f'Accuracy: {accuracy:.4f}', 
             transform=plt.gca().transAxes, ha='center', fontweight='bold')
    
    save_path = os.path.join(save_dir, f'{save_name}_confusion_matrix.png')
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"âœ… Saved: {save_path}")

def plot_ensemble_analysis(models, val_loader, device, save_dir=PLOTS_DIR):
    """ç»˜åˆ¶é›†æˆåˆ†æå¯è§†åŒ–"""
    print("ğŸ“Š Generating ensemble analysis visualizations...")
    
    # æ”¶é›†æ‰€æœ‰æ¨¡å‹çš„é¢„æµ‹æ¦‚ç‡
    all_probs = []
    all_preds = []
    y_true = []
    
    for model in models:
        model.eval()
        probs = []
        preds = []
        
        with torch.no_grad():
            for inputs, labels in val_loader:
                inputs, labels = inputs.to(device), labels.to(device)
                outputs = model(inputs)
                prob = F.softmax(outputs, dim=1)
                probs.extend(prob.cpu().numpy())
                preds.extend(torch.argmax(outputs, dim=1).cpu().numpy())
                
                if len(y_true) == 0:  # åªåœ¨ç¬¬ä¸€ä¸ªæ¨¡å‹æ—¶æ”¶é›†çœŸå®æ ‡ç­¾
                    y_true.extend(labels.cpu().numpy())
        
        all_probs.append(np.array(probs))
        all_preds.append(np.array(preds))
    
    all_probs = np.array(all_probs)  # shape: (n_models, n_samples, n_classes)
    all_preds = np.array(all_preds)  # shape: (n_models, n_samples)
    y_true = np.array(y_true)
    
    # è®¡ç®—é›†æˆé¢„æµ‹
    ensemble_probs = np.mean(all_probs, axis=0)  # å¹³å‡æ¦‚ç‡
    ensemble_preds = np.argmax(ensemble_probs, axis=1)
    ensemble_confidence = np.max(ensemble_probs, axis=1)
    
    # åˆ›å»º2x2å­å›¾
    fig, axes = plt.subplots(2, 2, figsize=(16, 12))
    fig.suptitle('Ensemble Analysis', fontsize=16, fontweight='bold')
    
    # 1. é¢„æµ‹æ¦‚ç‡ç›´æ–¹å›¾
    for i, model_name in enumerate(['EfficientNet-B0', 'ResNet18', 'ConvNeXt-Tiny']):
        if i < len(all_probs):
            fake_probs = all_probs[i][:, 1]  # å‡å›¾ç‰‡çš„æ¦‚ç‡
            axes[0, 0].hist(fake_probs, bins=30, alpha=0.6, label=model_name, density=True)
    
    axes[0, 0].set_title('Prediction Probability Distribution (Fake Class)', fontweight='bold')
    axes[0, 0].set_xlabel('Probability')
    axes[0, 0].set_ylabel('Density')
    axes[0, 0].legend()
    axes[0, 0].grid(True, alpha=0.3)
    
    # 2. æ¨¡å‹ä¸€è‡´æ€§çƒ­å›¾
    n_models = len(all_preds)
    consistency_matrix = np.zeros((n_models, n_models))
    
    for i in range(n_models):
        for j in range(n_models):
            consistency_matrix[i, j] = np.mean(all_preds[i] == all_preds[j])
    
    model_names = ['EfficientNet-B0', 'ResNet18', 'ConvNeXt-Tiny'][:n_models]
    sns.heatmap(consistency_matrix, annot=True, fmt='.3f', cmap='YlOrRd',
                xticklabels=model_names, yticklabels=model_names, ax=axes[0, 1])
    axes[0, 1].set_title('Model Prediction Consistency', fontweight='bold')
    
    # 3. é›†æˆç½®ä¿¡åº¦å¯¹æ¯”ï¼ˆæ­£ç¡®vsé”™è¯¯é¢„æµ‹ï¼‰
    correct_mask = ensemble_preds == y_true
    correct_confidence = ensemble_confidence[correct_mask]
    incorrect_confidence = ensemble_confidence[~correct_mask]
    
    axes[1, 0].hist(correct_confidence, bins=30, alpha=0.7, label='Correct Predictions', 
                   color='green', density=True)
    axes[1, 0].hist(incorrect_confidence, bins=30, alpha=0.7, label='Incorrect Predictions', 
                   color='red', density=True)
    axes[1, 0].set_title('Ensemble Prediction Confidence', fontweight='bold')
    axes[1, 0].set_xlabel('Confidence')
    axes[1, 0].set_ylabel('Density')
    axes[1, 0].legend()
    axes[1, 0].grid(True, alpha=0.3)
    
    # 4. å„ç±»åˆ«F1-scoreæŸ±çŠ¶å›¾
    class_names = ['Real', 'Fake']
    # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„F1åˆ†æ•°
    f1_scores = f1_score(y_true, ensemble_preds, average=None)  # è¿”å›æ¯ä¸ªç±»åˆ«çš„F1åˆ†æ•°
    
    bars = axes[1, 1].bar(class_names, f1_scores, color=['skyblue', 'lightcoral'], alpha=0.8)
    axes[1, 1].set_title('Per-Class F1-Score', fontweight='bold')
    axes[1, 1].set_ylabel('F1-Score')
    axes[1, 1].set_ylim(0, 1)
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for bar, score in zip(bars, f1_scores):
        height = bar.get_height()
        axes[1, 1].text(bar.get_x() + bar.get_width()/2., height + 0.01,
                       f'{score:.3f}', ha='center', va='bottom', fontweight='bold')
    
    axes[1, 1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    save_path = os.path.join(save_dir, 'ensemble_analysis.png')
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"âœ… Saved: {save_path}")

def plot_interpretability_analysis(models, val_loader, device, save_dir=PLOTS_DIR, num_samples=4):
    """ç»˜åˆ¶æ¨¡å‹è§£é‡Šæ€§åˆ†æï¼ˆGrad-CAM + Integrated Gradientsï¼‰"""
    print("ğŸ“Š Generating interpretability analysis...")
    
    if not CAPTUM_AVAILABLE:
        print("âš ï¸ Captum not available, skipping interpretability analysis")
        return
    
    # è·å–ä¸€äº›æ ·æœ¬è¿›è¡Œåˆ†æ
    sample_images = []
    sample_labels = []
    sample_preds = []
    
    models[0].eval()
    with torch.no_grad():
        for inputs, labels in val_loader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = models[0](inputs)
            preds = torch.argmax(outputs, dim=1)
            
            # é€‰æ‹©ä¸€äº›æœ‰è¶£çš„æ ·æœ¬ï¼ˆé¢„æµ‹æ­£ç¡®å’Œé”™è¯¯çš„ï¼‰
            for i in range(min(num_samples, len(inputs))):
                sample_images.append(inputs[i])
                sample_labels.append(labels[i].item())
                sample_preds.append(preds[i].item())
            
            if len(sample_images) >= num_samples:
                break
    
    # ä¸ºæ¯ä¸ªæ¨¡å‹ç”Ÿæˆè§£é‡Š
    for model_idx, model in enumerate(models):
        model_name = ['EfficientNet-B0', 'ResNet18', 'ConvNeXt-Tiny'][model_idx]
        
        # åˆ›å»ºå­å›¾
        fig, axes = plt.subplots(num_samples, 4, figsize=(16, 4*num_samples))
        if num_samples == 1:
            axes = axes.reshape(1, -1)
        
        fig.suptitle(f'{model_name} - Interpretability Analysis', fontsize=16, fontweight='bold')
        
        for sample_idx in range(num_samples):
            input_tensor = sample_images[sample_idx].unsqueeze(0)
            true_label = sample_labels[sample_idx]
            pred_label = sample_preds[sample_idx]
            
            # åŸå§‹å›¾åƒ
            img_np = input_tensor.squeeze().cpu().numpy().transpose(1, 2, 0)
            img_np = (img_np - img_np.min()) / (img_np.max() - img_np.min())  # å½’ä¸€åŒ–åˆ°[0,1]
            axes[sample_idx, 0].imshow(img_np)
            axes[sample_idx, 0].set_title(f'Original\nTrue: {true_label}, Pred: {pred_label}')
            axes[sample_idx, 0].axis('off')
            
            try:
                # Grad-CAM
                if hasattr(model, 'features'):  # EfficientNet/ResNet
                    target_layer = model.features[-1]
                elif hasattr(model, 'stages'):  # ConvNeXt
                    target_layer = model.stages[-1]
                else:
                    # å°è¯•æ‰¾åˆ°æœ€åä¸€ä¸ªå·ç§¯å±‚
                    target_layer = None
                    for name, module in model.named_modules():
                        if isinstance(module, torch.nn.Conv2d):
                            target_layer = module
                
                if target_layer is not None:
                    grad_cam = LayerGradCam(model, target_layer)
                    attribution = grad_cam.attribute(input_tensor, target=pred_label)
                    
                    # æ˜¾ç¤ºGrad-CAM
                    grad_cam_np = attribution.squeeze().cpu().numpy()
                    axes[sample_idx, 1].imshow(grad_cam_np, cmap='jet', alpha=0.7)
                    axes[sample_idx, 1].imshow(img_np, alpha=0.3)
                    axes[sample_idx, 1].set_title('Grad-CAM')
                    axes[sample_idx, 1].axis('off')
                else:
                    axes[sample_idx, 1].text(0.5, 0.5, 'Grad-CAM\nNot Available', 
                                           ha='center', va='center', transform=axes[sample_idx, 1].transAxes)
                    axes[sample_idx, 1].axis('off')
                
                # Integrated Gradients
                ig = IntegratedGradients(model)
                attribution = ig.attribute(input_tensor, target=pred_label, n_steps=50)
                
                # æ˜¾ç¤ºIntegrated Gradients
                ig_np = attribution.squeeze().cpu().numpy()
                ig_np = np.transpose(ig_np, (1, 2, 0))
                ig_np = np.abs(ig_np).sum(axis=2)  # å–ç»å¯¹å€¼å¹¶æ±‚å’Œ
                axes[sample_idx, 2].imshow(ig_np, cmap='hot')
                axes[sample_idx, 2].set_title('Integrated Gradients')
                axes[sample_idx, 2].axis('off')
                
                # å åŠ æ˜¾ç¤º
                axes[sample_idx, 3].imshow(img_np, alpha=0.7)
                axes[sample_idx, 3].imshow(ig_np, cmap='hot', alpha=0.3)
                axes[sample_idx, 3].set_title('Overlay')
                axes[sample_idx, 3].axis('off')
                
            except Exception as e:
                print(f"âš ï¸ Error generating interpretability for sample {sample_idx}: {e}")
                for col in range(1, 4):
                    axes[sample_idx, col].text(0.5, 0.5, f'Error:\n{str(e)[:50]}...', 
                                             ha='center', va='center', transform=axes[sample_idx, col].transAxes)
                    axes[sample_idx, col].axis('off')
        
        plt.tight_layout()
        save_path = os.path.join(save_dir, f'{model_name.lower().replace("-", "_")}_interpretability.png')
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()
        print(f"âœ… Saved: {save_path}")

# Cell 8: é›†æˆé¢„æµ‹å‡½æ•°
def load_trained_models(model_paths):
    """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
    models_dict = {}
    for model_key, path in model_paths.items():
        if os.path.exists(path):
            model = MODEL_CONFIGS[model_key]['create_fn']()
            model.load_state_dict(torch.load(path, map_location=DEVICE))
            model = model.to(DEVICE)
            
            # å¤šGPUæ”¯æŒ
            if NUM_GPUS > 1:
                model = nn.DataParallel(model)
                print(f"âœ… å·²åŠ è½½ {MODEL_CONFIGS[model_key]['name']} (å¤šGPUæ¨¡å¼)")
            else:
                print(f"âœ… å·²åŠ è½½ {MODEL_CONFIGS[model_key]['name']}")
            
            model.eval()
            models_dict[model_key] = model
        else:
            print(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {path}")
    return models_dict

def ensemble_predict(models_dict, data_loader, voting_type='soft', weights=None):
    """é›†æˆé¢„æµ‹"""
    all_predictions = []
    all_labels = []
    model_outputs = {key: [] for key in models_dict.keys()}
    
    # å¦‚æœæ˜¯åŠ æƒæŠ•ç¥¨ä½†æ²¡æœ‰æä¾›æƒé‡ï¼Œåˆ™ä½¿ç”¨ç­‰æƒé‡
    if voting_type == 'weighted' and weights is None:
        weights = {key: 1.0 for key in models_dict.keys()}
    
    with torch.no_grad():
        for imgs, labels in tqdm(data_loader, desc="é›†æˆé¢„æµ‹"):
            imgs, labels = imgs.to(DEVICE), labels.to(DEVICE)
            
            # æ”¶é›†æ¯ä¸ªæ¨¡å‹çš„é¢„æµ‹
            batch_predictions = []
            for model_key, model in models_dict.items():
                outputs = model(imgs)
                if voting_type in ['soft', 'weighted']:
                    probs = torch.softmax(outputs, dim=1)
                    batch_predictions.append(probs.cpu().numpy())
                else:  # hard voting
                    _, predicted = torch.max(outputs, 1)
                    batch_predictions.append(predicted.cpu().numpy())
                
                model_outputs[model_key].extend(outputs.cpu().numpy())
            
            # é›†æˆé¢„æµ‹
            if voting_type == 'soft':
                # è½¯æŠ•ç¥¨ï¼šå¹³å‡æ¦‚ç‡
                ensemble_probs = np.mean(batch_predictions, axis=0)
                ensemble_pred = np.argmax(ensemble_probs, axis=1)
            elif voting_type == 'weighted':
                # åŠ æƒæŠ•ç¥¨ï¼šæ ¹æ®æƒé‡åŠ æƒå¹³å‡æ¦‚ç‡
                weighted_probs = np.zeros_like(batch_predictions[0])
                total_weight = 0
                for i, (model_key, probs) in enumerate(zip(models_dict.keys(), batch_predictions)):
                    weight = weights[model_key]
                    weighted_probs += probs * weight
                    total_weight += weight
                ensemble_probs = weighted_probs / total_weight
                ensemble_pred = np.argmax(ensemble_probs, axis=1)
            else:
                # ç¡¬æŠ•ç¥¨ï¼šå¤šæ•°æŠ•ç¥¨
                batch_predictions = np.array(batch_predictions)
                ensemble_pred = []
                for i in range(batch_predictions.shape[1]):
                    votes = batch_predictions[:, i]
                    ensemble_pred.append(np.bincount(votes).argmax())
                ensemble_pred = np.array(ensemble_pred)
            
            all_predictions.extend(ensemble_pred)
            all_labels.extend(labels.cpu().numpy())
    
    return np.array(all_predictions), np.array(all_labels), model_outputs

def calculate_model_weights(model_results, weight_method='accuracy'):
    """è®¡ç®—æ¨¡å‹æƒé‡"""
    weights = {}
    
    if weight_method == 'accuracy':
        # åŸºäºéªŒè¯å‡†ç¡®ç‡è®¡ç®—æƒé‡
        accuracies = {key: results['best_acc'] for key, results in model_results.items()}
        total_acc = sum(accuracies.values())
        
        for key, acc in accuracies.items():
            weights[key] = acc / total_acc
            
    elif weight_method == 'softmax':
        # ä½¿ç”¨softmaxå½’ä¸€åŒ–å‡†ç¡®ç‡ä½œä¸ºæƒé‡
        accuracies = np.array([results['best_acc'] for results in model_results.values()])
        softmax_weights = np.exp(accuracies * 10) / np.sum(np.exp(accuracies * 10))  # ä¹˜ä»¥10å¢å¼ºå·®å¼‚
        
        for i, key in enumerate(model_results.keys()):
            weights[key] = softmax_weights[i]
            
    elif weight_method == 'rank':
        # åŸºäºæ’åçš„æƒé‡åˆ†é…
        sorted_models = sorted(model_results.items(), key=lambda x: x[1]['best_acc'], reverse=True)
        n_models = len(sorted_models)
        
        for i, (key, _) in enumerate(sorted_models):
            weights[key] = (n_models - i) / sum(range(1, n_models + 1))
    
    return weights

# Cell 9: åŠ è½½æ•°æ®
print("ğŸ“‚ åŠ è½½æ•°æ®é›†...")
train_df = create_dataframe(TRAIN_PATH, "è®­ç»ƒ")
val_df = create_dataframe(VAL_PATH, "éªŒè¯")

# é™åˆ¶éªŒè¯é›†å¤§å°ä¸º6400ä»¥å‡å°‘å†…å­˜ä½¿ç”¨
MAX_VAL_SAMPLES = 6400
if len(val_df) > MAX_VAL_SAMPLES:
    print(f"âš ï¸ éªŒè¯é›†è¿‡å¤§ ({len(val_df)} å¼ )ï¼Œéšæœºé‡‡æ · {MAX_VAL_SAMPLES} å¼ å›¾ç‰‡")
    # ä¿æŒç±»åˆ«å¹³è¡¡çš„éšæœºé‡‡æ ·
    val_df = val_df.groupby('label', group_keys=False).apply(
        lambda x: x.sample(min(len(x), MAX_VAL_SAMPLES//2), random_state=42)
    ).reset_index(drop=True)
    print(f"âœ… éªŒè¯é›†é‡‡æ ·å®Œæˆï¼Œå½“å‰å¤§å°: {len(val_df)}")
    print(f"éªŒè¯é›†ç±»åˆ«åˆ†å¸ƒ:")
    for idx, cls in enumerate(classes):
        count = len(val_df[val_df['label'] == idx])
        print(f"  {cls}: {count} ({count/len(val_df)*100:.1f}%)")

print(f"\nğŸ“Š æ•°æ®é›†æ€»è§ˆ:")
print(f"è®­ç»ƒé›†æ€»æ•°: {len(train_df)}")
print(f"éªŒè¯é›†æ€»æ•°: {len(val_df)}")
print(f"éªŒè¯æ‰¹æ¬¡æ•°: {len(val_df) // BATCH_SIZE + (1 if len(val_df) % BATCH_SIZE > 0 else 0)}")

# åˆ›å»ºæ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨
train_dataset = DeepfakeDataset(train_df, transform=train_transform)
val_dataset = DeepfakeDataset(val_df, transform=val_transform)

# ä½¿ç”¨åŠ¨æ€é…ç½®çš„num_workerså’Œpin_memory
train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)
val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)

# Cell 10: è®­ç»ƒæ‰€æœ‰æ¨¡å‹
print("\nğŸš€ å¼€å§‹è®­ç»ƒå¤šä¸ªæ¨¡å‹...")

# é€‰æ‹©è¦è®­ç»ƒçš„æ¨¡å‹ï¼ˆå¯ä»¥æ ¹æ®éœ€è¦è°ƒæ•´ï¼‰
selected_models = ['efficientnet_b0', 'resnet18', 'convnext_tiny']  # å‡å°‘æ¨¡å‹æ•°é‡ä»¥é€‚åº”Kaggleç¯å¢ƒ
model_paths = {}
model_results = {}

for model_key in selected_models:
    save_path = f"best_{model_key}_model.pth"
    model_paths[model_key] = save_path
    
    # ä½¿ç”¨æ–°çš„è®­ç»ƒå‡½æ•°è¿”å›æ ¼å¼
    model_results[model_key] = train_single_model(
        model_key, train_loader, val_loader, save_path
    )
    
    print(f"âœ… {MODEL_CONFIGS[model_key]['name']} è®­ç»ƒå®Œæˆï¼Œæœ€ä½³éªŒè¯å‡†ç¡®ç‡: {model_results[model_key]['best_acc']:.4f}")
    
    # æ¸…ç†GPUå†…å­˜
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        gc.collect()

# Cell 11: å¢å¼ºè®­ç»ƒå†å²å¯è§†åŒ–
print("\nğŸ“Š ç”Ÿæˆè®­ç»ƒå†å²å¯è§†åŒ–...")
plot_training_history(model_results)

# Cell 12: é›†æˆé¢„æµ‹å’Œè¯„ä¼°
print("\nğŸ”® å¼€å§‹é›†æˆé¢„æµ‹...")

# åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹
trained_models = load_trained_models(model_paths)

# è®¡ç®—æ¨¡å‹æƒé‡
print("\nâš–ï¸ è®¡ç®—æ¨¡å‹æƒé‡...")
model_weights = calculate_model_weights(model_results, weight_method='accuracy')
print("æ¨¡å‹æƒé‡åˆ†é…:")
for model_key, weight in model_weights.items():
    print(f"  {MODEL_CONFIGS[model_key]['name']:15}: {weight:.4f}")

# è½¯æŠ•ç¥¨é¢„æµ‹
print("\nğŸ“Š è½¯æŠ•ç¥¨é›†æˆé¢„æµ‹:")
soft_predictions, true_labels, _ = ensemble_predict(trained_models, val_loader, voting_type='soft')
soft_accuracy = accuracy_score(true_labels, soft_predictions)
print(f"è½¯æŠ•ç¥¨å‡†ç¡®ç‡: {soft_accuracy:.4f}")

# ç¡¬æŠ•ç¥¨é¢„æµ‹
print("\nğŸ“Š ç¡¬æŠ•ç¥¨é›†æˆé¢„æµ‹:")
hard_predictions, _, _ = ensemble_predict(trained_models, val_loader, voting_type='hard')
hard_accuracy = accuracy_score(true_labels, hard_predictions)
print(f"ç¡¬æŠ•ç¥¨å‡†ç¡®ç‡: {hard_accuracy:.4f}")

# åŠ æƒæŠ•ç¥¨é¢„æµ‹
print("\nğŸ“Š åŠ æƒæŠ•ç¥¨é›†æˆé¢„æµ‹:")
weighted_predictions, _, _ = ensemble_predict(trained_models, val_loader, voting_type='weighted', weights=model_weights)
weighted_accuracy = accuracy_score(true_labels, weighted_predictions)
print(f"åŠ æƒæŠ•ç¥¨å‡†ç¡®ç‡: {weighted_accuracy:.4f}")

# Cell 13: ç»“æœå¯¹æ¯”å’Œå¯è§†åŒ–
# å•æ¨¡å‹ç»“æœå¯¹æ¯”
print("\nğŸ“ˆ æ¨¡å‹æ€§èƒ½å¯¹æ¯”:")
print("="*50)
for model_key in selected_models:
    best_acc = model_results[model_key]['best_acc']
    print(f"{MODEL_CONFIGS[model_key]['name']:15}: {best_acc:.4f}")

print(f"{'è½¯æŠ•ç¥¨é›†æˆ':15}: {soft_accuracy:.4f}")
print(f"{'ç¡¬æŠ•ç¥¨é›†æˆ':15}: {hard_accuracy:.4f}")
print(f"{'åŠ æƒæŠ•ç¥¨é›†æˆ':15}: {weighted_accuracy:.4f}")

# å¢å¼ºæ··æ·†çŸ©é˜µå¯è§†åŒ–
print("\nğŸ“Š ç”Ÿæˆæ··æ·†çŸ©é˜µå¯è§†åŒ–...")
plot_confusion_matrix(true_labels, soft_predictions, "Soft Voting Ensemble", "soft_voting")
plot_confusion_matrix(true_labels, hard_predictions, "Hard Voting Ensemble", "hard_voting")
plot_confusion_matrix(true_labels, weighted_predictions, "Weighted Voting Ensemble", "weighted_voting")

# Cell 14: è¯¦ç»†åˆ†ç±»æŠ¥å‘Š
print("\nğŸ“‹ è½¯æŠ•ç¥¨è¯¦ç»†åˆ†ç±»æŠ¥å‘Š:")
print("="*50)
print(classification_report(true_labels, soft_predictions, target_names=classes))

print("\nğŸ“‹ ç¡¬æŠ•ç¥¨è¯¦ç»†åˆ†ç±»æŠ¥å‘Š:")
print("="*50)
print(classification_report(true_labels, hard_predictions, target_names=classes))

print("\nğŸ“‹ åŠ æƒæŠ•ç¥¨è¯¦ç»†åˆ†ç±»æŠ¥å‘Š:")
print("="*50)
print(classification_report(true_labels, weighted_predictions, target_names=classes))

# Cell 15: é›†æˆåˆ†æå’Œè§£é‡Šæ€§å¯è§†åŒ–
print("\nğŸ“Š ç”Ÿæˆé›†æˆåˆ†æå¯è§†åŒ–...")
plot_ensemble_analysis(trained_models, val_loader, device)

print("\nğŸ“Š ç”Ÿæˆæ¨¡å‹è§£é‡Šæ€§åˆ†æ...")
plot_interpretability_analysis(trained_models, val_loader, device)

# Cell 16: æœ€ç»ˆæ€»ç»“
print("\n" + "="*60)
print("ğŸ‰ å¤šæ¨¡å‹é›†æˆè®­ç»ƒå®Œæˆï¼")
print("="*60)
print(f"è®­ç»ƒçš„æ¨¡å‹æ•°é‡: {len(selected_models)}")
print(f"æœ€ä½³å•æ¨¡å‹å‡†ç¡®ç‡: {max([results['best_acc'] for results in model_results.values()]):.4f}")
print(f"è½¯æŠ•ç¥¨é›†æˆå‡†ç¡®ç‡: {soft_accuracy:.4f}")
print(f"ç¡¬æŠ•ç¥¨é›†æˆå‡†ç¡®ç‡: {hard_accuracy:.4f}")
print(f"åŠ æƒæŠ•ç¥¨é›†æˆå‡†ç¡®ç‡: {weighted_accuracy:.4f}")

# è®¡ç®—æå‡å¹…åº¦
best_single = max([results['best_acc'] for results in model_results.values()])
soft_improvement = (soft_accuracy - best_single) * 100
hard_improvement = (hard_accuracy - best_single) * 100
weighted_improvement = (weighted_accuracy - best_single) * 100

print(f"è½¯æŠ•ç¥¨ç›¸å¯¹æå‡: {soft_improvement:+.2f}%")
print(f"ç¡¬æŠ•ç¥¨ç›¸å¯¹æå‡: {hard_improvement:+.2f}%")
print(f"åŠ æƒæŠ•ç¥¨ç›¸å¯¹æå‡: {weighted_improvement:+.2f}%")

# æ‰¾å‡ºæœ€ä½³é›†æˆæ–¹æ³•
ensemble_results = {
    'è½¯æŠ•ç¥¨': soft_accuracy,
    'ç¡¬æŠ•ç¥¨': hard_accuracy,
    'åŠ æƒæŠ•ç¥¨': weighted_accuracy
}
best_ensemble = max(ensemble_results, key=ensemble_results.get)
print(f"\nğŸ† æœ€ä½³é›†æˆæ–¹æ³•: {best_ensemble} (å‡†ç¡®ç‡: {ensemble_results[best_ensemble]:.4f})")

print(f"\nğŸ’¾ ä¿å­˜çš„æ¨¡å‹æ–‡ä»¶:")
for model_key, path in model_paths.items():
    print(f"  {MODEL_CONFIGS[model_key]['name']}: {path}")

print(f"\nâš–ï¸ æ¨¡å‹æƒé‡åˆ†é…:")
for model_key, weight in model_weights.items():
    print(f"  {MODEL_CONFIGS[model_key]['name']}: {weight:.4f}")